arXiv:2404.16560v1  [stat.ML]  25 Apr 2024Automated Model Selection for Generalized Linear
Models
Benjamin Schwendingera,∗, Florian Schwendingerb, Laura Vana-G¨ urc
aInstitute of Computer Technology, TU Wien, Gußhausstraße
27-29, Vienna, 1040, Austria
bDepartment of Statistics, University of Klagenfurt, Unive rsit¨ atsstraße
65-57, Klagenfurt, 9020, Austria
cInstitute of Statistics and Mathematical Methods in Econom ics, TU Wien, Wiedner
Hauptstraße 7, Vienna, 1040, Austria
Abstract
In this paper, we show how mixed-integer conic optimization can be us ed to
combine feature subset selection with holistic generalized linear mode ls to
fully automate the model selection process. Concretely, we direct ly optimize
for the Akaike and Bayesian information criteria while imposing constr aints
designed to deal with multicollinearity in the feature selection task. S pecif-
ically, we propose a novel pairwise correlation constraint that comb ines the
sign coherence constraint with ideas from classical statistical mod els like
Ridge regression and the OSCAR model.
Keywords: Feature Subset Selection, Holistic Generalized Linear Models,
Mixed-Integer Conic Optimization, Best Subset Selection, Computa tional
Statistics, Generalized Linear Models
1. Introduction
Model selection is an important but typically time-consuming task. Se v-
eral authors suggest using mixed-integer optimization to automat e model
selection. The classical best subset selection (BSS) problem [1] iden tiﬁes the
bestkfeatures out of ppossible features according to some goodness-of-ﬁt
∗Corresponding author.
Email address: benjaminschwe@gmail.com (Benjamin Schwendinger)
Submitted preprint(GOF) measure. For linear regression, the BSS problem, in the least -squares
sense is non-convex and known to be NP-hard [2]. In practice, the BSS
problem is commonly approached using methods such as stepwise reg ression
or exhaustive enumeration [1]. Bertsimas et al. [3] highlight the signiﬁc ant
advancements in mixed-integer optimization solvers since the 1970s when
BSS problems were ﬁrst investigated. As a result, the BSS problem, which
was once perceived as intractable, can now be solved by mixed-integ er linear
optimization solvers and used to ﬁnd the solution of real-world linear r egres-
sion problems. In the case of logistic regression, the BSS problem be comes a
mixed-integer non-linear optimization problem. To address this, Ber tsimas
and King [4] explore the utilization of several general-purpose non- linear
mixed-integer solvers. Another approach to tackle the BSS proble m in lin-
ear and logistic regression is presented by Hazimeh and Mazumder [5] , who
employ a hybrid approach combining cyclic coordinate descent with a lo cal
combinatorial search.
An extension to the BSS problem is the integration of an information c ri-
terion (IC) such as the Akaike information criterion (AIC) or the Ba yesian
information criterion (BIC) into the optimization problem’s objective . This
enables the selection of not only the best features for a ﬁxed kbut the over-
all optimal features according to the chosen goodness-of-ﬁt me asure. This
approach is commonly referred to in the optimization literature as fe ature
subset selection (FSS). Based on the division of subset selection alg orithms
into classes of ﬁlter, wrapper and embedded methods as done by Gu yon and
Elisseeﬀ [6], FSS classiﬁes as an embedded method. FSS has been applie d for
variousmodel classes including generalized linear models (GLMs). Miya shiro
and Takano [7] apply FSS to linear regression using Mallows’ Cpstatistic as
a GOF measure. By using intelligent branch-and-bound strategies, Hofmann
etal.[8]demonstratetheabilitytosolvetheFSSproblemforlinearre gression
with1,000sofobservations and100soffeatureswithin seconds. S atoet al. [9]
propose a linear approximation approach to solve the FSS problem in lo gistic
regression models. They utilize a tangent line approximation of the log istic
loss function, resulting in a mixed-integer linear problem. Likewise, Sa ishu
et al. [10] suggest using a piecewise-linear approximation, solvable th rough
mixed-integer linear programming, to address the FSS problem in Pois son
models. Many heuristic approaches for solving the FSS problem for G LMs
are based on known metaheuristics. These include simulated annealin g [11],
genetic algorithms (Yang and Honavar 12, Calcagno and de Mazanco urt 13),
ant colony optimization [14] and particle swarm optimization [15].
2In this paper, we aim to provide a uniﬁed framework for performing a u-
tomated model selection in GLMs by solving the FSS problem using mixed -
integer conic programming and to extend existing approaches to FS S to ef-
fectively handle two common challenges in GLM optimization: separatio n
and multicollinearity. Furthermore, our paper is the ﬁrst to propos e utiliz-
ing conic optimization speciﬁcally for the FSS problem in Poisson regres sion
models.
The proposed framework builds on the class of holistic generalized line ar
models (HGLMs) introduced by Schwendinger et al. [16] in package holiglm
forR. HGLMs extend linear, binomial and Poison GLMs by adding con-
straints designed to improve the model quality (e.g., restricting the number
of variables entering the model, enforcing coherent signs of coeﬃc ients, etc.).
Insteadofapproximatingthelog-likelihoodoritscomponents, holiglmformu-
lates the underlying optimization problems as conic optimization proble ms,
providing a more reliable [17] and accurate solution approach. The pa rame-
ters of these constrained models are obtained by using (mixed-inte ger) conic
optimization.
While theframeworkin[16]allows forthespeciﬁcation ofanupper bou nd
on the number of variables to be selected in the model, this bound mus t
still be predetermined prior to solving the underlying optimization pro blem.
In particular, we modify the framework of HGLMs for the purpose o f FSS
and directly integrate the AIC and BIC into the objective function r ather
than using the likelihood function as an objective. This allows us to obt ain
an exact solution to the FSS problem without resorting to piecewise- linear
approximation methods while treating FSS for linear, binomial and Pois on
GLMs in a uniﬁed way.
As mentioned above, a primary focus of the paper is addressing the issue
of separation and multicollinearity in the automated model selection p rocess.
For tackling strong multicollinearity, we propose a novel pairwise cor relation
constraint that combines ideas from the sign coherence constrain t [18] with
a simultaneous restriction of equal-magnitude coeﬃcients. The idea of re-
stricting coeﬃcients to have the same magnitude can also be found in other
statistical models, such as Ridge regression [19], where the coeﬃcie nts shrink
equally or the OSCAR (Octagonal Shrinkage and Clustering Algorithm for
Regression) model [20], where exact equality of clustered coeﬃcien ts is re-
quired. Separation on the other hand is characterized by extreme overlap
or distinct separation in the data and can result in unbounded optimiz ation
problems that common solvers often fail to detect. To ensure relia ble results,
3it is therefore vital to verify the existence of solutions. For the bin omial
family with logit, probit, log and complementary log-log link, this can be
done with a linear program [21] which we employ to avoid wrongly report ing
solutions for problems whose solution is actually not determinable.
Through extensive simulation studies, we demonstrate the feasibilit y and
practicality of our approach. The results clearly indicate that our p roposed
constraint outperforms existing methods, oﬀering a more accura te and eﬃ-
cient model selection process for GLMs, eﬀectively addressing the challenges
posed by multicollinearity.
The remainder of this paper is structured as follows: Section 2 intro duces
best subset selection, feature subset selection and holistic gener alized linear
models. Section 3 explores common pitfalls that arise when estimating cer-
tain GLMs, such as failure to converge or the nonexistence of a solu tion and
possible solutions to mitigate these problems. In Section 4, we prese nt our
proposedoptimizationproblemfor automatedmodel selection inGLM s. The
importance of dealing with multicollinearity when aiming for automated f ea-
tureselection isillustrated throughasimulation study inSection5. Se ction 6
concludes the paper.
2. Feature subset selection in GLMs
In this section we set the stage for introducing the proposed mode ling ap-
proach for performing FSS in GLMs. More speciﬁcally, we start with a brief
introduction to GLMs in Section 2.1 and show how to formulate the likeli-
hood optimization problem such that it can be solved using (mixed-inte ger)
conic programming. FSS is an extension to BSS, in the sense that the objec-
tive (i.e., log-likelihood) in the BSS problem is replaced by an information
criterion. Before introducing the FSS problem, we introduce the fo rmulation
of the BSS problem as a conic program in Section 2.2. Section 2.3 introd uces
the information criteria we use to extend BSS to FSS.
2.1. Generalized linear models and conic optimization
In this work we focus on solving the FSS problem for GLMs with linear,
binomial and Poisson families. Generally, GLMs as introduced by Nelder
and Wedderburn [22], are a classs of models with probability density fu nc-
tions that belong to the exponential dispersion model (EDM) family w ith
4probability density function:
f(y;θ,φ) = exp((yθ−b(θ))
φ+c(y,φ))
. (1)
Here,b(·) andc(·) are well-deﬁned functions that vary depending on the
speciﬁc distribution. In addition, in the presence of a (design) matr ix of
covariates Xwithp+1 columns (including a column of ones), a GLM has a
linear predictor η=Xβ, and a link function gthat establishes the relation-
ship between the linear combination of the p+ 1 covariates and the mean
of response yi:g(E(yi)) =ηi. Givengandb,θis then a function of ηand
therefore of β. Given a sample of nindependent and identically distributed
response observations y⊤= (y1,...,y n) and observed covariates, the esti-
mation of the parameters is usually done by maximum likelihood, and the
maximum likelihood estimate (MLE) of ( β,φ) are the values ( β∗,φ∗) that
maximize the (log)-likelihood function for the EDM family:
logL(β;y) =n∑
i=1logf(y;θ,φ) =n∑
i=1yiθi(β)−b(θi(β))
φi+c(yi,φi).(2)
Conic optimization provides a framework for expressing the maximiza -
tion of the log-likelihood function of various GLMs as convex optimiza-
tion problems. A conic optimization problem is designed to model con-
vex problems by optimizing a linear objective function over the inters ec-
tion of an aﬃne hyperplane and a nonempty closed convex cone. The log-
likelihood maximization can be reformulated as a conic problem. The rea -
son for this lies in the fact that the log-likelihood of common GLMs in-
cludes functions that can be represented by convex cones, which in turn
can be solved by modern conic optimization solvers. The estimation by
means of the conic programming is in turn feasible given that eﬃcient o p-
timization solvers exist which can provide exact solutions. More spec iﬁ-
cally, the MLE for the linear regression model (Gaussian family with ide n-
tity link) is the solution of a convex optimization problem which uses the
second-order cone Kn
soc:={(t,x)∈Rn|x∈Rn−1,t∈R,||x||2≤t}.
Since both logistic regression and Poisson regression involve expone ntial
and logarithmic terms in their log-likelihoods, the primal exponential c one
Kexpp:={(x,y,z)∈R3|y >0,yex
y≤z}∪{(x,0,z)∈R3|x≤0,z≥0}is
utilized to represent them.
5Acomprehensive introduction toconicoptimizationcanbefoundinBo yd
and Vandenberghe [23]. For the detailed explanation and derivation o f conic
formulations for various GLMs based on the family and link information , we
refer to the appendix provided by Schwendinger et al. [16].
2.2. Best subset selection
Weintroducetheclassical best subset selection (BSS)problembef orepre-
senting its extension to FSS. The classical best subset selection (B SS) prob-
lemisconcernedwithdetermining thebest kfeaturesfromasetof ppotential
featuresusingsomegoodness-of-ﬁt(GOF)metric. However, th eBSSproblem
is non-convex and NP-hard [2]. Surrogate models are often used t o overcome
this computational burden, such as those that incorporate an L1penalty
or a combination of L1andL2penalties [24]. Adding an L1penalty gives
the least absolute shrinkage and selection operator (LASSO) [25]. H owever,
the performance of BSS and LASSO depends on the signal-to-noise ratio.
A comprehensive overview by Hastie et al. [26] highlights situations fo r the
linear regression case where BSS outperforms LASSO and vice vers a. Yang
et al. [27] approximate the BSS problem by solving a sequence of weigh ted
LASSO problems, with the weights determined progressively.
For generalized linear models where the log-likelihood is used as the
goodness-of-ﬁt measure, the BSS problem can be formulated as a n optimiza-
tion problem. The goal is to minimize the negative log-likelihood (equivale nt
to maximizing the likelihood) with respect to the parameter vector βwhile
constraining the number of selected features to be k. This results in the
following optimization problem:
minimize
β−logL(β;y) subject top∑
i=1I{βi̸=0}≤k. (3)
Here,k∈1,...,pis a user-deﬁned parameter that restricts the size of the
subset. We can formulate the whole problem as a mixed-integer conv ex
optimization problem containing the constraint with the ℓ0pseudo-norm,
which counts the number of non-zero entries in β. To do so, we introduce p
binary variables zithat indicate whether the covariate βiis selected for the
model or not. Note that β0denotes the intercept, which is always included
6in the model. Now, the BSS problem becomes the following problem:
minimize
β−logL(β;y)
subject to −Mzi≤βi≤Mzi, i= 1,...,p,
p∑
i=1zi≤k,
β∈Rp+1,z∈ {0,1}p.(4)
Here,M≥ ||ˆβ||∞is a constant that ensures that a coeﬃcient βiis zero if the
corresponding binary variable ziis zero. In other words, ziindicates whether
βiis included in the model. These types of constraints are often refer red to
as big-Mconstraints. It is well documented that problems containing big- M
constraints depend ona goodchoice of M. IfMis too small, the convergence
to the same optimum as the original problem is not guaranteed. If Mis too
large, loss of accuracy and numerical instability may occur. One sho uld also
beawarethatchoosinganarbitrarilylarge Mresultsinanunnecessarily large
feasibleregionfortheLPrelaxation[28]. Toaddressthischallenge, B ertsimas
etal.[3]proposedadata-drivenmethodtodeterminelowerandupp erbounds
forˆβiinlinearregression. Inthefollowing, weextendthisapproachtocon vex
GLMs. Let UB be an upper bound on the MLE of Problem 3. Then one ca n
ﬁnd lower and upper bounds by solving the following convex optimizatio n
problems:
u+
i:= maximize
ββi
s. t.−logL(β;y)≤UB,(5)
u−
i:= minimize
ββi
s. t.−logL(β;y)≤UB(6)
whereu+
iis an upper bound and u−
iis a lower bound to ˆβi. Now, let Mi:=
max{|u+
i|,|u−
i|}and one can choose the big- MasM= max
iMi.
This procedure involves solving several convex optimizationproblem s and
estimating an upper bound (UB) beforehand. An alternative simpler ap-
proach, which we have also employed in our simulation studies, is to sta n-
dardize the design matrix Xand choose an Mthat works well for most
settings. We have found that a value of M= 100 works well for many data
sets when using a standardized design matrix.
72.3. Information criteria for feature subset selection
While BSS obtains the best subset of features for a ﬁxed number of max-
imal active coeﬃcients k(where a coeﬃcient is said to be active if it is non-
zero), information criteria such as the Akaike Information Criterio n [29] or
the Bayesian Information Criterion [30] are often used (typically in a second
stage, see Hofmann et al. [8]) to select the best model out of the c andidate
models with diﬀerent number of active coeﬃcients. For linear regres sion, it
is computationally advantageous not to optimize the AIC directly but to use
Mallows’ Cpstatistic (CP) [31] instead. Boisbunon et al. [32] show that for
linear regression, the AIC and CP are equivalent in the sense that bo th reach
their minimum objective value with the same set of active coeﬃcients.
Instead of having a two stage procedure, in FSS we will replace the o bjec-
tive function in Equation 4 by an IC. Although many other ICs exist, w e will
focus on the AIC and BIC in this paper, as they are the most common ly used
inpractice. Forlinearregression, however, weusethecomputatio nallyadvan-
tageous CP to replace the AIC and a modiﬁed CP (CP2) to replace the BIC.
The proof of the equivalence of BIC and CP2 can be found in Appendix B.
The Akaike Information Criterion (AIC) is deﬁned as follows:
AIC = 2k−2log(L) (7)
whereLis the likelihood function and krepresents the number of active
covariates. Similarly, the Bayesian Information Criterion (BIC) is de ﬁned
as:
BIC =klog(n)−2log(L). (8)
The Mallows’ Cpstatistic (CP) for linear regression is deﬁned by:
CP = 2k+1
σ2||y−Xβ||2
2−n. (9)
Similarly, the modiﬁed CP, which is equivalent to the BIC, is deﬁned as
follows:
CP2 =klog(n)+1
σ2||y−Xβ||2
2−n. (10)
Hereσ2is the residual variance. The following observation, which is some-
times referred to as monotonicity of the GOF measure, applies to AI C, BIC,
CPandCP2. When weencounter two modelswiththesame likelihoodvalu e,
then the model with fewer selected coeﬃcients is better. We use th e unbiased
estimator of the variance ˆ σ2=||y−Xβ||2
2
n−pfrom the full regression model.
83. Issues in GLM optimization
In this section, we investigate common data settings of GLMs that c ause
optimizers to recover wrong solutions. Two main reasons are causin g opti-
mizers to recover an incorrect solution for GLMs. Firstly, for GLMs with a
binomial response, separation in the data can cause the maximum like lihood
estimate to contain inﬁnite components. Secondly, cases where th e data ex-
hibits strong multicollinearity can lead to instability in the estimation and
in inconsistent signs in the regression coeﬃents.
Before introducing our approach, we present in the following some p ro-
posed approaches in the literature to address these issues.
3.1. Separation
AlbertandAnderson[33]showthatforlogisticregressionandprob itmod-
els, the ﬁniteness and uniqueness of the MLE are connected to the overlap
of the data. They identify three diﬀerent data settings, complete separation ,
quasi-complete separation andoverlapand show that for logistic regression
overlap is necessary and suﬃcient for the ﬁniteness of the MLE. Ko nis and
Fokianos [34] translate these criteria into a linear problem, which can be
checked to verify the existence of the MLE.
Although, in theory, the solvers should be able to detect the unbou nd-
edness of the problem, we found in our experiments that all the solv ers we
tried failed to detect the unboundedness for GLMs with a binomial re sponse.
Therefore, it is important to check in the simulation that, indeed, a s olution
exists; otherwise, we would mainly compare the default tolerance se ttings of
the solvers.
Konis and Fokianos [34] suggest using the following linear program (LP )
to verify that the solution exists:
maximize
β∑
i∈Ix⊤
iβ−∑
i∈Jx⊤
iβ
subject to x⊤
iβ≤0∀i∈J={i|yi= 0}
x⊤
iβ≥0∀i∈I={i|yi= 1}.(11)
If the solution is a zero vector, this veriﬁes that the data is overlap ping
and that the solution of the corresponding logistic regression mode l is ﬁ-
nite and unique. In case the solution of the LP is unbounded, the dat a is
(quasi-)separated and the MLE does not exist.
9In the preparation of this paper, we found more than two examples of
peer-reviewed articles where the authors did not check their data for sepa-
ration, which led them to report results based on unbounded optimiz ation
problems. This likely occurred as a consequence of the solver incorr ectly
signaling convergence.
3.2. Multicollinearity
In thepresence of strong collinearity, the matrix X⊤Xin linear regression
becomes ill-conditioned, leading to unstable estimates of the coeﬃcie nts (ˆβ).
This instability can result in inﬂated estimates and inconsistent signs o f the
coeﬃcients. Diﬀerent approaches to solving this problem have been proposed
in the optimization and statistics literature. One approach, as sugg ested by
Bertsimas and King [35], is to limit collinearity by incorporating the at most
one constraint :
zi+zj≤1∀(i,j)∈ HC={(i,j) :ν≤ |ρij|}, (12)
where the binary zvariables are the ones introduced in Equation 4, νis a
predeﬁnedconstantand ρijdenotesPearson’sempiricalcorrelationcoeﬃcient
between the i-th and j-th columns of the design matrix X. Hence, HC
represents the set of highly correlated features. This constrain t limits the
pairwise correlation by ensuring that, at most one of the variables a mong a
pair with a correlation exceeding νis selected in the regression model.
Carrizosaetal.[18]relaxthisconstrainttothe sign coherenceconstraint (see
Equation 13)
−M(1−uij)≤βi,sign(ρij)βj≤Muij∀(i,j)∈ G={(i,j) :τ≤ |ρij|}
(13)
to force coeﬃcients of covariates with large pairwise multicollinearity to have
coherent signs. Hence, highly positively correlated features must have coef-
ﬁcients with the same sign, while highly negatively correlated feature s must
have coeﬃcients with opposite signs. Again, Mis a suﬃciently large enough
constant and uijisa binaryvariableintroduced to enforcethesign coherence.
One can see that for uij= 1 and positive ρijit holds that 0 ≤βi,βj≤M.
Similarly, for uij= 0 and negative ρijwe have that −M≤βi≤0 and
0≤βj≤M. Clearly, the sign coherence constraint is less restrictive than
theat most one constraint .
On the other hand, a diﬀerent but related approach in statistics is t o
assume that strongly correlated features should have similar estim ates. This
10assumption is utilized in models like Ridge regression [19] and the OSCAR
model [20]. In Ridge regression, a L2penalty term is added to the objec-
tive function, encouraging similar estimates for strongly correlate d features.
The OSCAR model goes further by enforcing exactly the same coeﬃ cient
for strongly correlated features, inducing a clustering behavior a mong the
coeﬃcients.
4. Suggested model
4.1. FSS for the Poisson model
WepresentinthissectiontheformulationoftheFSSproblemforthe Pois-
son model using the AIC, which, to the best of our knowledge, has n ot been
proposed before in the literature. The corresponding AIC and BIC formula-
tions for linear, logistic, andPoisson regression can befound in Appe ndix C.
Weformulatethefollowingmixed-integerconicprogramforfeature subset
selection:
minimize
β,δ2(
p∑
j=1zj)
−2(n∑
i=1yix⊤
iβ−δi)
subject to ( x⊤
iβ,1,δi)∈ Kexpp, i= 1,...,n,
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,δ∈Rn.(14)
This problem can be solved by oﬀ-the-shelf mixed-integer conic optim ization
solvers like ECOS[36] orMOSEK [37]. It is worth noting that Saishu et al.
[10] have highlighted the concave but non-linear nature of the log-lik elihood
function for Poisson regression and proposed a piecewise-linear ap proxima-
tion method. On the other hand, we are the ﬁrst to suggest utilizing conic
optimization to solve the feature subset selection problem speciﬁca lly for
Poisson regression. Similar problem formulations can be established f or all
family-link combinations introduced in [16].
4.2. Combined constraint for multicollinearity
To further enhance this FSS model to handle multicollinearity, we pro -
pose the so-called combined constraint where we integrate the sign coherence
constraint and aequal magnitude constraint as a uniﬁed criterion. Based on
the idea of similar estimates for highly correlated features, we believ e that an
equal magnitude constraint would be beneﬁcial in much the same way that
thesign coherence constraint extends the at most one constraint . Assuming
11that the design matrix Xhas been standardized, we can deﬁne the equal
magnitude constraint as follows:
βi= sign(ρij)βj∀(i,j)∈ HC={(i,j) :ν≤ |ρij|} (15)
hereρijagain refers to the Pearson’s correlation coeﬃcient, and νis a pre-
deﬁned constant threshold. This constraint ensures that the co eﬃcients of
strongly correlated features have the same magnitude but possib ly diﬀerent
signs based on the correlation direction.
Thiscombinationallowsustoautomaticallycontrolpairwisemulticollinea r-
ity in addition to the feature subset selection process. The sign coherence
constraint is applied to variables exhibiting a moderate to strong pairwise
correlation, while the equal magnitude constraint is exclusively imposed on
strongly correlated pairs. By employing the equal magnitude constraint in-
stead of the previous at most one constraint, users gain more insig htful in-
formation about the underlying data structure, instead of making a near-
random selection.
4.3. Final model
By incorporating FSS with our novel combined constraint , we formulate
the following optimization problem:
minimize
βIC
subject to −Mzi≤βi≤Mzi, i= 1,...,p,
−M(1−uij)≤βi,sign(ρij)βj≤Muij∀(i,j)∈ G
βi= sign(ρij)βj∀(i,j)∈ HC
β∈Rp+1,z∈ {0,1}p,u∈ {0,1}|G|(16)
whereG={(i,j) :τ≤ |ρij|< ν}andHC={(i,j) :ν≤ |ρij|}, with
0≤τ < ν≤1. Again, Mrepresents a suﬃciently large positive constant.
This model simultaneously enforces coherent coeﬃcient signs for m oderately
correlated features and equal coeﬃcient magnitudes for highly co rrelated
features. The thresholds for medium and high correlations are deﬁ ned by
τandν, respectively. Moreover, before estimating any binomial model, we
employ the linear program in [21] to identify the problems whose solutio n is
actually not determinable.
It is worth noting that the objective function of our ﬁnal model, as shown
in Equation (16), incorporates an arbitrary information criterion. Conse-
quently, our model can accommodate multiple information criteria, s uch as
12AIC, BIC, CP, and CP2, which can be expressed using the respectiv e for-
mulas in Equation (7, 8, 9, 10). This not only allows for greater ﬂexibilit y,
but also opens to the door to more customized models. The model ca n be
further extended by using further holistic constraints and exper t knowledge
can be seamlessly integrated into the model. Our entire approach sh ould be
viewed as an additional tool in the modern data scientist’s tool belt.
5. Simulation studies
In our simulation study, we present two key ﬁndings. Firstly, our ap -
proach demonstrates the ability to recover the true predictors. Speciﬁcally,
weachieveselectionaccuracycomparabletothatofexactmethod s, highlight-
ing the quality of our solutions. Secondly, our newly integrated cons traint
proves successful in estimating variables within a multicollinearity con text.
This ﬁnding emphasizes the eﬀectiveness of our approach in overco ming chal-
lenges posed by multicollinearity and recovering accurate estimates .
All computational experiments were conducted on a Dell XPS15 lap-
top with an Intel Core i7–8750H CPU @ 2.20GHzx12 processor and 32
GB of RAM. We utilized three mixed-integer optimization solvers: Gurobi
9.1.2 [38], MOSEK 10.0.34 [37], and ECOS2.0.5 [36]. The RpackageROI
was employed for representing the optimization problems.
To obtain the exact reference solutions, we utilized the Rpackages lm-
Subsets[39] and bestglm [40]. The bestglm package can also solve linear
regression problems, but we exclude it from the comparison as lmSubsets
exhibits signiﬁcantly faster performance. There exist many more Rpackages
for subset selection, such as glmulti[13],L0Learn [5] orabess[41], but only
the selected two ensure that the solutions are indeed globally optima l. In
order to ensure a fair comparison, all solvers were restricted to u tilizing only
a single core.
5.1. Simulation without multicollinearity
In this simulation, we compare the performance of the FSS formulat ions
suggested in Equation (14, C.1, C.2, C.3, C.4, C.5) with special purpos e
solvers for FSS and BSS.
We use true positives ( TP) and true negatives ( TN) to calculate the
selection accuracy. The true positives are the number of feature sjfor which
13both the estimated coeﬃcient ( ˆβj) and the true coeﬃcient ( βtrue
j) are non-
zero:
TP(β) =|j:ˆβj̸= 0,βtrue
j̸= 0|. (17)
Analogously, the true negatives represent the number of featur esjfor which
both the estimated coeﬃcient ( ˆβj) and the true coeﬃcient ( βtrue
j) are zero:
TN(β) =|j:ˆβj= 0,βtrue
j= 0|. (18)
OnceTPandTNare calculated, the selection accuracy ( A) is determined
as the sum of TPandTNdivided by the total number of potential features
(p):
A(β) =TP+TN
p. (19)
The runtime provides an indication of the computational eﬃciency of the
solvers. At the same time, the selection accuracy measures how we ll the
solvers are able to correctly identify the relevant and irrelevant fe atures.
For linear regression, we employ the specialized solver lmSelect from the
lmSubsets package, which utilizes a branch-and-bound strategy tailored for
this problem. For logistic and Poisson regression, we use the dedicat ed solver
bestglm. Unlike lmSubsets ,bestglm employs complete enumeration and is
only suitable for regression problems with a moderate number of fea tures.
To avoid excessively long runtimes, bestglm restricts the maximum number
of features (for non-Gaussian families) to 15.
Following the setting of Hofmann et al. [8] the simulation study adopts
the following design: the design matrix Xis generated from a multivariate
normal distribution, with xi∼ N(0,Σ) fori= 1,...,nwith a mean of zero
and the covariance matrix Σ is the identity matrix Ip. Each scenario consists
of 5 diﬀerent runs, with n= 1000 observations. The number of features p
varies, and for each scenario, the ﬁrst ⌈p
2⌉coeﬃcients of βare set to 1, while
the remaining coeﬃcients are set to 0.
In the linear regression setting, we generate 175 datasets. The n umber
of features pvaries among {20,25,30,35,40,45,50}, and diﬀerent standard
deviations σ∈ {0.05,0.10,0.50,1.00,5.00}are employed. The response vari-
ableyiis generated according to yi=x⊤
iβ+ǫi, whereǫifollows a normal
distribution with mean 0 and variance σ2. To avoid long runtimes in the
brute force setting, we limit the allowed time for each run to 4200 sec onds.
In the logistic and Poisson regression settings, we vary the number of
features within p∈ {5,10,15,20,25,30,35,40}. The inverse link function is
14used to calculate µi=g−1(ηi), where ηi=x⊤
iβ. Depending on the model,
the response yiis sampled from either yi∼Binomial(1 ,p=µi) oryi∼
Poisson(λ=µi), based on the underlying distribution assumption.
Tables C.1–C.6 summarize the results of the computation time and sele c-
tion accuracy for the diﬀerent GLMs.
For linear regression, Table C.2 shows that all tested methods achie ve a
high selection accuracy. However, in some cases, lmSelect ﬁnds models with
a slightly lower AIC or BIC, including more features. Upon inspecting t he
data, it is discovered that the true coeﬃcients of the additionally se lected
features are all 0. This discrepancy can be attributed to the fact that the
residual variance σ2is only estimated, and as a result, the minima of AIC
and Mallows’ Cpmay not coincide perfectly. As for the computation time,
TableC.1indicatesthatourFSSmodelisonlyslightlyslower than lmSelect .
Compared to the enumerating BSS approach of trying all diﬀerent v alues for
the sparsity parameter k, the FSS approach is faster and scales better with
the number of features.
Regarding logistic regression, FSS can recover the actual coeﬃcie nts with
high accuracy, as can be seen in Table C.4. In terms of scalability, our
FSS approach scales better than the complete enumeration of bestglm. See
Table C.3 for the full timings.
Our ﬁndings for Poisson regression are similar to those for logistic re gres-
sion. The computation time and selection quality results are summariz ed in
Table C.5 and Table C.6. Here we achieve excellent selection accuracies and
FSS scales better than complete enumeration. For Poisson regres sion, we
found that ECOSencounters numerical problems for the simulation setting
with more than 20 features (see also Table C.6).
5.2. Simulation with multicollinearity
In this section, we compare the performance of FSS with and withou t
pairwise correlation constraints. We use the mean squared error ( MSE) as
a performance measure of how well a method performs at recover ing the
estimates. We consider three types of pairwise correlation constr aints: the
at most one constraint , thesign coherence constraint and thecombined con-
straint. Moreover, toestablishabaseline, wealsobenchmarkamodelselec ted
basedontheinformationcriterionalonewithout additionalconstra intswhich
wecallno constraint model. Followingtherecommendation ofBertsimasand
King [35] we set the threshold for the at most one constraint toν= 0.7. If
the correlation between two features exceeds 0 .7, at most, one of the features
15can be selected in the feature subset. The sign coherence constraint evalu-
ates two thresholds, τ= 0.5 andτ= 0.7. This constraint ensures that if
two features have a correlation magnitude above the threshold, t hey must
have a coherent sign in the feature subset. Thus, highly positively c orrelated
features are forced to have the same sign, while highly negatively co rrelated
features must have opposite signs. The combined constraint combines the
ideas of the equal magnitude constraint and thesign coherence constraint by
enforcing the equal magnitude for highly correlated feature pairs and ensur-
ing sign coherence for moderately correlated feature pairs. To dis tinguish
between moderately and highly correlated feature pairs, we used t he thresh-
oldsτ= 0.5 andν= 0.7.
It is important to note that the simulation study design will impact the
study’s outcome, especially when comparing pairwise correlation con straints.
By using the simulation setting proposed by McDonald and Galarneau [ 42],
we aim to provide a common ground for comparing the constraints. T his
simulation setting is widely used in statistics to simulate collinearity amon g
features.
In this setting, the design matrix Xis constructed as follows:
xij=√
(1−α2)zij+αi(p+1), where i= 1,...,nandj= 1,...,p. (20)
Here,n= 100 represents the number of observations, p= 3 denotes the
number of features, and zijis drawn from a normal distribution with mean
0 and standard deviation σ. The design matrix Xis standardized, resulting
inX⊤Xbeing in correlation form. The parameter αtakes values from the
set{0.7,0.8,0.9,0.95,0.99}. In this particular setup, ρij=α2gives the
correlationbetween anytwofeatures. Consequently, thepairwis ecorrelations
areρij={0.49,0.64,0.81,0.90,0.98}. Theβcoeﬃcients are generated by
computing the eigenvectors of X⊤Xand selecting the eigenvector associated
with the largest eigenvalue.
ηi=β0+β1x1i+β2x2i+···+βpxpi=x⊤
iβ (21)
Subsequently, Equation (21) is employed to calculate ηiwhereβ0is set to
zero and µiis given by µi=g−1(ηi). Finally, the response yiis sampled
either from yi∼Normal(µi,σ) oryi∼Binomial(1 ,µi) oryi∼Poisson(µi).
This simulation setup allows for the generation of data with controlled
correlations among features, enabling the evaluation and comparis on of the
16diﬀerent pairwise correlation constraints in the context of featur e subset se-
lection. Table C.7, Table C.8 and Table C.9 summarize the results for the
diﬀerent constraints and correlation settings for 1,000 simulations and 100
observations.
In the linear regression setting, the AIC and BIC exhibited almost ide n-
tical results, leading us to report only the AIC outcomes. Analysis o f Ta-
bleC.9suggeststhatwhenthestandarddeviation σisverysmall(speciﬁcally,
σ= 0.01) and the αvalues range from small to medium (0.7, 0.8, and 0.9),
both the no constraint and thesign coherence constraint slightly outperform
thecombined constraint . Theat most one constraint performs best for small
correlations. This observation can be attributed to the decrease d likelihood
of the constraint becoming active at lower αvalues, rendering its perfor-
mance similar to the one of the no constraint . Overall, the simulation results
indicate that the combined constraint performs exceptionally well in thissim-
ulation setting, eﬀectively harnessing the advantages of the sign coherence
constraint and theequal magnitude constraint .
Table C.7 presents the results for the logistic regression setting, w here the
generated µivalues implicitly determine the standard deviation. In this sce-
nario, the combined constraint outperformed all other constraints. While the
MSE values for all constraints were similar under low correlation cond itions,
thecombined constraint exhibited signiﬁcantly lower MSE values compared
to the other constraints under strong correlation.
Similarly, Table C.8 showcases the results for the Poisson regression set-
ting. In this case, all constraints performed similarly under low corr elation
conditions. However, as the correlation increased, the combined constraint
emerged as advantageous in this simulation setting. Furthermore, the results
indicate that the MSE values for the no constraint and thesign coherence
constraint were almost identical, suggesting that inconsistent signs were rare
in this simulation setup.
6. Conclusion
This paper proposes an automated model selection approach by co mbin-
ing FSS with a correlation constraint for speciﬁc types of generalize d lin-
ear models (GLMs), including linear, logistic, and Poisson regression. Our
method utilizes conic optimization and information criteria such as AIC or
BIC for feature subset selection (FSS). Moreover, it also enables the integra-
tion of additional constraints such as limiting pairwise correlation. We have
17shown that our approach achieves high selection accuracy and impr oves com-
putational eﬃciency as the dimensionality of the problem increases, outper-
forming naive enumeration methods. The key contribution of our wo rk is the
development of a new mixed-integer conic programming (MICP) prob lem for
model selection under multicollinearity. Our experiments have demon strated
thatourproposedcombinedconstraintsurpassestheexistingco llinearitycon-
straints in the literature regarding performance. Future resear ch directions
could explore incorporating additional regularization terms or lever aging lin-
ear approximations to speed up computation times.
Acknowledgements
This work was supported by the Austrian Science Fund (FWF) under
grant number ZK 35.
Appendix A. GLMs
The PDF of the Normal distribution can be expressed as:
f(y;µ,σ2) =1
√
2πσ2exp(
−(y−µ)2
2σ2)= exp((yµ−µ2/2)
σ2+(
−y2
2σ2−1
2log(2πσ2)))
(A.1)
where one can see that it is indeed part of the EDM family with θ=µ,
φ=σ2,b(θ) =θ2
2andc(y,φ) =−y2
2φ−1
2log(2πφ).
The PDF of the binomial distribution can be written as:
f(y;p) =(n
y)
py(1−p)n−y= exp(
log(n
y)
+ylog(p
1−p)+nlog(1−p))
.
(A.2)
Together with the link function θ= logit(p) = log(
p
1−p)
we get
f(y;p) = exp(
yθ+nlog(1
1+exp(θ))
+log(n
y))
.(A.3)
Here we can see that the binomial distribution is part of the EDM family
withθ= log(
p
1−p)
,φ= 1,b(θ) =−nlog(
1
1+exp(θ))
andc(y,φ) = log(n
y)
.
The PDF of the Poisson distribution can be expressed by:
f(y;µ) =µyexp(−µ)
y!= exp(ylog(µ)−µ−log(y!)).(A.4)
18Now using the link function θ= log(µ) which is equal to exp( θ) =µ, we get
f(y;µ) = exp(yθ−exp(θ)−log(y!)). (A.5)
We can see that the Poisson distribution is also part of the EDM family w ith
θ= log(µ),φ= 1,b(θ) = exp(θ) andc(y,φ) =−log(y!).
Appendix B. Mallows Cpfor BIC
The log-likelihood for linear regression is given by:
logL(β;y) =−1
2σ2(yi−x⊤
iβ)2−1
2log(2πσ2). (B.1)
Given the deﬁnition of BIC (in Equation (8)) and CP2 (in Equation (10) )
we have
BIC =klog(n)−2log(L)
=klog(n)+∑n
i=11
σ2(yi−x⊤
iβ)2+log(2πσ2)
=klog(n)+nlog(2πσ2)+||y−Xβ||2
2
σ2
= CP2+ n(log(2πσ2)+1)(B.2)
This implies that they reach their respective minima at the same coeﬃc ient
values.
Appendix C. Mixed-integer conic optimization problems for FSS
FSS for AIC
Gaussian
minimize
β,ζ2(
p∑
j=1zj)
+1
σ2ζ
subject to ( ζ+1,ζ−1,2(y1−x⊤
1β),...,2(yn−x⊤
nβ))∈ Kn+2
soc
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,ζ∈R.(C.1)
19Binomial
minimize
β,δ,γ2(
p∑
j=1zj)
−2(n∑
i=1yix⊤
iβ−δi)
subject to ( δi,1,1+γi)∈ Kexpp, i= 1,...,n
(x⊤
iβ,1,γi)∈ Kexpp, i= 1,...,n
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,δ∈Rn,γ∈Rn.(C.2)
Poisson
The optimization problem is already given in Equation (14).
FSS for BIC
Gaussian
minimize
β,ζlog(n)(
p∑
j=1zj)
+1
σ2ζ
subject to ( ζ+1,ζ−1,2(y1−x⊤
1β),...,2(yn−x⊤
nβ))∈ Kn+2
soc
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,ζ∈R.(C.3)
Binomial
minimize
β,δ,γlog(n)(
p∑
j=1zj)
−2(n∑
i=1yix⊤
iβ−δi)
subject to ( δi,1,1+γi)∈ Kexpp, i= 1,...,n
(x⊤
iβ,1,γi)∈ Kexpp, i= 1,...,n
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,δ∈Rn,γ∈Rn.(C.4)
Poisson
minimize
β,δlog(n)(
p∑
j=1zj)
−2(n∑
i=1yix⊤
iβ−δi)
subject to ( x⊤
iβ,1,δi)∈ Kexpp, i= 1,...,n,
−Mzi≤βi≤Mzi, i= 1,...,p,
β∈Rp+1,z∈ {0,1}p,δ∈Rn.(C.5)
20Result Tables
Table C.1: Simulation without multicollinearity: Comparison of proposed method, HLM
with enumeration and special purpose solver for linear regression; average execution times
in seconds.
Linear model training time
AIC BIC
Proposed method HLM with brute force Proposed method HLM with b rute force
σp (Gurobi) lmSelect (Gurobi) (Gurobi) lmSelect (Gurobi)
0.05 20 0.049 0.009 1.140 0.047 0.002 1.140
25 0.063 0.003 2.989 0.062 0.003 2.989
30 0.085 0.003 9.551 0.069 0.003 9.551
35 0.196 0.004 58.876 0.086 0.004 58.876
40 0.179 0.004 224.310 0.094 0.004 224.310
45 0.214 0.007 1384.208 0.123 0.005 1384.208
50 0.381 0.007 – 0.149 0.006 –
0.1 20 0.059 0.003 1.252 0.055 0.003 1.252
25 0.065 0.003 3.347 0.060 0.003 3.347
30 0.084 0.004 10.526 0.074 0.003 10.526
35 0.169 0.005 51.952 0.111 0.005 51.952
40 0.215 0.005 198.132 0.106 0.005 198.132
45 0.229 0.006 1441.896 0.120 0.006 1441.896
50 1.012 0.007 – 0.185 0.008 –
0.5 20 0.070 0.003 1.187 0.059 0.003 1.187
25 0.073 0.004 2.900 0.076 0.004 2.900
30 0.102 0.004 10.476 0.084 0.004 10.476
35 0.141 0.005 54.417 0.113 0.005 54.417
40 0.194 0.006 206.109 0.122 0.006 206.109
45 0.353 0.007 1411.889 0.164 0.007 1411.889
50 0.397 0.007 – 0.146 0.007 –
1 20 0.057 0.003 1.239 0.049 0.002 1.239
25 0.063 0.004 3.074 0.076 0.003 3.074
30 0.085 0.005 11.753 0.071 0.004 11.753
35 0.124 0.005 65.864 0.091 0.004 65.864
40 0.283 0.005 202.285 0.117 0.005 202.285
45 0.275 0.007 1490.630 0.126 0.006 1490.630
50 0.400 0.007 – 0.143 0.008 –
5 20 0.086 0.003 1.314 0.075 0.003 1.314
25 0.095 0.004 3.085 0.093 0.004 3.085
30 0.126 0.004 11.729 0.133 0.005 11.729
35 0.154 0.004 45.243 0.164 0.006 45.243
40 0.433 0.006 155.281 0.254 0.010 155.281
45 0.309 0.007 1231.948 0.606 0.020 1231.948
50 0.388 0.008 – 1.151 0.035 –
21Table C.2: Simulation without multicollinearity: Comparison of proposed method, HLM
with enumeration and special purpose solver for linear regression; average (sd) selection
accuracy.
Linear model support recovery accuracy
AIC BIC
Proposed method HLM with brute force Proposed method HLM with b rute force
σp (Gurobi) lmSelect (Gurobi) (Gurobi) lmSelect (Gurobi)
0.05 20 0.91 (0.02) 0.91 (0.02) 0.91 (0.02) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
25 0.90 (0.04) 0.90 (0.04) 0.90 (0.04) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
30 0.89 (0.05) 0.89 (0.05) 0.89 (0.05) 0.99 (0.03) 0.99 (0.03) 0.99 (0.03)
35 0.90 (0.09) 0.90 (0.09) 0.90 (0.09) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
40 0.92 (0.03) 0.92 (0.04) 0.92 (0.04) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
45 0.90 (0.03) 0.90 (0.03) 0.90 (0.03) 1.00 (0.01) 1.00 (0.01) 1.00 (0.01)
50 0.94 (0.03) 0.94 (0.03) – 1.00 (0.00) 1.00 (0.00) –
0.1 20 0.90 (0.05) 0.89 (0.07) 0.90 (0.05) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
25 0.93 (0.05) 0.93 (0.05) 0.93 (0.05) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
30 0.93 (0.04) 0.93 (0.04) 0.93 (0.04) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
35 0.91 (0.04) 0.90 (0.05) 0.91 (0.05) 0.99 (0.01) 0.99 (0.03) 0.99 (0.03)
40 0.93 (0.04) 0.93 (0.04) 0.93 (0.04) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
45 0.91 (0.05) 0.91 (0.05) 0.91 (0.05) 1.00 (0.01) 1.00 (0.01) 1.00 (0.01)
50 0.90 (0.03) 0.90 (0.03) – 0.99 (0.01) 0.99 (0.01) –
0.5 20 0.91 (0.11) 0.91 (0.11) 0.91 (0.11) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
25 0.91 (0.08) 0.91 (0.08) 0.91 (0.08) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
30 0.96 (0.04) 0.96 (0.04) 0.96 (0.04) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
35 0.93 (0.06) 0.93 (0.06) 0.93 (0.06) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
40 0.92 (0.02) 0.92 (0.02) 0.92 (0.03) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
45 0.90 (0.06) 0.90 (0.06) 0.90 (0.06) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
50 0.93 (0.04) 0.93 (0.04) – 1.00 (0.01) 1.00 (0.01) –
1 20 0.89 (0.07) 0.89 (0.07) 0.89 (0.07) 1.00 (0.00) 0.99 (0.02) 0.99 (0.02)
25 0.94 (0.04) 0.93 (0.03) 0.93 (0.03) 0.99 (0.02) 0.99 (0.02) 0.99 (0.02)
30 0.94 (0.08) 0.94 (0.08) 0.94 (0.08) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
35 0.91 (0.06) 0.91 (0.06) 0.91 (0.06) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
40 0.93 (0.04) 0.93 (0.04) 0.93 (0.04) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
45 0.90 (0.03) 0.90 (0.03) 0.90 (0.03) 1.00 (0.01) 1.00 (0.01) 1.00 (0.01)
50 0.94 (0.02) 0.94 (0.03) – 1.00 (0.00) 1.00 (0.00) –
5 20 0.92 (0.07) 0.92 (0.07) 0.92 (0.07) 0.99 (0.02) 0.99 (0.02) 0.99 (0.02)
25 0.92 (0.05) 0.92 (0.05) 0.92 (0.05) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
30 0.87 (0.04) 0.87 (0.03) 0.87 (0.03) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
35 0.94 (0.04) 0.93 (0.03) 0.93 (0.03) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
40 0.94 (0.04) 0.94 (0.04) 0.94 (0.04) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01)
45 0.92 (0.06) 0.92 (0.06) 0.92 (0.06) 1.00 (0.01) 1.00 (0.01) 1.00 (0.01)
50 0.94 (0.02) 0.93 (0.03) – 0.99 (0.01) 0.99 (0.01) –
22Table C.3: Simulation without multicollinearity: Comparison of diﬀerent s olvers and com-
plete enumeration for logistic regression; average execution times in seconds.
Logistic model training time
AIC BIC
Proposed Method Proposed Method Proposed Method Proposed M ethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 0.896 2.329 0.081 0.878 2.188 0.081
10 1.971 4.244 2.522 2.035 4.188 2.510
15 4.553 7.220 94.975 3.076 5.809 95.105
20 12.635 19.999 – 5.051 11.751 –
25 21.486 29.610 – 7.755 13.668 –
30 27.868 38.201 – 11.001 14.549 –
35 77.166 105.110 – 21.279 25.351 –
40 47.305 91.766 – 33.676 28.208 –
Table C.4: Simulation without multicollinearity: Comparison of diﬀerent s olvers and com-
plete enumeration for logistic regression; average (sd) selection a ccuracy.
Logistic model support recovery accuracy
AIC BIC
Proposed Method Proposed Method Proposed Method Proposed M ethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
10 0.94 (0.06) 0.94 (0.06) 0.94 (0.06) 0.98 (0.04) 0.98 (0.04) 0.98 (0.04)
15 0.95 (0.03) 0.95 (0.03) 0.95 (0.03) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
20 0.93 (0.03) 0.93 (0.03) – 1.00 (0.00) 1.00 (0.00) –
25 0.92 (0.07) 0.92 (0.07) – 0.99 (0.02) 0.99 (0.02) –
30 0.95 (0.04) 0.95 (0.04) – 1.00 (0.00) 1.00 (0.00) –
35 0.95 (0.02) 0.95 (0.02) – 1.00 (0.00) 1.00 (0.00) –
40 0.95 (0.02) 0.95 (0.02) – 0.99 (0.01) 0.99 (0.01) –
23Table C.5: Simulation without multicollinearity: Comparison of diﬀerent s olvers and com-
plete enumeration for Poisson regression; average execution time s in seconds.
Poisson model training time
AIC BIC
Proposed Method Proposed Method Proposed Method Proposed M ethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 1.962 1.290 0.141 1.868 1.229 0.141
10 6.847 3.052 3.610 5.667 3.061 3.619
15 13.111 5.903 141.839 12.000 5.562 141.700
20 35.552 11.577 – 34.650 9.628 –
25 238.299 18.425 – 220.857 12.383 –
30 896.185 49.401 – 952.785 20.371 –
35 2408.308 51.174 – 2346.494 33.675 –
40 4030.605 215.312 – 3354.529 35.426 –
Table C.6: Simulation without multicollinearity: Comparison of diﬀerent s olvers and com-
plete enumeration for Poisson regression; average (sd) selection accuracy.
Poisson model support recovery accuracy
AIC BIC
Proposed Method Proposed Method Proposed Method Proposed M ethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 0.96 (0.09) 0.96 (0.09) 0.96 (0.09) 0.92 (0.18) 1.00 (0.00) 1.00 (0.00)
10 0.92 (0.04) 0.92 (0.04) 0.92 (0.04) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
15 0.87 (0.09) 0.87 (0.09) 0.87 (0.09) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
20 0.97 (0.07) 0.94 (0.06) – 1.00 (0.00) 1.00 (0.00) –
25 0.90 (0.21) 0.95 (0.04) – 0.90 (0.21) 1.00 (0.00) –
30 0.87 (0.20) 0.94 (0.06) – 0.90 (0.19) 1.00 (0.00) –
35 0.81 (0.22) 0.93 (0.04) – 0.82 (0.23) 0.99 (0.02) –
40 0.67 (0.26) 0.94 (0.03) – 0.78 (0.28) 0.99 (0.01) –
24Table C.7: Simulation with multicollinearity: Comparison of the impact of d iﬀerent
pairwise correlation constraints, for diﬀerent levels of correlation , on the MSE for the
logistic regression model.
Logistic model mean-squared error
IC alpha no constraint at most one
constraintsign
coherence 0.5
constraintsign
coherence 0.7
constraintcombined
constraint
AIC 0.70 1.41e-01 1.41e-01 1.41e-01 1.41e-01 1.40e-01
AIC 0.80 1.96e-01 2.20e-01 1.96e-01 1.96e-01 1.55e-01
AIC 0.90 3.13e-01 4.94e-01 3.07e-01 3.07e-01 3.07e-02
AIC 0.95 5.18e-01 5.66e-01 4.90e-01 4.90e-01 2.95e-02
AIC 0.99 1.21e+00 6.20e-01 6.20e-01 6.20e-01 2.89e-02
BIC 0.70 2.10e-01 2.10e-01 2.10e-01 2.10e-01 2.10e-01
BIC 0.80 2.75e-01 2.89e-01 2.75e-01 2.75e-01 2.22e-01
BIC 0.90 4.50e-01 4.94e-01 4.50e-01 4.50e-01 3.10e-02
BIC 0.95 5.67e-01 5.66e-01 5.64e-01 5.64e-01 2.96e-02
BIC 0.99 7.68e-01 6.20e-01 6.20e-01 6.20e-01 2.89e-02
Table C.8: Simulation with multicollinearity: Comparison of the impact of d iﬀerent
pairwise correlation constraints, for diﬀerent levels of correlation , on the MSE for the
Poisson regression model.
Poisson model mean-squared error
IC alpha no constraint at most one
constraintsign
coherence 0.5
constraintsign
coherence 0.7
constraintcombined
constraint
AIC 0.70 8.98e-03 9.50e-03 8.98e-03 8.98e-03 8.97e-03
AIC 0.80 9.54e-03 5.81e-02 9.54e-03 9.54e-03 8.51e-03
AIC 0.90 1.36e-02 3.93e-01 1.36e-02 1.36e-02 4.13e-03
AIC 0.95 2.71e-02 4.42e-01 2.71e-02 2.71e-02 3.11e-03
AIC 0.99 1.58e-01 4.95e-01 1.57e-01 1.57e-01 5.77e-03
BIC 0.70 8.98e-03 9.50e-03 8.98e-03 8.98e-03 8.97e-03
BIC 0.80 9.66e-03 5.83e-02 9.66e-03 9.66e-03 8.56e-03
BIC 0.90 1.59e-02 3.93e-01 1.59e-02 1.59e-02 4.70e-03
BIC 0.95 4.54e-02 4.42e-01 4.54e-02 4.54e-02 3.11e-03
BIC 0.99 2.42e-01 4.95e-01 2.41e-01 2.41e-01 4.98e-03
25Table C.9: Simulation with multicollinearity: Comparison of the impact of d iﬀerent
pairwise correlation constraints, for diﬀerent levels of correlation , on the MSE for the
linear regression model.
Linear model mean-squared error
alpha sd no constraint at most one
constraintsign
coherence 0.5
constraintsign
coherence 0.7
constraintcombined
constraint
0.70 0.01 1.46e-06 5.04e-04 1.46e-06 1.46e-06 1.57e-06
0.70 0.10 1.46e-04 6.45e-04 1.46e-04 1.46e-04 1.45e-04
0.70 0.20 5.84e-04 1.08e-03 5.84e-04 5.84e-04 5.81e-04
0.70 0.30 1.31e-03 1.81e-03 1.31e-03 1.31e-03 1.31e-03
0.70 0.40 2.33e-03 2.83e-03 2.33e-03 2.33e-03 2.32e-03
0.70 0.50 3.65e-03 4.14e-03 3.65e-03 3.65e-03 3.63e-03
0.70 1.00 1.48e-02 1.52e-02 1.46e-02 1.48e-02 1.46e-02
0.80 0.01 1.90e-06 4.71e-02 1.90e-06 1.90e-06 1.18e-05
0.80 0.10 1.90e-04 4.72e-02 1.90e-04 1.90e-04 1.57e-04
0.80 0.20 7.59e-04 4.79e-02 7.59e-04 7.59e-04 5.97e-04
0.80 0.30 1.71e-03 4.88e-02 1.71e-03 1.71e-03 1.33e-03
0.80 0.40 3.04e-03 5.02e-02 3.04e-03 3.04e-03 2.36e-03
0.80 0.50 4.75e-03 5.18e-02 4.75e-03 4.75e-03 3.67e-03
0.80 1.00 1.98e-02 6.63e-02 1.90e-02 1.94e-02 1.47e-02
0.90 0.01 3.27e-06 3.86e-01 3.27e-06 3.27e-06 1.69e-05
0.90 0.10 3.27e-04 3.87e-01 3.27e-04 3.27e-04 5.07e-05
0.90 0.20 1.31e-03 3.88e-01 1.31e-03 1.31e-03 1.53e-04
0.90 0.30 2.94e-03 3.90e-01 2.94e-03 2.94e-03 3.24e-04
0.90 0.40 5.23e-03 3.92e-01 5.23e-03 5.23e-03 5.63e-04
0.90 0.50 8.17e-03 3.95e-01 8.17e-03 8.17e-03 8.70e-04
0.90 1.00 4.52e-02 4.09e-01 3.25e-02 3.25e-02 3.43e-03
0.95 0.01 6.04e-06 4.39e-01 6.04e-06 6.04e-06 3.86e-06
0.95 0.10 6.04e-04 4.40e-01 6.04e-04 6.04e-04 3.70e-05
0.95 0.20 2.42e-03 4.41e-01 2.42e-03 2.42e-03 1.38e-04
0.95 0.30 5.44e-03 4.43e-01 5.44e-03 5.44e-03 3.05e-04
0.95 0.40 9.67e-03 4.44e-01 9.67e-03 9.67e-03 5.40e-04
0.95 0.50 1.55e-02 4.46e-01 1.51e-02 1.51e-02 8.42e-04
0.95 1.00 1.05e-01 4.57e-01 5.85e-02 5.85e-02 3.36e-03
0.99 0.01 2.83e-05 4.87e-01 2.83e-05 2.83e-05 4.56e-07
0.99 0.10 2.83e-03 4.88e-01 2.83e-03 2.83e-03 3.32e-05
0.99 0.20 1.14e-02 4.88e-01 1.13e-02 1.13e-02 1.32e-04
0.99 0.30 3.25e-02 4.89e-01 2.54e-02 2.54e-02 2.98e-04
0.99 0.40 7.63e-02 4.91e-01 4.43e-02 4.43e-02 5.29e-04
0.99 0.50 1.26e-01 4.92e-01 6.75e-02 6.75e-02 8.27e-04
0.99 1.00 4.04e-01 4.99e-01 2.10e-01 2.10e-01 3.31e-03
26References
[1] A. Miller, Subset Selection in Regression, CRC Press, 2002.
doi:10.1201/9781420035933 .
[2] B. K. Natarajan, Sparse approximate solutions to linear system s, SIAM
journal on computing 24 (1995) 227–234.
[3] D.Bertsimas, A.King,R.Mazumder, Bestsubsetselectionviaam odern
optimization lens, The annals of statistics 44 (2016) 813–852.
[4] D. Bertsimas, A. King, Logistic Regression: From Art
to Science, Statistical Science 32 (2017) 367–384. URL:
https://projecteuclid.org/journals/statistical-scie nce/volume-32/issue-3/Logis 
doi:10.1214/16-STS602 .
[5] H. Hazimeh, R. Mazumder, Fast best subset selection: Coordina te de-
scent and local combinatorial optimization algorithms, Operations R e-
search 68 (2020) 1517–1537.
[6] I. Guyon, A. Elisseeﬀ, An introduction to variable and feature se lection,
Journal of machine learning research 3 (2003) 1157–1182.
[7] R. Miyashiro, Y. Takano, Subset selection by mallows’ cp: A mixed
integer programming approach, Expert Systems with Applications 4 2
(2015) 325–331.
[8] M. Hofmann, C. Gatu, E. J. Kontoghiorghes, A. Colubi, A. Zeileis,
lmsubsets: Exact variable-subset selection in linear regression
for r, Journal of Statistical Software 93 (2020) 1–21. URL:
https://www.jstatsoft.org/index.php/jss/article/vie w/v093i03 .
doi:10.18637/jss.v093.i03 .
[9] T. Sato, Y. Takano, R. Miyashiro, A. Yoshise, Feature sub-
set selection for logistic regression via mixed integer optimiza-
tion, Computational Optimization and Applications 64 (2016)
865–880. URL: https://doi.org/10.1007/s10589-016-9832-2 .
doi:10.1007/s10589-016-9832-2 .
[10] H. Saishu, K. Kudo, Y. Takano, Sparse poisson regres-
sion via mixed-integer optimization, PLOS ONE 16 (2021).
doi:10.1371/journal.pone.0249916 .
27[11] J. C. Debuse, V. J. Rayward-Smith, Feature subset selection within
a simulated annealing data mining algorithm, Journal of Intelligent
Information Systems 9 (1997) 57–81.
[12] J. Yang, V. Honavar, Feature subset selection using a genetic algorithm,
IEEE Intelligent Systems and their Applications 13 (1998) 44–49.
[13] V. Calcagno, C. de Mazancourt, glmulti: An r package for
easy automated model selection with (generalized) linear mod-
els, Journal of Statistical Software 34 (2010) 1–29. URL:
https://www.jstatsoft.org/index.php/jss/article/vie w/v034i12 .
doi:10.18637/jss.v034.i12 .
[14] Y. Chen, D. Miao, R. Wang, A rough set approach to feature se lection
based onantcolonyoptimization, PatternRecognitionLetters31( 2010)
226–233.
[15] X. Wang, J. Yang, X. Teng, W. Xia, R. Jensen, Feature selectio n based
on rough sets and particle swarm optimization, Pattern recognition
letters 28 (2007) 459–471.
[16] B. Schwendinger, F. Schwendinger, L. Vana, Holistic generalize d lin-
ear models, arXiv (2022). URL: https://arxiv.org/abs/2205.15447 .
doi:10.48550/ARXIV.2205.15447 .
[17] F. Schwendinger, B. Gr¨ un, K. Hornik, A comparison of
optimization solvers for log-binomial regression including conic
programming, Computational Statistics 36 (2021) 1721–1754.
doi:10.1007/s00180-021-01084-5 .
[18] E. Carrizosa, A. V. Olivares-Nadal, P. Ram´ ırez-Cobo, Intege r con-
straints for enhancing interpretability in linear regression, SORT-
Statistics and Operations Research Transactions (2020) 67–98.
[19] A. E. Hoerl, R. W. Kennard, Ridge regression: Biased estimation for
nonorthogonal problems, Technometrics 12 (1970) 55–67.
[20] H. D. Bondell, B. J. Reich, Simultaneous regression shrinkage, v ariable
selection, and supervised clustering of predictors with oscar, Biom etrics
64 (2008) 115–123.
28[21] K. Konis, Linear programming algorithms for detecting separat ed data
in binary logistic regression models, Ph.D. thesis, University of Oxfor d,
2007.
[22] J. A. Nelder, R. W. Wedderburn, Generalized linear models, Jour nal of
the Royal Statistical Society: Series A (General) 135 (1972) 370– 384.
[23] S. Boyd, L. Vandenberghe, Convex optimization, Cambridge un iversity
press, 2004.
[24] H. Zou, T. Hastie, Regularization and variable selection via the ela stic
net, Journal of the royal statistical society: series B (statistic al method-
ology) 67 (2005) 301–320.
[25] R. Tibshirani, Regression shrinkage and selection via the lasso, J ournal
of the Royal Statistical Society: Series B (Methodological) 58 (199 6)
267–288.
[26] T. Hastie, R. Tibshirani, R. Tibshirani, Best subset, forward st epwise or
lasso? analysis and recommendations based on extensive compariso ns,
Statistical Science 35 (2020) 579–592.
[27] Y. Yang, C. S. McMahan, Y.-B. Wang, Y. Ouyang, Estimation of l0
norm penalized models: A statistical treatment, Computational St atis-
tics & Data Analysis 192 (2024) 107902.
[28] J. D. Camm, A. S. Raturi, S. Tsubakitani, Cut-
ting big m down to size, Interfaces 20 (1990) 61–
66. URL: https://doi.org/10.1287/inte.20.5.61 .
doi:10.1287/inte.20.5.61 .
[29] H. Akaike, A new look at the statistical model identiﬁcation, IEE E
transactions on automatic control 19 (1974) 716–723.
[30] G. Schwarz, Estimating the dimension of a model, The annals of st atis-
tics (1978) 461–464.
[31] C. Mallows, Choosing variables in a linear regression: A graphical a id,
in: Central RegionalMeeting oftheInstituteofMathematical Sta tistics,
Manhattan, KS, 1964, 1964.
29[32] A. Boisbunon, S. Canu, D. Fourdrinier, W. Strawderman, M. T.
Wells, Akaike’s information criterion, cpand estimators of loss for
elliptically symmetric distributions, International Statistical Review
82 (2014) 422–439. URL: http://www.jstor.org/stable/43299006 .
doi:https://doi.org/10.1111/insr.12052 .
[33] A. Albert, J. A. Anderson, On the existence of maximum likelihood
estimates in logistic regression models, Biometrika 71 (1984) 1–10.
[34] K. Konis, K. Fokianos, Safe density ratio modeling, Statistics & P rob-
ability Letters 79 (2009) 1915–1920. doi: 10.1016/j.spl.2009.05.020 .
[35] D. Bertsimas, A. King, An algorithmic approach to linear regress ion,
Operations Research 64 (2015) 2–16. doi: 10.1287/opre.2015.1436 .
[36] A. Domahidi, E. Chu, S. Boyd, ECOS: An SOCP solver for embedde d
systems, in: European Control Conference (ECC), 2013, pp. 30 71–3076.
doi:10.23919/ECC.2013.6669541 .
[37] M. ApS, The MOSEK Rmosek package manual. Version 10.0.34, 202 2.
URL:https://docs.mosek.com/latest/rmosek/index.html .
[38] Gurobi Optimization, LLC, Gurobi Optimizer Reference Manual, 2023.
URL:https://www.gurobi.com .
[39] M. Hofmann, C. Gatu, E. J. Kontoghiorghes, A. Colubi, A. Zeileis ,
lmSubsets: Exact Variable-Subset Selection in Linear Regression, 2 021.
URL:https://CRAN.R-project.org/package=lmSubsets , r package
version 0.5-2.
[40] A. McLeod, C. Xu, Y. Lai, bestglm: Best Sub-
set GLM and Regression Utilities, 2020. URL:
https://CRAN.R-project.org/package=bestglm , r package ver-
sion 0.37.3.
[41] J. Zhu, X. Wang, L. Hu, J. Huang, K. Jiang, Y. Zhang, S. Lin, J. Zhu,
abess: a fast best-subset selection library in python and r, The Jo urnal
of Machine Learning Research 23 (2022) 9206–9212.
[42] G. C. McDonald, D. I. Galarneau, A monte carlo evaluation of som e
ridge-type estimators, Journal of the American Statistical Asso ciation
70 (1975) 407–416. URL: http://www.jstor.org/stable/2285832 .
30