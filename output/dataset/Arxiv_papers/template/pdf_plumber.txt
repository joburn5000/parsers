AUTOMATICALLY EXTRACTING PARTIAL DIFFERENTIAL
EQUATIONS FROM DATA
APREPRINT
WeizhenLi RuiCarvalho
DepartmentofEngineering DepartmentofEngineering
DurhamUniversity DurhamUniversity
Durham,UK,DH13HN Durham,UK,DH13HN
weizhen.li@dur.ac.uk rui.carvalho@dur.ac.uk
ABSTRACT
Identifyingpartialdifferentialequations(PDEs)fromdataiscrucialforunderstandingthegoverning
mechanismsofnaturalphenomena,yetitremainsachallengingtask. Wepresentanextensiontothe
ARGOSframework,ARGOS-RAL,whichleveragessparseregressionwiththerecurrentadaptive
lassotoidentifyPDEsfromlimitedpriorknowledgeautomatically.Ourmethodautomatescalculating
partialderivatives,constructingacandidatelibrary,andestimatingasparsemodel. Werigorously
evaluatetheperformanceofARGOS-RALinidentifyingcanonicalPDEsundervariousnoiselevels
andsamplesizes,demonstratingitsrobustnessinhandlingnoisyandnon-uniformlydistributeddata.
Wealsotestthealgorithm’sperformanceondatasetsconsistingsolelyofrandomnoisetosimulate
scenarioswithseverelycompromiseddataquality. OurresultsshowthatARGOS-RALeffectively
andreliablyidentifiestheunderlyingPDEsfromdata,outperformingthesequentialthresholdridge
regressionmethodinmostcases.Wehighlightthepotentialofcombiningstatisticalmethods,machine
learning,anddynamicalsystemstheorytoautomaticallydiscovergoverningequationsfromcollected
data,streamliningthescientificmodelingprocess.
Keywords Systemidentification·Machinelearning·Sparseregression·Partialdifferentialequations·Nonlinear
dynamics
1 Introduction
Inrecentyears,scientistshaveincreasinglyemployedstatisticalandmachinelearningmethodstouncoverthegoverning
equationsofdynamicalsystems,particularlydifferentialequations,fromobservationaldata[1–5]. Data-drivenmethods
offerseveraladvantagesovertraditionalapproachesthatrelyonfirstprinciplesandexpertknowledge. Thesemethods
canrevealpatternsandrelationshipsinthedatathatmaynotbeapparentfromfirstprinciples,providingnewinsights
intocomplexsystems[6,7]. Theyarealsoadeptatworkingwithnoisyorincompletedatacommonlyencounteredin
real-worldapplications,employingtechniquesfrommachinelearningtoenhancetherobustnessofdiscoveries[8–11].
Furthermore,byreducingtheneedformanualinterventionanddomainexpertise,data-drivenmethodscansignificantly
streamlinethediscoveryprocess[12].
Data-drivendiscoveryindynamicalsystemshasevolvedfromearlyparameterestimationusingsplineapproximation
and system reconstruction [13, 14], to leveraging statistical methods such as least squares [15–17], mixed-effects
models[18,19],andBayesianapproaches[2,20]forparameterestimationinODEsandPDEs. Theadventofhigh-
performancecomputinghasfurtherpropelledsymbolicregression,enablingthediscoveryofgoverningequationsfrom
datainphysicsandengineering[1,21–23]. AnotabledevelopmentinthisfieldistheSparseIdentificationofNonlinear
Dynamics(SINDy)approach[3,4],whichconstructsanextensivelibraryofpotentialtermsandemploystheSequential
ThresholdRidgeRegression(STRidge)algorithm[4]toselectsignificanttermsiteratively.
SINDyanditsvariousenhancements[24–29]havebeenextensivelyusedtodiscoverabroadspectrumofODEsand
PDEs,describingdiversephenomenasuchasfluidmechanics[30],turbulencemodels[31],aerodynamics[32],and
4202
rpA
52
]GL.sc[
1v44461.4042:viXraarXivTemplate APREPRINT
biologicalandchemicalsystems[33,34]. Recentdevelopmentshavecombinedneuralnetwork-basedtechniquesand
SINDy, leading to innovative approachesthat enhance noisetolerance inidentifying PDEs[5, 23, 35–39]. Neural
networkscanlearncomplexnonlinearrelationshipsandeffectivelyfilteroutnoise,complementingSINDy’sabilityto
identifyparsimoniousmodels. However,bothneuralnetworkandSINDymethodsrequirespecifichyperparameter
tuning,suchassettingregularizationparametersorchoosingnetworkarchitectures. Forexample,STRidgerequires
settingathresholdtoselectactivetermsfromthecandidatelibrary[4,29,36,38]. Additionally,SINDy-basedmethods
typicallyapproximatenumericalderivativesfromnoisydatausingtheSavitzky-Golayfilter,atechniqueforsmoothing
databyfittinglocallow-degreepolynomials[40]. Theparametersofthisfilter,suchasthepolynomialdegreeand
windowsize,mustbecarefullytunedforoptimalperformance[4,12]. Neuralnetworkapproaches,ontheotherhand,
requiredetaileddecisionsregardingtheirarchitectureandfunctioning,suchasthenumberofneurons,thestructureof
hiddenlayers,thetypesofactivationandlossfunctions,andthelearningrate. Inparticular,usingphysics-informed
neural networks [5, 36, 38] requires a prior understanding of the equation terms, as well as initial and boundary
conditions. Consequently,usingneuralnetworksandSINDy-basedmethodspresentsatrade-off: theabsenceoffully
automatedalgorithmsrequiresuserstoengageinmanualtuninganditerativeusageofsemi-automatedalgorithms. This
scenariohighlightsakeychallengeinthefield: developinganautomatedalgorithmtoidentifyPDEswithminimal
manualintervention,streamliningtheprocess,andimprovingitsapplicabilityacrossdiversescientificdomains.
To address the challenge of parameter tuning, Egan et al. [12] proposed the Automatic Regression for Governing
Equations(ARGOS)algorithm,whichidentifiesODEsbyautomatingtheparametertuningprocess. ARGOSassumes
theunderlyingsystemisunknown,automatesthefine-tuningofparametersfornumericaldifferentiation,andleverages
sparseregressionwithbootstrapconfidenceintervalstoselectactivetermsfromthecandidatelibrary. Toautomatically
identifyPDEs,wedevelopARGOSwiththeRecurrentAdaptiveLasso(ARGOS-RAL).ThisextensionoftheARGOS
frameworkemploysonlysparseregressiontoidentifyequationsratherthanengaginginlarge-scalebootstrapping.
WeevaluatetheperformanceoftheARGOS-RALalgorithmthroughaseriesofthreenumericaltests,eachdesigned
toassessitsabilitytoidentifycanonicalPDEsacrossdiversefields,includingbiology,neuroscience,earthscience,
fluidmechanics,andquantummechanics. Thefirsttestexploresthealgorithm’sresilienceagainstvaryingnoiselevels
by altering the signal-to-noise ratio (SNR) in Gaussian random noise integrated into the PDE solutions, which is
crucialforunderstandingtherobustnessofARGOS-RALunderrealisticnoisyconditions. Thesecondtestaddresses
thepracticalchallengesencounteredinreal-worlddatacollection,whichoftenresultsinnon-uniformlydistributed
datapointsinspace andtime, by exploringtheminimumpercentageof datapointsnecessaryforthealgorithmto
accuratelyidentifytheunderlyingequation. Thefinalevaluationassessesthealgorithm’sabilitytoprocessdatasets
characterizedbysignificantnoise,challengingitslimitsandpracticalapplicabilityinscenarioswheredataqualityis
compromised. OurresultsdemonstratethatARGOS-RALcaneffectivelyandreliablyidentifytheunderlyingPDEs
fromdata,outperformingtheSTRidgemethodusedinSINDy.
2 Methods
2.1 OverviewoftheARGOS-RALFramework
ThegeneralformofahomogeneousPDEis
u +F(x,t,u,u ,u ,···)=0 (1)
t x xx
where F(·) governs the behavior of the system, with u = u(x,t) denoting its state. The notation u ,u ,u ,···
t x xx
representsthepartialderivativesofuwithrespecttotimeandspace,respectively. Equation(1)servesasafoundational
representationofthedynamicalsystem,encapsulatingawiderangeofphenomenathroughitsgeneralizedform,which
canbeadaptedtoincludemultiplespatialdimensionsortomodelsystemswithoutexplicittimedependence.
Tofocusondata-drivenmodelingofspatiotemporaldynamicalsystems,weincorporateempiricaldatadirectlyintothe
modelingprocess:
∂U
U = =F(x,U,U ,U ,...), (2)
t ∂t x xx
where U ∈ Rn×m is a matrix representing the solution of the PDE as a function of x and t, and F(·) denotes the
unknownmappinginferredfromthecollecteddata,whichcontainslinearandnonlinearoperators.
We aim to estimate the unknown mapping F(·) with sparse regression by constructing a comprehensive library of
potential terms and assuming that only a few of them are active [3, 4, 12]. To cover a broad spectrum of possible
influencesonthedynamicsofthesystem,thislibraryincludesawidevarietyoffunctions,suchasconstants,monomials,
interactionterms(productsofvariables),possiblytrigonometric,andotherfunctions,dependingonthedynamical
systembeingstudied[4]. InthecaseofBurgers’equation,u =−uu +0.1u ,thetruedynamicsinvolvesonlytwo
t x xx
2arXivTemplate APREPRINT
terms: thenonlinearconvectiontermuu andthelineardiffusiontermu . Whenapplyingsparseregressiontodata
x xx
generatedfromBurgers’equation,themethodshouldideallyselectonlythesetwotermsfromthecandidatelibrary.
AllfeaturesrelatedtoU(x,t)inEq.(2)arematrices. Implementingthismatrixdatainsparseregressionleadstothe
creationofmdistinctregressionmodels. Eachmodelcapturesthespatialdynamicsofthesystemataspecifictime
pointt ,wherej =1,2,...,m. Toconsolidatethemregressionmodelsintoasinglelinearregressionproblem,we
j
reshapethematrixU(x,t)anditsderivativematricesintovectors. Thesevectorsthenserveaspredictorswithinthe
candidatelibraryΘ,whichcanberepresentedinR(n·m)×porC(n·m)×p. Bystackingthevectorizeddataandcandidate
terms,wecanestimateasinglesparsecoefficientvectorβ thatrepresentsthegoverningequationacrossalltimepoints
ratherthanestimatingseparatemodelsforeachtimepoint. Here,U∈Rn×misrepresentedinmatrixformas
 u(x ,t ) u(x ,t ) ··· u(x ,t ) 
1 1 1 2 1 m
u(x ,t ) u(x ,t ) ··· u(x ,t )
 2 1 2 2 2 m 
U(x,t)=  . . . . . . ... . . .  . (3)
u(x ,t ) u(x ,t ) ··· u(x ,t )
n 1 n 2 n m
VectorizingEq.(3)yields:
u=vec(U)=( u(x ,t ) ··· u(x ,t ) ··· u(x ,t ) ··· u(x ,t ) )T . (4)
1 1 n 1 1 m n m
Similarly, u = vec(U ) = vec(∂U/∂t), u = vec(U ) = vec(∂U/∂x), u = vec(U ) = vec(∂2U/∂x2),
t t x x xx xx
u2 =vec(U⊙U),anduu =vec(U⊙U ),where⊙denotestheHadamardproduct. Thedesignmatrixisgivenby
x x
 
Θ(u)= 1 u ··· ud ··· u
x
u
xx
··· uu
x
··· udu
xx
··· , (5)
where ud is a vector where all elements denote a d-th degree monomial. For example, if our data U(x,t) is on
a 200×100 grid (i.e. 200 spatial measurements and 100 time-steps) and the candidate library has 30 terms, then
Θ∈R20000×30.
Aftervectorization,weestimateF(·)bytransformingEq.(2)toalinearregressionmodel
u =Θ(u)β+ϵ, (6)
t
whereβ ∈Rpisasparsecoefficientvectorinwhichonlyafewvaluesarenonzero,andϵisthevectorofresiduals.
2.2 AutomatedNumericalDifferentiationusingtheSavitzky-GolayFilterandtheGaussianBlur
AcrucialstepinconstructingthecandidatelibraryinEq.(5)isthenumericalcalculationofderivatives(seeFig.1A
andB).TheSavitzky-Golayfilter[40]hasbecomeafavoredsolutioninsystemidentificationforsignalsmoothingand
differentiation[4,41]. Thismethodappliesaleastsquarespolynomialfitoveraslidingwindowofdatapoints,thereby
achievingsimultaneoussignalsmoothinganddifferentiation. TheselectionoftheSavitzky-Golayfilterisgroundedin
itsprovenabilitytoaccuratelymaintaintheoriginalcontourofthesignalwhilesignificantlyreducingnoiseandto
approximatehigher-ordernumericalderivativeswithsymbolicdifferentiation[42].
TheSavitzky-Golayfilterischaracterizedbytwointegerhyperparameters:thepolynomialorderoandthewindowlength
l,whichareconstrainedbytheconditionsthatomustbeatleast2,lshouldbeanoddnumber,ando+1+mod(o)≤
l ≤ n−1[42]. Toautomatetheselectionofthesehyperparameters,wefirstapplyaGaussianblurwiththekernel
(1,2,1)tosmooththeobservationaldata(seeA.1). Wethentreatthissmootheddata,denotedasGB(U˜),astheground
truth. Next,wefindtheoptimalsetofhyperparameters{o∗,l∗}byminimizingthemeansquarederror(MSE)between
theSavitzky-GolayfiltereddataSG(U˜,o,l)andthegroundtruthGB(U˜)(seeAlgorithm1inA.2). Afterfindingthe
optimal set {o∗,l∗}, we use the Savitzky-Golay filter with these parameters to compute the smoothed data and its
derivatives.
2.3 SparseRegressionwiththeRecurrentAdaptiveLasso
Theadaptivelassoisatwo-stepmethod[12,43]. Thefirststepusestheordinaryleastsquares(OLS)toobtainunbiased
estimatesandderivetheweightsw:
w =|βˆ |−γ, γ >0 (7)
ols
3arXivTemplate APREPRINT
A
Smoothing and Derivatives
Savitzky-Golay filter
Observed data Filtered points
Savitzky-Golay filter Local polynomial fitted line Boundary points
Two-dimensional Gaussianblur
Vectorize
2 222 8 0 000 11 1 14 66 6 2 2 2 22 8 0 0 00 33 1 3 3 22 2 2 6 2 2 22 8 0 0 00 1 2 1 2 4 2 1 2 1 F1 11 i5 72 lter1 21 e9 25
d
d1 2 2 a8 3 7
ta
Raw data Gaussian kernel
B
Constructing the Candidate Library
C
The Recurrent Adaptive Lasso Algorithm
Figure1: ProcessofidentifyingPDEsfromdatausingARGOSwiththerecurrentadaptivelasso. Theidentification
processconsistsofthreemainsteps: (A)automaticsmoothingandcalculationofderivatives,(B)constructionofthe
candidate library, and (C) implementation of the recurrent adaptive lasso. We begin by collecting the data U˜ and
applyingtheautomaticSavitzky-GolayfilterwithGaussianblurtocalculatethesmoothedUanditspartialderivatives.
Next,wevectorizethesmootheddata,allpartialderivatives,andotherrelatedtermstoconstructthecandidatelibrary.
Finally,weemploytherecurrentadaptivelassotoidentifytheactivefeaturesinthelibrary,andweestimatetheunbiased
coefficientsoftheidentifiedmodelusingordinaryleastsquaresregression.
4arXivTemplate APREPRINT
whereβˆ istheOLSestimate,andγ isanexponenttuningtheshapeofthesoft-thresholdingfunction. Inthesecond
ols
step,weobtaintheestimatedcoefficientsβˆ usingtheglmnetpackage[44]inRbysolvingtheproblem
alasso
p
βˆ =argmin∥u −Θβ∥2+λ(cid:88) w |β |, (8)
alasso t 2 j j
β
j=1
whereλisanonnegativeregularizationparametercontrollingtheamountofshrinkageappliedtothecoefficientsofthe
predictors.Unlikethelasso,wheretheweightvectorisw =1,theadaptivelassovariestheweightsintheregularization
function,resultinginastrongerpenaltyonsmallercoefficients,thusdrivingmoreofthemtozeroandleadingtoa
sparsermodelcomparedtothestandardlasso. Therecurrentadaptivelassoappliestheadaptivelassorepeatedlyuntil
convergence,resultinginasparsemodelwithfewernon-zerocoefficients.
Tobalancethemodel’scomplexityagainstitsaccuracy,wedeterminetheregularizationparameterλbyemployingthe
Paretocurve,whichillustratestheoptimaltrade-offbetweentheregularizationpenaltyandthemodelresiduals[45–47]
(see Fig. 2). Although cross-validation is an alternative method, Cortiella et al. [27] have shown that it finds a λ
optimizedforprediction,potentiallyoverfittingthetrueunderlyingequationwithextrafeatures.
1
0
3.5 4.0 4.5 5.0 5.5
( )
b -
log ||X y||
10 2
)
b
T
(
||
w||
gol
1
01
Figure2: ParetocurveoftheadaptivelassoforasampleddatasetfromaNavier-StokessystemwithanSNRof36dB.
TheParetocurvebalancesthetrade-offbetweensparsityandgoodness-of-fit. Theredpointonthecurveindicatesthe
optimalvalueoftheregularizationparameterλthatachievesthebestbalancebetweenthesetwocompetingobjectives.
Increasingλleadstosparsersolutionsatthecostofapoorerfittothedata,whiledecreasingλimprovesthefitbut
yieldslesssparsesolutions.
Theadaptivelassoregressionoftendetectsmoretermsthanthoseinthetruesystem. Toimproveparsimony,Eganet
al.[12]suggestedcombiningtheadaptivelassowithbootstraptechniquestoidentifyODEs. Similarly,Cortiellaet
al.[27]adoptedamodifiedversionofthemulti-stepadaptivelasso[48]todevelopasparsermodelthatmoreaccurately
identifiesthetrueequations. Thisisachievedbyiterativelyadjustingtheadaptiveweightsusingpreviousestimatesfrom
theadaptivelasso. AsignificantadvancementmadebyCortiellaetal.[27]istheirmethod’sabilitytomaintainfinite
weightsintheadaptivelassoequationbyensuringthattheestimatedcoefficientsshrinktoasmall,nonzerovaluerather
thandroppingtozero. However,thisapproximationunintentionallyintroducesnumericalinaccuraciesasatrade-offfor
preventingoverflowduringtheequationidentificationprocess.
Therecurrentadaptivelassoisaniterativealgorithmthatestimatesaninitialsparsemodelusingtheadaptivelasso
and subsequently refining it by trimming the candidate library (see Fig. 1 C). At each iteration, it removes terms
whosecoefficientstheadaptivelassopenalizedtozero(seeA.2Algorithm2step9). Itthenemploysleastsquaresto
re-estimatethecoefficientsoftheremainingterms,whichareusedtoupdatetheadaptiveweightsinthenextadaptive
lassoiteration. Thisfocusestheregularizationonthetermsthathadsmallcoefficientsinthepreviousiteration. As
5arXivTemplate APREPRINT
thisprocessrepeats,therecurrentadaptivelassoincreasinglyconcentratestheℓ -normshrinkageontermsthatare
1
likely irrelevant, driving their coefficients to zero [43, 49]. Meanwhile, it relaxes the regularization on terms that
consistentlyhavelargercoefficients,allowingthemodeltoretainthem. Thecandidatesetgetssmallerateachiteration
untilthealgorithmconvergesonasparsemodelcontainingonlythekeyterms. Thisiterativere-weightingallowsthe
recurrentadaptivelassotopruneirrelevanttermsmoreaggressivelythanthestandardadaptivelassowhileretaining
goodpredictiveperformance. Theresultisaparsimoniousmodelthatidentifiesthetruegoverningequationmore
reliably,eveninthepresenceofmanyextraneouscandidateterms.
Increasingthenumberofiterationsmaycausetherecurrentadaptivelassotounderestimatethemodel. Thiscanlead
totheomissionofactivetermsthatshouldbeincludedinthetrueunderlyingequation. Therefore,whileiteratingthe
candidatelibraryΘ,werecordallcandidatemodelsandcalculatetheAkaikeinformationcriterion(AIC)foreach
modeltodeterminethefinalgoverningequationcorrespondingtothelowestAIC.Giventheuncertaintythatthetrue
modelfallswithinallcandidates,theAICservestoselectthemodelthatbestapproximatesthetruemodel[50,51].
3 ResultsandDiscussion
3.1 EvaluatingthePerformanceofARGOS-RALunderVaryingNoiseLevelsandSampleSizes
WecomparetheperformanceofARGOS-RALandSTRidge[4]inidentifyingtencanonicalPDEsundervariousSNRs
andsamplesizes(N). Weevaluatetheirperformanceonbothnoisyandnoiselessdata. Figure3demonstratesthe
impactofintroducingincreasinglevelsofGaussianrandomnoiseintothesolutionoftheBurgers’equation,effectively
decreasingtheSNRvalues.
In the evaluation of noise-contaminated data, we express the SNR as SNR = 20log (σ /σ ), where σ is the
10 U Z U
standarddeviationoftheoriginaldata,andσ representsthestandarddeviationoftheaddednoise. Wesystematically
Z
varyσ tospanabroadrangeofnoiselevels,facilitatingacomprehensiveevaluationoftheefficacyofARGOS-RAL
Z
andSTRidgeinidentifyingvariousPDEsunderdifferentnoiseconditions. Forthispurpose, wegeneratedatasets
withSNRssetat{0,2,··· ,58,60,∞}[12],eachcomprisingpairedelements{u ,Θ(u)}. Thisapproachallowsusto
t
examinetherobustnessofeachPDEidentificationmethodasitcopeswithvaryingnoiselevels.
In investigating sample size, N, our objective is to determine the smallest number of samples needed to reliably
identifyPDEswithasuccessrateexceeding80%. Toachievethis,wefirstgenerateafulldatasetforeachPDEby
calculatingpartialderivativesandassemblingacandidatelibraryasdescribedinEq.(5). Thesizeofthefulldataset,
denotedasN,variesdependingonthespecificPDEunderconsideration. Specifically,N = 104 fortheadvection-
diffusion, Burgers, and cable equations, N = 105 for the quantum harmonic oscillator, transport, Navier-Stokes,
and reaction-diffusion equations, and N = 104.8 for the heat and Korteweg-De Vries (KdV) equations. Next, we
randomlysamplesmallersubsetsofsizeN fromthefulldataset,whereN ischosenfromalogarithmicallyspaced
grid: N =102,102.2,102.4,··· ,N[12](seethebluepointsinFig.3A).ByapplyingthePDEidentificationmethods
tothesesubsetsandevaluatingtheirsuccessrates, wecandeterminethesmallestsamplesizerequiredforreliable
identificationofeachPDE.
3.2 QuantifyingSuccessRatesinIdentifyingCanonicalPDEs
ToevaluatetheimpactofdifferentSNRsanddatasizesonthemethod,wemeasuretheuncertaintyofmodelidentification
causedbyrandomsampling.Todoso,wecreate100uniquedatasetsateachpointonthegrid,correspondingtodifferent
SNRsandN values. Foreachdataset,wequantifytheidentificationaccuracywiththesuccessrate,η =#correct/100,
where#correctrepresentsthenumberoftimesthemodelcorrectlyidentifiesallactiveterms. Ouraccuracyassessment
ignoressmalldifferencesbetweentheoreticalandempiricalcoefficients,suchasatheoreticalvalueof0.1compared
to an estimated value of 0.098. Figure 4 illustrates these results for a selection of systems: the Burgers’, Cable,
Navier-Stokes,reaction-diffusion,andquantumharmonicoscillatormodels. Weprovidefurtheranalysisonadditional
PDEs–Transport,Heat,Advection-Diffusion,andKdVequations–inA.3Fig.6.
ARGOS-RALidentifiesBurgers’,cable,Navier-Stokes,reaction-diffusion,andadvection-diffusionequations,achieving
asuccessrateof100%whentheSNRexceeds30dB(seeFig.4and6).However,accuratelydetectingspecificequations
requiresahighSNR,particularlyforthequantumharmonicoscillator,KdV,transport,anddiffusionequations.TheKdV
equation,whichinvolvesthird-orderpartialderivatives,presentschallengesduetothesignificantbiasesinnumerical
approximationsofthesederivatives[39],resultingindatasetsunsuitableforsystemidentificationwithsparseregression.
Toimplementsparseregressionwithintherealnumberdomainforthecomplexnumberquantumharmonicoscillator
PDE,weapplythetransformationshowninA.3Eq.(17). ThistransformationexpandsthedesignmatrixΘfrom
nm×pto2nm×2p,effectivelyquadruplingitssizeandpotentiallyleadingtohighcorrelationsbetweenthevariables
inΘ.Thetransportanddiffusionequations,containingonlytermsu andu respectively,exhibithighcorrelationwith
x xx
6arXivTemplate APREPRINT
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
A
Burgers (SNR = ¥ )
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
B
Burgers (SNR = 40 dB)
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
C
Burgers (SNR = 30 dB)
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
D
Burgers (SNR = 20 dB)
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
E
Burgers (SNR = 10 dB)
10.0
7.5
5.0
2.5
0.0
−5 0 5
x
t
F
Burgers (SNR = 0 dB)
0.00 0.25 0.50 0.75 1.00
Figure3: InfluenceofSNRontheBurgers’equationdataset. (A)Noiselessdatapoints(blue)serveasareferencefor
evaluatingtheimpactofsamplesizeonPDEidentificationaccuracy. (B-F)Noisydatasetsaregeneratedbyadding
GaussiannoiseatSNRlevelsof40dB,30dB,20dB,10dBand0dB,respectively,tocomprehensivelycharacterize
thesystem’sbehaviorundervaryingnoiseconditions.
theircorrelatedtermsinthelibrary,suchas{u ,uu }and{u ,uu },whichhinderstheeffectivenessofℓ -norm
x x xx xx 1
shrinkageregressioninidentifyingcorrectterms[43].
Figures4and6illustratethatARGOS-RALachievesahighersuccessratethanSTRidgeinidentifyingPDEswith
limiteddatapoints. ARGOS-RALconsistentlyidentifiesasignificantnumberofPDEsusingasfewas1000datapoints,
maintainingasuccessrateabove80%. However,someequations,suchasthereaction-diffusionandKdVequations,
requirelargersamplesizesofapproximately104and103.8datapoints,respectively,forreliableidentification. Wethus
demonstrateARGOS-RALasaconsistentandefficientmethodforPDEidentificationwithnon-uniformlysampledand
noiselessdatasets.
ARGOS-RALshowsaremarkableabilitytoidentifyPDEsaccuratelyandconsistentlyacrossawiderangeofSNRs
andsamplesizes. ItssuccessrateimprovesastheSNRandsamplesizeincrease,reaching100%whenbothvalues
aresufficientlylarge. ThistrendhighlightstherobustnessofARGOS-RALinhandlingvariousdataconditionsand
underscoresitseffectivenessinidentifyingPDEs,evenwhenfacedwithvaryinglevelsofdataqualityandquantity.
However,incertainscenarios,STRidge[4]withspecificd thresholdsexceedstheperformanceofARGOS-RAL.For
tol
instance,STRidgeachieveshighersuccessratesinidentifyingNavier-Stokesandreaction-diffusionequationsata30
dBSNR,usingd settingsof2and10,respectively(seeFig.4CandD).Moreover,STRidgewithd =2ismore
tol tol
proficientinidentifyingthequantumharmonicoscillatorandthetransportequationwithanSNRlowerthan52dB,see
Fig.4Eand6C,respectively. TheseresultsfromtheSNRandN experimentsrevealthatusingasinglefixedthreshold
inSTRidgecanleadtoperformancevariabilitydependingontheinputdata,highlightingthedifficultyofselecting
anappropriated thresholdwithoutpriorknowledgeofthesystem. Thisvariabilityunderscoresthesensitivityof
tol
STRidgetospecificthresholdsettings,whichcanimpactitsconsistencyacrossdifferentdatasets. Overall,STRidge
surpassesARGOS-RALinidentifyingsimplerPDEs,suchasthetransportanddiffusionequations;seeFig.6CandD.
3.3 RobustnessAnalysisusingWhiteGaussianNoise
Tobetterunderstandthelimitsofidentificationalgorithms,wedesignedanextremetestonasinglespatialdimension.
Thistesteffectivelycreatesasituationwithoutvaliddatacollection(σ =0),equivalenttoanSNRofnegativeinfinity,
U
7