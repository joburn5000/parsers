4
2
0
2

r
p
A
5
2

]
L
M

.
t
a
t
s
[

1
v
7
8
2
6
1
.
4
0
4
2
:
v
i
X
r
a

Differentially Private Federated Learning:
Servers Trustworthiness, Estimation, and Statistical
Inference

Zhe Zhang∗

Ryumei Nakada†

Linjun Zhang‡

April 26, 2024

Abstract

Differentially private federated learning is crucial for maintaining privacy in dis-
tributed environments. This paper investigates the challenges of high-dimensional
estimation and inference under the constraints of differential privacy. First, we study
scenarios involving an untrusted central server, demonstrating the inherent difficul-
ties of accurate estimation in high-dimensional problems. Our findings indicate that
the tight minimax rates depends on the high-dimensionality of the data even with
sparsity assumptions. Second, we consider a scenario with a trusted central server
and introduce a novel federated estimation algorithm tailored for linear regression
models. This algorithm effectively handles the slight variations among models dis-
tributed across different machines. We also propose methods for statistical infer-
ence, including coordinate-wise confidence intervals for individual parameters and
strategies for simultaneous inference. Extensive simulation experiments support our
theoretical advances, underscoring the efficacy and reliability of our approaches.

1

Introduction

1.1 Overview

Federated learning is an efficient approach for training machine learning models on dis-
tributed networks, such as smartphones and wearable devices, without moving data to a
central server [26, 25, 30]. Since its proposal in [32], federated learning has gained sig-
nificant attention in both practical and theoretical machine learning communities. One
of the key attractions of federated learning is its ability to provide a certain level of
data privacy by keeping raw data on local machines. However, without specific design
choices, there are no formal privacy guarantees. To fully exploit the benefits of federated
learning, researchers have introduced the concept of differential privacy [1, 12, 13, 14, 15]

∗Rutgers University. Email: zzres0131@gmail.com.
†Rutgers University. Email: rn375@stat.rutgers.edu.
‡Rutgers University. Email: lz412@stat.rutgers.edu.

1

 
 
 
 
 
 
to quantify the exact privacy level in federated learning. A series of research papers
have focused on federated learning with differential privacy, applying various algorithms
and methods [23, 37, 39]. Despite these efforts, there remains a significant gap between
practical usage and statistical guarantees, particularly in the high-dimensional setting
with sparsity assumptions, where theoretical results for the optimal rate of convergence
and statistical inference results are largely missing.

In this paper, we focus on studying the estimation and inference problems in the
federated learning setting under differential privacy, particularly in the high-dimensional
regime. In federated learning, there are several local machines containing data sets from
different sources, and a central server to coordinate all local machines to train learning
models collaboratively. We present our key results in two major settings for privacy and
federated learning. In the first setting, we consider an untrusted central server [31, 39, 23]
where each machine sends only privatized information to the central server. For example,
when using smartphones, where users may not fully trust the server and do not want their
personal information to be directly updated on the remote central server. In the second
setting, we consider a trusted central server where each machine sends raw information
without making it private. [33, 19, 34] For example, in different hospitals, patient data
may not be shared among hospitals to protect patient privacy, but they can all report
their data to a central server, such as a non-profit organization or an institute, to gain
more information and publish statistics on certain diseases.

In the first part of our paper, we demonstrate that under the assumption that
the central server is untrusted, the optimal rate of convergence for mean estimation
is O(sd/(mnϵ2)), where m is the number of local machines and each containing n data
points, d is the parameter of interest, s is the sparsity level, and ϵ is the privacy pa-
rameter. As commonly assumed in high-dimensional settings where the dimension is
comparable or even larger than the number of data, such an optimality result shows the
incompatibility of untrusted central server setting and high-dimensional statistics. As a
result, we can only hope to get a good estimation under the trusted central server setting
in the high-dimensional regime.

In the second part of the paper, we consider the case of a trusted central server and
design algorithms that allow for accurate estimations and obtain a near-optimal rate of
convergence up to logarithm factors. We also present statistical inference results, includ-
ing the construction of coordinate-wise confidence intervals with privacy guarantees, and
the solution to conduct simultaneous inference privately. This will assist in hypothesis
testing problems and construction of confidence intervals for a given subset of indices of
a vector simultaneously in high-dimensional settings. We emphasize that our algorithms
for estimation and inference are suited for practical purposes, considering its capacity
to (1) leverage data from multiple devices to improve machine learning models and (2)
draw accurate conclusions about a population from a sample while preserving individual
privacy. For instance, in healthcare, we could combine patient data from multiple hospi-
tals to develop more accurate models for disease diagnosis and treatment, while ensuring
that patient privacy is protected. We summarize our major contributions as follows:

• For the untrusted central server setting, we provably show that federated learning is

2

not suited for high-dimensional mean estimation problems by providing the optimal
rate of convergence under the untrusted central server constraints. This suggests
us to consider a trusted central server setting to utilize federated learning for such
problems.

• For the trusted central server setting, we design novel algorithms to achieve private
estimation with federated learning. We first consider the estimation in homoge-
neous federated learning setting and then we extend it to a more complicated het-
erogeneous federated learning setting. We also provide a sharp rate of convergence
for our algorithm in both settings.

• In addition, we consider statistical inference problems in both homogeneous and
heterogeneous federated learning settings. We provide algorithms for coordinate-
wise and simultaneous confidence intervals, which are two common inference prob-
lems in high-dimensional statistics. It is worth mentioning that our proposed meth-
ods for high-dimensional differentially private inference problems are novel and
unique, which has not been developed even for the single-source and non-federated
learning setting. Theoretical results show that our proposed confidence intervals
are asymptotically valid, supported by simulations.

1.2 Related Work

In the literature, several works focused on designing private algorithms in federated
learning/distributed learning based on variants of stochastic gradient decent algorithms.
[3] proposed a communication efficient algorithm, CP-SGD algorithm for learning models
with local differential privacy (LDP). [17] proposed a distributed LDP gradient descent
algorithm by applying LDP on gradients with ESA framework [6]. [20] extended works
on LDP approach for federated learning and proposed a distributed communication-
efficient LDP stochastic gradient descent algorithm through shuffled model and analyzed
the upper bound of the convergence rate. However, the trade-off between statistical
accuracy and the privacy cost has not been considered in these works.

In the distributed settings, the trade-off between statistical accuracy and information
constraints has been discussed in various papers. Two common types of information
constraints are communication constraints and privacy constraints. We refer to [43, 7,
21, 5, 18] for more discussions on communication constraints, considering the situation
where the bits of the information during communication have constraints.

A series of work discusses the trade-off between accuracy and privacy in high-dimensional

and non-federated learning problems, including top-k selection [35], sparse mean estima-
tion [8], linear regression [8, 36], generalized linear models [9], latent variable models [46].
However, the discussion on privacy constraints in the distributed settings are still largely
lacking. Among the existing works, most of them focus on the local differential privacy
In [4], the mean estimation under ℓ2 loss for Gaussian and sparse
(LDP) constraint.
Bernoulli distributions are discussed.
[11] discussed the lower bounds under LDP con-
straints in the blackboard communication model for mean estimation of product Bernoulli

3

distributions and sparse Gaussian distributions.
[2] proposed a more general approach
to combine both communication constraints and privacy constraints. Compared with
previous works, we focus on the problem where there are n data points on each machine.
Our interest lies in the (ϵ, δ)-DP instead of LDP, which is a weaker constraint containing
broader settings. We further note that, compared with the blackboard communication
model [7, 18], in the federated learning setting, we assume that the existence of a central
server and that each server is only allowed to communicate with the central server. This
setting enables us to enhance more privacy.

When we are finalizing this paper1, we realized an independent and concurrent work
[28] also considers differentially private federated transfer learning under high-
[28].
dimensional sparse linear regression model. Namely, they proposed a notion of federated
differential privacy that allows multiple rounds of (ϵ, δ)-differentially private transmis-
sions between local machines and the central server, and provides algorithms to filter
out irrelevant sources, and exploit information from relevant sources to improve the per-
formance of estimation of target parameters. We differntiate our research with their
paper as follows: (1) While they consider differentially private federated learning under
untrusted server setting, we deal with both trusted and untrusted server settings. We
also highlight a fundamental difficulty of pure ϵ-differentially private estimation under
untrusted central server settings in federated learning by establishing a tight minimax
lower bound, and resort to trusted server settings for estimation and inference prob-
lems. (2) While their investigation centers on differentially private estimation within a
federated transfer learning framework—specifically focusing on parameter estimation for
a target distribution using similar source data—our work focuses on private estimation
and inference for parameters that are either common across all participating machines,
or vary across different machines.

We also cite papers that provided us inspirations for the design of our proposed
algorithms and methods. [24] introduces a de-biasing produce for the statistical inference
problems. [29] considers the transfer learning problem in high-dimensional settings, which
enables us to combine information from other sources to benefit the estimation problems.
Such idea could be adopted in the hetergeneous federated learning problems. For the
simultaneous inference problems, we refer to [42, 41], which discussed how to conduct
simultaneous inference for high-dimensional problems.

Notation. We introduce several notations used throughout the paper. Let v =
(v1, v2, . . . , vd)⊤ ∈ Rd represent a vector. Given a set of indices S, vS refers to the
components of v corresponding to the indices in S. The ℓq norm of v, for 1 ≤ q ≤ ∞, is
given by ∥v∥q, whereas ∥v∥0 represents the number of non-zero elements in v, also called
as its sparsity level.

We use m to indicate the number of machines, n for the number of samples per
machine, d for the dimensionality of vectors, and s for their sparsity level. The total
number of samples across all machines is denoted by n0 = m · n. Additionally, we define
the truncation function ΠT : Rd → Rd, which projects a vector onto the ℓ∞ ball of radius
T centered at the origin.

1An initial draft of this paper was published as a Ph.D. dissertation in 2023 [45].

4

For a matrix Σ, max∥v∥2=1,∥v∥0≤s v⊤Σv and min∥v∥2=1,∥v∥0≤s v⊤Σv denote the largest

and smallest s-restricted eigenvalues of Σ, denoted as µs(Σ) and νs(Σ), respectively.

For sequences an and bn, an = o(bn) implies an/bn → 0 as n grows, an = O(bn)
signifies that an is upper bounded by a constant multiple of bn, and an = Ω(bn) indicates
that an is lower bounded by a constant multiple of bn, where constants are independent
of n. The notation an ≍ bn denotes that an is both upper and lower bounded by constant
multiples of bn.

In this work, we often use symbols c0, c1, m0, m1, C, C′, K, K′ to represent univer-
sal constants. Their specific values may vary depending on the context, but they are
independent from other tunable parameters.

2 Preliminaries

2.1 Differential Privacy

We start form the basic concepts and properties of differential privacy [13]. The intuition
behind differential privacy is that a randomized algorithm produces similar outputs even
when an individual’s information in the dataset is changed or removed, thereby preserving
the privacy of individual data. The formal definition of differential privacy is given below.

Definition 2.1 (Differential Privacy [13]) Let X be the sample space for an individ-
ual data, a randomized algorithm M : X n → R is (ϵ, δ)-differentially private if and only
if for every pair of adjacent data sets X, X ′ ∈ X n and for any S ⊆ R, the inequality
below holds:

P (M (X) ∈ S) ≤ eε · P (cid:0)M (X ′) ∈ S(cid:1) + δ,

where we say that two data sets X = {xi}n
if they differ by one individual datum.

i=1

and X ′ = {x′

i}n

i=1

are adjacent if and only

In the above definition, the two parameters ϵ, δ control the privacy level. From the
definition, with smaller ϵ and δ, the outcomes given adjacent X and X ′ become closer,
making it harder for an adversary to distinguish if the original dataset is X or X ′,
indicating the privacy constraint becomes more stringent. Furthermore, when δ = 0, we
could use ϵ-differentially private as the abbreviation of (ϵ, 0)-differentially private.

In the rest of this section, we introduce several useful properties of differential privacy
and how to create a differential private algorithm from non-private counterparts. One
common strategy is through noise injection. The scale of noise is characterized by the
sensitivity of the algorithm:

Definition 2.2 For any algorithm f : X n → Rd and two adjacent data sets X and X ′,
the ℓp-sensitivity of f is defined as:

∆p(f ) =

sup
X,X ′∈X n adjacent

∥f (X) − f (X ′)∥p.

5

We then introduce two mechanisms. For algorithms with finite ℓ1-sensitivity, we add
Laplace noises to achieve differential privacy, while for ℓ2-sensitivity, we inject Gaussian
noises.

Proposition 2.3 (The Laplace Mechanism [13, 14]) Let f : X n → Rd be a deter-
ministic algorithm with ∆1(f ) < ∞. For w ∈ Rd with coordinates w1, w2, · · · , wd be i.i.d
samples drawn from Laplace(∆1(f )/ϵ), f (X) + w is (ϵ, 0)-differentially private.

Proposition 2.4 (The Gaussian Mechanism [13, 14]) Let f : X n → Rd be a de-
terministic algorithm with ∆2(f ) < ∞. For w ∈ Rd with coordinates w1, w2, · · · , wd be
i.i.d samples drawn from N (0, 2(∆2(f )/ϵ)2 log(1.25/δ)), f (X) + w is (ϵ, δ)-differentially
private.

The post-processing and composition properties are two key properties in differential
privacy, which enable us to design complicated differentially private algorithms by com-
bining simpler ones. Such properties are pivotal in the design of algorithms in later
chapters.

Proposition 2.5 (Post-processing Property [13]) Let M be an (ϵ, δ)-differentially
private algorithm and g be an arbitrary function which takes M (X) as input, then
g(M (X)) is also (ϵ, δ)-differentially private.

Proposition 2.6 (Composition property [13]) For i = 1, 2, let Mi be (εi, δi)-differentially
private algorithm, then (M1, M2) is (ϵ1 + ϵ2, δ1 + δ2)-differentially private algorithm.

We also mention NoisyHT algorithm (Algorithm 1) introduced by [16], which stands for
the noisy hard-thresholding algorithm. The algorithm aims to pursue both sparsity of
the output and privacy at the same time.

Algorithm 1: Noisy Hard Thresholding Algorithm (NoisyHT(v, s, λ, ϵ, δ)) [16]
1 Input: vector-valued function v = v(X) ∈ Rd with data X, sparsity s, privacy

parameters ε, δ, sensitivity λ.

2 Initialization: S = ∅.
3 For i in 1 to s:

4 Generate wi ∈ Rd with wi1, wi2, · · · , wid

i.i.d.∼ Laplace

√
2

(cid:18)

λ ·

3s log(1/δ)

ε

(cid:19)
.

Append j∗ = argmax

j∈[d]\S(|vj| + wij) to S.

5
6 End For

7 Generate ˜w with ˜w1, · · · , ˜wd
8 Output: PS(v + ˜w).

i.i.d.∼ Laplace

√
2

(cid:18)

λ ·

3s log(1/δ)

ε

(cid:19)
.

In the last step, PS(u) denotes the operator that makes uSc = 0 while preserving uS.
This algorithm could be seen as a private top-k selection algorithm, which helps build
our proposed algorithm in later section.

6

Specifically, when the sparsity s is chosen to be 1, the algorithm outputs the maximum
element chosen after a single iteration in the private manner. We refer this special case as
the Private Max algorithm, which is implemented in Algorithm 7 used for simultaneous
inference.

2.2 Federated Learning

Federated learning introduced in [32] is a technique designed to train a machine learning
algorithm across multiple devices, without exchanging data samples. A central server
coordinates the process, with each local machine sending model updates to be aggregated
centrally. Figure 1 illustrates the basic concept of federated learning

Figure 1: Federated Learning

One characteristic of federated learning is that the training of machine learning mod-
els occurs locally, and only parameters and updates are transferred to the central server
and shared by each node. Specifically, communication between local machines and the
server is bidirectional: machines send updates to the central server, and in return, they
receive aggregated information after processing. Communication among local machines is
prohibited to prevent privacy leakage. Intuitively, federated learning inherently provides
a certain level of privacy.

Although without rigorous definitions, there are two main branches of central server
settings in federated learning: the untrusted central server setting and the trusted central
server setting [31, 39, 33, 19]. In the first setting, where the central server is untrusted,
each piece of information sent from the machine to the central server should be differ-
entially private.
In
this scenario, it is safe to send raw information from the machine to the central server
without additional privacy measures. However, to prevent information leakage among
local machines, the information sent back from the server should also be differentially
private.

In the second setting, we assume a trusted central server exists.

Another key aspect of federated learning is that the datasets on each local machine
are commonly not independent and identically distributed (i.i.d.). This allows federated

7

learning to train on heterogeneous datasets, aligning with practical scenarios where the
datasets on different machines are typically diverse and their sizes may also vary. We
will demonstrate that federated learning can efficiently estimate the local model when
models on different local machines differ but share some similarities, a concept we refer
to as heterogeneous federated learning in Section 5.

2.3 Problem Formulation

In this paper, we assume that there exists a central server and m local machines. We de-
note the data on these machines by X1, X2, X3, . . . , Xm, respectively, with Xi ∈ Rni×d.
On any machine i = 1, 2, . . . , m, there are ni data points Xi = [Xi1, Xi2, . . . , Xini]. For
simplicity, we assume that there are equal data points n = n1 = n2 = · · · = ni for each
machine. We note that the result could be easily generalized to cases where the sample
sizes on each machine differ.

We consider both untrusted and trusted central server settings. For the untrusted
setting, we require that the information sent from local machines to the server is pri-
vate. In this scenario, we show that in the high-dimensional setting, even with sparsity
assumptions, it is impossible to achieve small estimation error when the central server
is untrusted. In the trusted setting, we consider the high-dimensional linear regression
problem Y = Xβ + W with s-sparse β. We will first study the case where all machines
share the same β, (referred to homogeneous federation learning,) and then study a more
general case where models on different machines are not equal, but share certain simi-
larities (referred to heterogeneous federation learning.) We show that our algorithm can
adapt to such similarity—with larger similarity, the algorithm achieves a faster rate of
convergence.

3 An Impossibility Result in the Untrusted Central Server

setting

In this section, we study the untrusted server setting where the local machines need to
send privatized information to the central server to ensure privacy. We show an impos-
sibility result that in high-dimensional settings where the data dimension is comparable
to or greater than the sample size, accurate estimation is not feasible even if we consider
a simple sparse mean estimation problem.

As mentioned in Section 2.3, we consider a federated learning setting with m ma-
chines, where each machine i ∈ [m] handles n data points Xi := [Xi1, Xi2, . . . , Xin] ∈
Rn×d. Let Dall = {Xi}m
. We assume that each data point Xij ∈ Rd follows a Gaussian
i=1
distribution N (µ, Id), where µ is a sparse d-dimensional vector with sparsity s. The goal
is to estimate µ in the federated learning setting when the central server is untrusted. In
this section, we provide an optimal rate of convergence for this problem and show that
the untrusted central server setting is not suited for high-dimensional problems.

We begin by deriving the minimax lower bound, which characterizes the fundamental
difficulty of this estimation problem. In untrusted server setting, we additionally assume

8

that each piece of information sent from the local machine to the central server follows
ϵ-differential privacy. To achieve this, we introduce the privacy channel W ϵ-priv : X n →
Z, a function that is responsible for privatizing the information transmitted from the
local machines. Given the input X ∈ X n and the privacy channel W ϵ-priv, Z ∈ Z
representing all the information (from multiple rounds) transmitted to the central server.
More precisely, we require privacy guarantees such that for any two adjacent datasets X
and X ′ ∈ X n, differing by only one data point on any local machine, and for an output
Z ∈ Z representing the information sent from the local machine to the central server,
differential privacy guarantee P(W ϵ-priv(X) = Z) ≤ eϵ · P(W ϵ-priv(X ′) = Z) holds.

We consider any mechanism M in the federated learning setting with m local machines
and one central server, operated on the dataset Dall. M serves as a procedure to estimate
µ, where each local machine collaborates exclusively with the central server without
direct interaction among themselves. On each machine i, the mechanism M uses the
privacy channel W ϵ-priv
and data sample Xi to generate Zi, which is then transmitted to
the central server. The central server receives the information from all machines. After
multi-rounds of collaboration between local machines and the central server, we obtain
the sparse and private estimator ˆµ ∈ Rd. We denote the class of all mechanisms that
satisfy the above constraints as Muntrust
(Dall). Under this setting we establish a lower
bound for the estimation error of the mean in Theorem 1.

m,ϵ

i

Theorem 1 Suppose Dall is generated as above. Let µ be a s-sparse d-dimensional mean
of Gaussian distribution satisfying ∥µ∥∞ ≤ 1. We consider the estimation of the mean
vector ˆµ under the untrusted central server federated learning setting with m local ma-
chines and n data points in each machine. Then, there exists a constant c > 0 such
that

inf
M ∈Muntrust

m,ϵ

sup
µ∈Rd,∥µ∥∞≤1

∥µ − M (Dall)∥2

2 ≥ c · min

(cid:18) s
n

,

sd
mnϵ2

(cid:19)

.

The lower bound contains two terms. The first term, of order s/n, represents the minimax
risk of mean estimation using only the samples from a local machine. The second term, of
order sd/(mnϵ2), accounts for the error from federated learning across multiple machines
under privacy constraints. Theorem 1 suggests that we cannot perform better than either
choosing to estimate the mean using only the local machine or adopting the federated
learning approach and combining information from different machines. However, in the
latter approach, we must at least incur a rate of O(d/(mnϵ2)), which is linearly pro-
portional to the dimension d. This result suggests that privacy constraints significantly
impact the efficiency of federated learning in high-dimensional settings. Furthermore, as
the number of machines m increases, we can possibly attain better performance, high-
lighting the merit of federated learning.

We also show the tightness of the lower bound in Theorem 1 by providing the upper

bound.

Theorem 2 Suppose that conditions in Theorem 1 hold. Then, there exists an ϵ-differentially

9

private algorithm for the estimation of µ as ˆµ

where c > 0 is some constant.

∥µ − ˆµ∥2

2 ≤ c ·

s · d
mnϵ2 ,

The proof follows by constructing an algorithm that transforms Gaussian mean to
Bernoulli mean according to the sign of the Gaussian mean, motivated by Algorithm 2
discussed in [2], where the authors discuss l-bit protocol for estimating the product of
Bernoulli family. More details of the algorithm are deferred to Section A.2. Based
on the results from Theorems 1 and 2, we obtain the optimal rate of convergence for
sparse mean estimation under differentially private federated learning setting. As a
result, when the central server is untrusted, it is impossible to find an approach to
achieve accurate estimation under the untrusted server assumption. This highlights the
necessity of the trusted server setting for statistical estimation and inference in high-
dimensional federated learning scenarios. In the following sections, we develop estimation
and inference procedures under the trusted server settings.

4 Homogeneous Federated Learning Setting

4.1 Algorithms for Estimation Problems

In this section, we consider the setting of a trusted central server, where local machines
fully trust the central server and send unprivatized information to it without implement-
ing privacy measures. However, when the central server sends information back to the
local machines, it must ensure that this information is privatized to avoid any privacy
leakage across local machines.

In this subsection, we first focus on the statistical estimation problems in this setting
and then develop inference results in the next subsection. More specifically, our primary
focus is on the linear regression problem in a high-dimensional setting, where the ground
truth, denoted as β, is a sparse d-dimensional vector. We initially study the simpler
case in this section, where the underlying generative models for each local machine are
identical, which we refer to as the homogeneous federated learning setting. A more
complicated heterogeneous setting will be discussed in the following section. Specifically,
we consider the following high-dimensional linear regression model:

Y = Xβ + W ,

where we assume W is the error term whose coordinates are independent and following
sub-Gaussian distribution with variance proxy σ2, denoted by Wi ∼ subG(σ2). X is
a random matrix whose rows are following sub-Gaussian distribution with a covariance
matrix Σ.

We first introduce the parameter estimation algorithm under differentially private

federated settings with a trusted central server.

10

Algorithm 2: Differentially Private Sparse Linear Regression under Federated
Learning

Input : Dataset Dall = {(Xi, Yi)}i∈[m], number of machines m, number of

samples on each machine n, step size η0, privacy parameters ε, δ, noise
scale B0, number of iterations T , truncation level R, feasibility
parameter C0, sparsity s, initial value β0.

1 for t from 0 to T − 1 do
2

Step 1:

On each local machine i = 1, 2, . . . , m, calculate the local gradient

gi =

1
n

n
(cid:88)

(X ⊤

ij βt − ΠR(yij))Xij.

j=1

Send the gradient gi to the central server.
Step 2:

Compute βt+0.5 = βt − (η0/m) (cid:80)m
Compute βt+1 = ΠC0
server.
Step 3: Send the output βt+1 back to each local machine from the server.

i=1 gi at the central server;
T , η0B0
mn )

NoisyHT(βt+0.5, s, ε

at the central

T , δ

(cid:17)

(cid:16)

3

4

5

6

7
8 end
9 Output: Return βT .

In Step 1 of Algorithm 2, the information computed on each local machine is trans-
mitted to the central server. The second step involves calculations performed at the
central server. Prior to sending the information back to the local machines, it undergoes
privacy preservation through the application of the NoisyHT algorithm, as introduced
in Algorithm 1. Subsequently, the local machine updates its estimation based on the
information received from the central server.

We compare Algorithm 2 with Algorithm 4.2 in [8], which addresses the private
estimation in linear regression under non-federated learning settings. Unlike the latter,
our algorithm does not transmit all data points to the central server. Instead, we calculate
the gradient updates locally on each machine and send only these local gradients to the
server. This design enhances privacy protection, as the original data remains visible
only on the local machine and is not exposed externally. Furthermore, this approach of
gradient updates also reduces communication costs by transmitting only a d-dimensional
vector from each local machine for the gradient update. Previous research has also
considered non-private distributed methods for linear regression problems, such as [27,
44]. Our algorithm, however, ensures differential privacy. In practice, the sparsity level
s can be determined using a private version of cross-validation, while other parameters
may be pre-chosen based on our theoretical analysis.

11

4.2 Algorithms for Inference Problems

In this subsection, we focus on statistical inference problems in the homogeneous feder-
ated learning setting, such as constructing coordinate-wise confidence intervals for pa-
rameters and performing simultaneous inference. To begin, we develop a method for
constructing coordinate-wise confidence intervals, for example, for the k-th index of β,
βk. However, it is important to note that the output of Algorithm 2 is biased due to
hard thresholding. To overcome this bias, we employ a de-biasing procedure, a common
technique in high-dimensional statistics, as demonstrated in previous studies [24]. This
procedure involves approximating the k-th column of the precision matrix Θ = Σ−1
to construct confidence intervals for each βk. Subsequently, we focus on obtaining an
estimate of the precision matrix in a private manner.

Algorithm 3: Differentially Private Precision Matrix Estimation in Federated
Learning

Input : Number of machines m, number of data points in each machine n,
dataset Xi = (xi1, . . . , xin) for i = 1, . . . , m, step size η1, privacy
parameters ε, δ, noise scale B1, number of iterations T , feasibility
parameter C1, sparsity s, initial value Θ0
k

.

1 for t from 0 to T − 1 do
2

(cid:80)n

Step 1: On each local machine i = 1, 2, . . . , m, calculate local gradient
k − ej. Send the gradients (g1, g2, . . . , gm) to the
gi = 1
n
central server.
Step 2:

j=1 XijX ⊤

ij Θt

3

4

5

= Θt
k
k = ΠC1

Compute Θt+0.5
Compute Θt+1
server.
Step 3: Send Θt+1

6
7 end
8 Output: Return ΘT
k

k

.

k − (η0/m) (cid:80)m
(cid:16)

NoisyHT(wt+0.5

i=1 gi at the central server;
T , η1B1
mn )

T , δ

, s, ε

(cid:17)

k

at the central

back to each local machine from the server.

The structure of Algorithm 3 is similar to Algorithm 2, as both adopt an iterative
communication between the central server and the local machines; the information is ini-
tially transmitted from the local machines to the server, then, the central server performs
calculations and use the NoisyHT algorithm (Algorithm 1) to ensure the privacy of the
information. Subsequently, each local machine updates the gradient and progresses to the
next iteration. The primary distinction between two algorithms lies in the computation
of the gradient on each machine.

Denote the output of Algorithm 2 as ˆβ and the output of Algorithm 3 as ˆΘk. Then

the de-biased differentially private estimator of βk is given by

k = ˆβk +
ˆβu

1
m

m
(cid:88)

i=1

ˆΘ⊤

k gi + Ek,

(4.1)

where gi = (1/n) (cid:80)n

j=1(X ⊤
ij

ˆβ − ΠR(yij))Xij, and Ek is the injected random noise to

12

√

sc1cxR + sc0c1c2
x

1 log(1.25/δ)/(n2m2ϵ2)), where

ensure privacy, following a Gaussian distribution N (0, 8∆2
∆1 =

with some constants c0, c1, cx defined later.
The debiased estimator in (4.1) enables us to construct a differentially private con-
fidence intervals. Although the variance σ of the error term W in the linear regression
model is usually unknown, we can estimate σ from the data in a private manner. The
estimation is based on the residual term between the response Y and the fitted value
X ˆβ. We summarize the method to estimate σ in the private federated learning setting
in Algorithm 4.

Algorithm 4: Differentially Private Variance Estimation in Federated Learning
Input : Dataset (Xi, Yi)[i=1,2,...,m], privacy parameters ε, noise scale B2,

truncation level R, estimated parameter ˆβ from Algorithm 2.

1 Step 1: On each machine i = 1, 2, . . . , m, compute ˆWi = ∥ΠR(Yi) − Xi ˆβ∥2

2/n

and send ˆWi to the central server.

2 Step 2: Generate a random variable Evar, where Evar ∼

N (0, 2B2

2 log(1.25/δ)/ϵ2)

3 Step 3: Compute ˆσ2 such that ˆσ2 = (cid:80)m
i=1

ˆWi/m + Evar at the central server

Output: Estimated variance ˆσ2.
When examining the convergence rates of ˆβ and ˆΘk in Theorem 3, we observe the
crucial roles of the largest and smallest restricted eigenvalues of Σ. Since these eigenvalues
directly influence the construction of confidence intervals and cannot be directly obtained
from the data, their private estimation becomes essential. Below, we outline an algorithm
to estimate the largest restricted eigenvalue, µs(Σ). To estimate the smallest restricted
eigenvalue, νs(Σ), the same algorithm can be used by modifying Step 4 from “argmax”
to “argmin”.

Algorithm 5: Differentially Private Restricted Eigenvalue Estimation in Fed-
erated Learning

Input : Number of machines m, dataset (Xi)[i=1,...,m], number of data points

in each machine n, privacy parameters ε, noise scale B3, number of
vectors n1.

1 Step 1: Sample n1 d-dimensional, s-sparse unit vectors v1, v2, . . . , vn1
2 Step 2: On each machine, compute ti,k = (vT

i Xivk)/n where

k X T

.

k = 1, 2 . . . , n1 and send them on to the central server.

3 Step 3: Sample ξ1, . . . , ξn1 ∼ Laplace (2B3/ϵ).
4 Step 4: Compute kmax such that kmax = arg maxk

(cid:80)m

i=1 ti,k/m + ξk.

Output: µs( ˆΣ) = (cid:80)m
Based on Algorithms 4 and 5, we provide a constuction for coordinate-wise confidence

i=1 ti,kmax/m + ξ, where ξ ∼ Laplace(2B3/ϵ) independently.

intervals in Algorithm 6.

13

Algorithm 6: Differentially Private Coordinate-wise Confidence interval for βk
in Federated Learning

Input : Number of machines m, dataset (Xi, Yi)[i=1,...,m], number of data

points in each machine n, privacy parameters ε, δ, truncation level R,
sparsity s, estimators of parameters ˆβ, ˆΘk from Algorithms 2 and 3,
constants ∆1, γ.

1 Step 1: On each local machine i = 1, 2, . . . , m, calculate local gradient

(cid:80)n

gi = 1
n
central server.

j=1(X ⊤
ij

ˆβ − ΠR(yij))Xij. Send the gradient (g1, g2, . . . , gm) to the

2 Step 2: Generate a random variable E from the Gaussian distribution

N (0, 8∆2

1 log(1.25/δ)/n2m2ϵ2).

Calculate de-biased estimation, ˆβu

3 Step 3:
4 Step 4: Estimate ˆσ from Algorithm 4 and ˆµs, ˆνs from Algorithm 5.
5 Step 5:

Calculate the confidence interval Jk(α).

k = ˆβk + 1

ˆΘ⊤

i=1

m

(cid:80)m

k gi + E

Jk(α) =

(cid:20)
ˆβu
k −

6

ˆβu
k +

γ ˆµ2
s
ˆν2
s
γ ˆµ2
s
ˆν2
s

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2
s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

− Φ−1(1 − α/2)

√

(cid:114)

σ
mn

ˆΘ⊤
k

ˆΣ ˆΘk +

+ Φ−1(1 − α/2)

√

(cid:114)

ˆΘ⊤
k

σ
mn

ˆΣ ˆΘk +

8∆2

1 log(1/δ)
mnϵ2

,

(cid:21)

8∆2

1 log(1/δ)
mnϵ2

Output: Return the final result Jk(α).
So far we focused on constructing confidence intervals for individual coordinates of
the parameter vector β. However, in high-dimensional settings, we are often interested
in group inference problem, where we test hypotheses involving multiple coordinates
simultaneously. Specifically, we consider the problem of testing the null hypothesis given
by

against the alternative hypothesis,

H0 : ˆβk = βk, for all k ∈ G

H1 : ˆβk ̸= βk,

for at least one k ∈ G, where G is a subset of all coordinates {1, 2, . . . , d} and we allow
|G| to be the same order as d. Additionally, we also construct simultaneous confidence
intervals for all coordinates in G. Note that the problem discussed above are common in
high-dimensional data analysis, with applications such as multi-factor analysis of variance
[22], additive modeling [40]. Previous research works have discussed similar problems in
the non-private setting, including [10, 42, 41].

To address the problem, simultaneous inference can be conducted using a test statistic

| ˆβu

k − βk|.

max
k∈G

Major challenges of simultaneous inference in a private federated learning setting
include: (1) minimizing the communication cost from local machines to the server while

14

retaining all data on the local machines, and (2) ensuring the privacy of the procedure,
which necessitates a tailored privacy-preserving mechanism at each step of the algorithm.
In our framework, we propose an algorithm based on the bootstrap method. As previ-
ously mentioned, to build confidence intervals, our interest lies in the statistic computed
by the maximum coordinate of ˆβu
k − βk over G. By decomposing this statistic, we obtain
a term σ√
ij β). To determine the distribution of this term,
we bootstrap the residuals yij − X T

ˆΘXij(yij − X T
ij β.

(cid:80)m

(cid:80)n

j=1

i=1

mn

We outline the algorithm as follows: we first estimate ˆβ and ˆΘk using Algorithm 2
and 3, respectively. Accordingly, by stacking ˆΘk for all k, we get an estimator of the
precision matrix ˆΘ. The details are provided in Algorithm 7.

Algorithm 7: Private Bootstrap Method for Simultaneous Inference in Feder-
ated Learning

Input : number of machines m, dataset (Xi, Yi)[i=1,2,...m], number of data on
each machine n, privacy parameters ε, δ, estimators of parameters ˆβ,
ˆΘ from Algorithms 2 and 3, number of iterations for bootstrap q,
quantile α, noise level B4, subset of coordinates G.

1 for t from 0 to q do
2

(cid:80)n

Step 1: For each local machine i = 1, . . . , m, generate n independent
standard Gaussian random variables ei1, . . . , ein. Calculate
ui = 1√
n
Step 2: Send (ui)[i=1,2,...,m] from local machines to the central server.
i=1 ui]G, ϵ, δ, B4) at the
Step 3: Calculate Ut = Privatemax([(1/
central server.

ˆΘXijeij.

m) (cid:80)m

j=1

√

3

4

5 end
6 Output: Compute the α-quantile CU (α) of (|U1|, |U2|, . . . |Uq|) for α ∈ (0, 1).

On line 4 of Algorithm 7, we employ the Private Max algorithm, which we mentioned
earlier as a variation of NoisyHT algorithm (Algorithm 1) by directly picking s = 1, to
obtain the maximum element in a vector in a private manner. It is also important to
note that the Private Max algorithm is applied to a subset of G. After presenting the
algorithm, we denote M as:

M ( ˆβ) = max
k∈G

√
|

mn( ˆβu

k − βk)|.

M is used as the statistic for inference problems later.

As previously mentioned, we can easily construct a simultaneous confidence interval

for each k ∈ G by:

(cid:20)
ˆβu
k −

√

ˆσ
mn

CU (α), ˆβu

k +

√

(cid:21)
CU (α)

,

ˆσ
mn

where CU (α) is obtained from our algorithm with prespecified α. We can similarly
perform hypothesis testing; first calculate the test statistic and obtain CU (α) from our
algorithm with prespecified α, then reject if the statistic lies in the rejection region.

15

4.3 Theoretical Results

In this subsection, we provide theoretical guarantee for the algorithms and methods
discussed in the previous subsections. Before proceeding, we outline key assumptions
concerning the design matrix X, precision matrix Θ, and the true parameter β of the
linear regression model, which are essential for our subsequent analyses.

constant 0 < c0 < ∞ and ∥β∥0 ≤ s∗

(P1) Parameter Sparsity: The true parameter vector β satisfies ∥β∥2 < c0 for some
0 = o(n).
(P2) Precision matrix sparsity: For each column of the precision matrix Θk, k =
it satisfies that ∥Θk∥2 < c1 for some constant 0 < c1 < ∞ and
1 = o(n).

1, 2, . . . , d,
∥Θk∥0 ≤ s∗

(D1) Design Matrix:

for each row of the design matrix X, denote by x, xΣ−1/2 is

sub-Gaussian with sub-Gaussian norm κ := ∥Σ−1/2x∥ψ2

.

(D2) Bounded Eigenvalues of the covariance matrix: For the covariance matrix Σ =
Exx⊤, there exists a constant 0 < L < ∞ such that 0 < 1/L < λmin(Σ) ≤
λmax(Σ) < L.

The above assumptions (P1) and (P2) bounds the ℓ2 norm and ℓ0 norm of the parameters
β and Θk, and assumption (D1) guarantees that each row of X follows a sub-Gaussian
distribution, and assumption (D2) requires the covariance matrix has bounded eigen-
values. These assumptions are commonly used for theoretical analysis of differentially
private algorithms and debiased estimators [9, 8, 24].

With assumptions (P1)-(D2), we analyze the algorithms we presented. We begin with

the estimation problem and provide a rate of convergence of ˆβ and ˆΘk.

Theorem 3 Let {(yij, Xij)}i∈[m],j∈[n] be an i.i.d. samples from the high-dimensional
linear model. Suppose that assumptions (P1), (P2), (D1), (D2) are satisfied. Addition-
ally,

• we choose parameters as follows: let s∗ = max(s∗
√
C1 = c1, cx = 3(cid:112)2Lκ2 log d, B0 = 2(R +
√

2 log mn, C0 = c0,
, ∆1 =
sc1c2
x
and γ = max(µs(9µs + 1/4), 17/16µs + 1/96), where µs, νs are

1), R = σ
0, s∗
sc0cx)cx, B1 = 2

sc1cxR + sc0c1c2
x

√

the largest and smallest s-restricted eigenvalues of ˆΣ.

√

• we set β0 = 0 and Θ0

k = 0 as the initialization used in Algorithm 2 and Algorithm 3.
Then there exists some absolute constant ρ > 0 such that, if s = ρL4s∗, η0 = η1 = s/6L,
0Ln(cid:1) and n ≥ KR(s∗)3/2 log d(cid:112)log(1/δ) log n/ε for a sufficiently large
T = ρL2 log(cid:0)8c2
constant K > 0, then, for the output from Algorithm 2 and Algorithm 3,

∥ ˆβ − β∥2

2 ≤ σ2

(cid:18)

k0 ·

s log d
mn

+

6γµs
ν2
s

·

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

(cid:19)

,

(4.2)

16

and

∥ ˆΘk − Θk∥2

2 ≤ σ2

(cid:18)

k1 ·

s log d
mn

+

6γµs
ν2
s

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

(cid:19)
,

(4.3)

hold with probability 1 − exp(−Ω(log(d/s log d) + log n)).

The upper bound of Algorithm 3 in (4.3) can be interpreted as follows. The first
term represents the statistical error, while the second term accounts for the privacy cost.
Furthermore, the result is comparable to that of Theorem 4.4 in [8], which addresses
private linear regression in a non-federated setting. This comparison suggests that the
federated learning approach does not affect the convergence rate adversely; instead, it
allows us to leverage the benefits of federated learning. We also note that the advantages
of federated learning will be further explored in the heterogeneous federated learning
setting, which will be discussed in the next chapter.

The remainder of this subsection presents the theoretical results for the inference
problem. We begin with the construction of coordinate-wise confidence intervals. As
mentioned before, σ is usually unknown and we estimate σ in a private manner, presented
in Algorithm 4. Lemma 4.1 states the statistical guarantee of our algorithm.

Lemma 4.1 Let ˆσ2 be the output from Algorithm 4 by choosing R = O(
B2 = 4
(ϵ, δ)-differentially private, and it follows that

2 log mn),
x) and ˆβ as the output from Algorithm 2. Then, Algorithm 4 is

mn (R2 + s2c2

0c2

√

|σ2 − ˆσ2| ≤ c ·

mn
where c > 0 is a universal constant.

(cid:18) 1
√

+

s log d
mn

+

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

(cid:19)
,

Next, we consider a simplified version of the confidence interval, where the privacy
cost is dominated by the statistical error. In this scenario, we assume that the privacy
level is relatively low and the privacy constraints are loose, meaning that the privacy
parameters ϵ and δ are relatively large, allowing for nearly cost-free estimation. We
present our result in the following theorem.

Theorem 4 Suppose that the conditions in Theorem 3 hold. Assume that s∗ log d√
and s∗2 log2 d log(1.25/δ) log3 mn
by statistical error, i.e., there exists a constant k0 such that s∗2 log2 d log(1/δ) log3 mn
k0 · s∗ log d
mn
is asymptotically valid:

mn = o(1)
= o(1). Also assume that the privacy cost is dominated
≤
defined in (4.1), the confidence interval

. Then, given the de-biased estimator ˆβu
k

m2n2ϵ2

mnϵ2

where

lim
mn→∞

P(βk ∈ Jk(α)) = 1 − α,

Jk(α) =

(cid:20)
ˆβu
k − Φ−1(1 − α/2)

√

ˆσ
mn

(cid:113)

ˆΘ⊤
k

ˆΣ ˆΘk,

ˆβk + Φ−1(1 − α/2)

√

(cid:113)

ˆσ
mn

(cid:21)

ˆΘ⊤
k

ˆΣ ˆΘk

Also, the confidence interval Jk(α) is (ϵ, δ)-differentially private.

17

Theorem 4 assumes that the privacy cost is dominated by the statistical error. How-
ever, when the privacy constraint is more stringent with small privacy parameters ϵ and
δ, the privacy cost may be larger than the statistical error. In this scenario, we general-
ize Theorem 4 to analyze Algorithm 6. We note that the largest and smallest restricted
eigenvalues of ˆΣ also need to be estimated by Algorithm 5. Lemma 4.2 quantifies the
estimation error of the largest restricted eigenvalue of ˆΣ.

Lemma 4.2 If n1 = cds and B3 = 2sc2
x/n for some constant c > 0, then the output
from Algorithm 5 is (ϵ, 0)-differentially private. Moreover, (1/9)λs ≤ ˆλs ≤ λs holds where
λs is the largest restricted eigenvalue of ˆΣ.

We then present a theoretical result for the confidence interval in a more general case

in Theorem 5.

Theorem 5 Assume the conditions in Theorem 3 hold. Suppose that s∗ log d√
mn = o(1)
and s∗2 log2 d log(1.25/δ) log3 mn
defined in
(4.1) and the estimated restricted eigenvalues ˆµs and ˆνs from Algorithm 5, the confidence
interval constructed by Algorithm 6 is asympotically valid:

= o(1), then, given the de-biased estimator ˆβu
k

m2n2ϵ2

lim
mn→∞

P(βk ∈ Jk(α)) = 1 − α,

where

(cid:20)

Jk(α) =

ˆβu
k −

ˆβj +

γ ˆµ2
s
ˆν2
s
γ ˆµ2
s
ˆν2
s

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2
s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

− Φ−1(1 − α/2)

√

(cid:114)

ˆΘ⊤
k

ˆσ
mn

ˆΣ ˆΘk +

8∆2

1 log(1/δ)
mnϵ2

,

+ Φ−1(1 − α/2)

(cid:114)

ˆΘ⊤
k

√

ˆσ
mn

ˆΣ ˆΘk +

8∆2

1 log(1/δ)
mnϵ2

(cid:21)
.

Also, Jk(α) is (ϵ, δ)-differentially private.

Compared to the non-private counterpart in [24], we claim that our confidence interval
has a similar form but with additional noise injected to ensure privacy. When the noise
level is low, the confidence interval closely approximates the non-private counterpart,
allowing us to nearly achieve privacy without incurring additional costs. Furthermore,
when the privacy level is high, the confidence interval has a larger length to attain the
same confidence level.

Finally, for the simultaneous inference problems, we demonstrate that α-quantile
of statistic M in (6) is close to the α-quantile of U calculated in Algorithm 7 for each
α ∈ (0, 1) using the bootstrap method. The next theorem states the statistical properties
of Algorithm 7.

Theorem 6 Assume the conditions in Theorem 4 hold. Additionally, we assume that
s∗ log d√
mn = o(1) and the privacy cost is dominated by the statistical error, i.e., there exists
a constant c > 0 such that s∗2 log2 d log(1/δ) log3 mn
. We also assume that

≤ c · s∗ log d
mn

m2n2ϵ2

18

there exists a constant k0 such that log7(dmn)/mn ≤
, and that q = o(mn),
where q is the number of iterations for bootstrap q. The noise level is chosen as B4 =
4L

mn. Then, CU (α) computed in Algorithm 7 satisfies

log mcx(R + c0cx

1
(mn)k0

s∗)/

√

√

√

|P(M ≤ CU (α)) − α| = o(1).

sup
α∈(0,1)

Theorem 6 has useful applications: we can obtain a good estimator of the α-quantile
of U using the bootstrap method and then use it to construct confidence intervals or
perform hypothesis testing. Numerical results will be presented in later chapters to
further support our claims.

5 Heterogeneous Federated Learning Setting

5.1 Methods and Algorithms

In this section, we consider a more general setting where the parameters of interest on
each machine are not identical, but they share some similarities. Specifically, we consider
the scenario where, on each machine i = 1, 2, . . . , m, we assume a linear regression model:

Y = Xβ(i) + Wi,
where β(i) represents the true parameter on machine i. We assume that each Wi is
a vector whose coordinates follow a sub-Gaussian distribution: Wik ∼ subG(σ2), k =
1, 2 . . . , d i.i.d. We also assume that each row of X follows a sub-Gaussian distribution
i.i.d. with mean zero and covariance matrix Σ. We further quantify the similarity of
each β(i) by assuming that that there exists a subset S ∈ {1, 2, . . . , d} with |S| = s0
satisfying β(i1)

for any i1, i2 ∈ {1, 2, . . . , m}.

A naive approach would be estimating each β(i) locally, as in the non-private setting.
However, in the context of federated learning, we can improve the estimation with a
sharper rate of convergence by exploiting similarities of the model across machines. To
achieve this, we decompose β(i) into the sum of two vectors, β(i) = u + vi, where u
captures the signals common to all β(i), and vi captures the signals unique to each
machine.

S = β(i2)

S

We employ a two-stage procedure to estimate each β(i): in the first stage, we esti-
mate u using Algorithm 2 with a sparsity level of ∥u∥0 = s0 indicating the number of
shared signals. In the second stage, we estimate vi on the individual machine. Our final
estimation of β(i) is given by ˆβ(i) = ˆvi + ˆu. The procedure is summarized in Algorithm 8.
Similar to the previous section, we next address inference problems. Our algorithms
consist of two parts: the construction of coordinate-wise confidence intervals and simul-
taneous inference. We begin by describing the algorithm for coordinate-wise confidence
intervals in Algorithm 9.

In Algorithm 9, ˆΘj is the (ϵ, δ)-differentially private estimator of the j-th row of the
. We define the

precision matrix of covariance matrix ˆΣ = 1/(mn) (cid:80)m
i=1

j=1 XijX ⊤
ij

(cid:80)n

19

Algorithm 8: Differentially Private Sparse Linear Regression in Heterogeneous
Federated Learning Setting

Input : Number of machines m, dataset (yi, Xi)[i=1,...,m], number of data on
each machine n, step size η0, privacy parameters ε, δ, noise scale B5,
number of iterations T , truncation level R, feasibility parameter C0,
initial value v0
i

, sparsity level of similar vector s0, sparsity level s.

1 Step 1: Estimate a s0 sparse vector u using Algorithm 2.
2 Step 2: Estimate a s1 := s − s0 sparse vector vi with samples (yi, Xi) on

machine i with the following iterations from line 3-6.

3 for t from 0 to T − 1 do
4

Compute vt+0.5
i
vt+1
i = ΠC0

= vt
(cid:0)NoisyHT(vt+0.5

5
6 end
7 Step 3: Estimate β(i) by ˆβ(i) := ˆvi + ˆu.

i − (η0/n) (cid:80)n

i

i=1(x⊤

i vt

i − ΠR(yi − x⊤

i u))xi;

, (yi, Xi), s, ε/T, δ/T, η0B5/n)(cid:1).

Output: ˆβ(i).

Algorithm 9: Differentially Private Coordinate-wise Confidence interval for βk
in Heterogeneous Federated Learning

Input : Number of machines m, dataset (Xi, Yi)[i=1,...,m], number of data

points in each machine n, privacy parameters ε, δ, truncation level R,
sparsity s, estimated parameters ˆβ(i), ˆΘk from Algorithms 8 and 3,
and estimated eigenvalues ˆµs, ˆνs from Algorithm 5, constants ∆1, γ.

1 Step 1: Generate a random variable E3 from a Gaussian distribution

N (0, 8∆2

1 log(1.25/δ)/(n2ϵ2)).

2 Step 2:
k = ˆβ(i)
ˆβ(i,u)
3 Step 3:

Calculate de-biased estimation,
k + 1
n
Calculate the confidence interval Jk(α).

k XijΠR(yij) − ˆΘ⊤

j=1( ˆΘ⊤

k XijX ⊤
ij

(cid:80)n

ˆβ(i)
k ) + E3.

(cid:20)
Jk(α) =

ˆβ(i,u)
k − a −

σΦ−1(1 − α/2)
√
n

(cid:114)

ˆΘ⊤
k

ˆΣ ˆΘk +

8∆2

1 log(1/δ)

nϵ2

,

ˆβ(i,u)
k + a +

σΦ−1(1 − α/2)
√
n

(cid:114)

ˆΘ⊤
k

ˆΣ ˆΘk +

8∆2

1 log(1/δ)

nϵ2

(cid:21)
,

where a is defined in (5.1).

4 Output: Return the final result Jk(α).

variable a in step 3 by

a :=

2γ ˆµ2
s
ˆν2
s

1 log2 d log(1/δ) log3 mn
s2
m2n2ϵ2

+

2γ ˆµ2
s
ˆν2
s

0 log2 d log(1/δ) log3 n
s2
n2ϵ2

.

(5.1)

We then provide Algorithm 10 for the simultaneous inference problem. Similar to the

20

previous chapter, we can perform simultaneous inference for each β(i) to build simulta-
neous confidence interval and hypothesis testing.

Algorithm 10: Private Bootstrap Method for Simultaneous Inference in Het-
erogeneous Federated Learning for Machine i ∈ {1, . . . , m}

Input : Dataset (yi, Xi), number of data n, privacy parameters ε, δ, estimators

of parameters ˆβ(i), ˆΘ from Algorithms 8 and 3, number of iterations
for Bootstrap q, quantile α, noise level B6. (RN: σ?)

1 for t from 0 to q do
2

Generate n independent standard Gaussian random variables e1, . . . , en.
(cid:80)n
Calculate Ut = ∥Privatemax([ σ√
n

ˆΘXijej]G, ϵ, δ, B6)∥∞

j=1

3

4 end
5 Output: Compute the α-quantile CU (α) of (|U1|, |U2|, . . . , |Uq|) for α ∈ (0, 1).

Compared with Algorithm 7 introduced for simultaneous inference in homogeneous
federated learning, bootstrap algorithm in Algorithm 10 runs within the local machine
of interest. Using the output from Algorithm 10, we build a simultaneous confidence
interval for each β(i)
k

(k ∈ G) using CU (α) by

(cid:20)

ˆβ(i,u)
k −

1
√
n

CU (α), ˆβ(i,u)

k +

(cid:21)
.

CU (α)

1
√
n

5.2 Theoretical Results

In this subsection, we provide theoretical analysis for the algorithms in heterogeneous fed-
erated learning settings. We begin our theoretical analysis with the estimation problem,
which resembles Theorem 3.

Intuitively, when β(i) are similar but not identical, federated learning can be used to
estimate their common elements and the remaining parameters can be estimated indi-
vidually on each machine. This results in a sharper rate of convergence as the estimation
of the common component u can exploit the information from more data points. We
summarize the result in Theorem 7.

Theorem 7 Assume that the conditions in Theorem 5 hold. Further assume that for
Algorithm 8, ∥vi∥0 = s1 = s − s0 for all i = 1, . . . , m, ∥u∥0 = s0, ∥u∥2 ≤ c0/2, and
s1c0cx). Then, for the output ˆβ(i) from Algorithm 8,
∥vi∥2 ≤ c0/2. Let B5 = cx(2R +
we have

√

∥ ˆβ(i) − β(i)∥2

2 ≤ c0

s0 log d
mn

+ c1

s0

2 log d2 log(1/δ) log3 mn
m2n2ϵ2

+ c2

s1 log d
n

+ c3

s1

2 log d2 log(1/δ) log3 n
n2ϵ2
(5.2)

,

where c0, c1, c2, c3 > 0 are some constants.

In the case where s0 ≪ s1, i.e., the models are largely different across machines, the third
and fourth term on the right hand side of (5.2) dominates the estimation error, and the
estimation accuracy of β(i) via federated learning becomes closer to that with a single

21

machine (m = 1). In high level, this is because the information from other machines
is not helpful in the estimation when there exists a large dissimilarity of models across
machines. However, with a large s0 ≫ s1, federated learning can leverage the similarity
of models to improve estimation accuracy. As a result, the rate in (5.2) becomes closer
to the rate in 4.2 for homogeneous federated learning setting when s0/s1 → 0.

We next present our results for the inference problems. To start we verify that the

output from Algorithm 9 is a asymptotic 1 − α confidence interval for β(i)
k

.

Theorem 8 Assume the conditions in Theorem 3 hold and assume that s∗ log d√
0 log2 d log(1/δ) log3 n
s2
and max( 2γ ˆµ2
n2ϵ2
ˆν2
s
defined in (5.1). Then, for the de-biased estimator ˆβ(i,u)
confidence interval is asympotically valid:

n = o(1)
) = o(1). Let a be the variable
defined in (4.1), the constructed

1 log2 d log(1/δ) log3 mn
s2
m2n2ϵ2

, 2γ ˆµ2
ˆν2
s

k

s

s

lim
n→∞

P(β(i)

k ∈ Jk(α)) = 1 − α,

where

(cid:20)
Jk(α) =

ˆβ(i,u)
k − a −

(cid:114)

ˆσΦ−1(1 − α/2)
√
n

ˆΘ⊤
k

ˆΣ ˆΘk +

8∆2

1 log(1/δ)

nϵ2

,

ˆβ(i,u)
k + a +

ˆσΦ−1(1 − α/2)
√
n

(cid:114)

ˆΘ⊤
k

ˆΣ ˆΘk +

8∆2

1 log(1/δ)

(cid:21)

nϵ2

Also, Jk(α) is (ϵ, δ)-differentially private.

Finally, we provide a statistical guarantee for Algorithm 10. Similar to the previous

section, we define M as:

M = M ( ˆβ(i,u)) = max
k∈G

√
|

n( ˆβ(i,u)

k − β(i)

k )|.

Theorem 9 Assume that the conditions in Theorem 4 hold. We additionally assume that
s∗ log d√
n = o(1) and the privacy cost is dominated by the statistical error, i.e., there exists a
constant c such that s∗2 log2 d log(1/δ) log3 mn
and s∗2 log2 d log(1/δ) log3 n
≤ c· s∗ log d
≤ c· s∗ log d
.
mn
We also assume that there exists a constant k0 such that log7(dn)/n ≤ 1
. The noise
nk0
level is chosen as B6 = 2
n cxc1. Then,

(cid:113) s log n

m2n2ϵ2

n2ϵ2

n

|P(M ≤ CU (α)) − α| = o(1).

sup
α∈(0,1)

Theorem 9 states that α-quantile of M is asymptotically close to CU (α), which vali-
dates the 1 − α simultaneous confidence intervals based on CU (α) obtained by the boot-
strap method. This result allows us to perform simultaneous inference such as the con-
fidence intervals and hypothesis testing based on CU (α).

22

6 Simulations

In this section, we conduct simulations to investigate the performance of our proposed
algorithm as discussed in the preceding sections. Specifically, we explore the more com-
plex heterogeneous federated learning setting, where each machine operates on different
models yet exhibits similarities. Our simulations are divided into three main parts.

In Section 6.1, we present the simulation results for the coordinate-wise estimation
problem within a private federated setting, discussing the differences between the esti-
mated ˆβ and the true β∗ across various scenarios. We also examine the coverage of our
proposed confidence intervals. Section 6.2 extends the settings to simultaneous inference.
We generate simulateion simulation datasets as follows. First, we sample the data Xi,
for i = 1, 2, . . . , m, where each Xi follows a Gaussian distribution with mean zero and
covariance matrix Σ. We set Σ such that for each j, j′ ∈ {1, 2, . . . , d}, Σj,j′ = 0.5|j−j′|.
On each machine, we assume a s∗-sparse unit vector β(i) with s∗ = s0 + s1, where s0 is
the number of non-zero shared signals. For each β(i), we set the first s0 shared elements
to 1/
s∗ and additionally select machine-specific s1 entries from the remaining d − s0
indices to be 1/
s∗. We then compute Yi = Xiβ(i) + Wi, where each Wi follows a
Gaussian distribution N (0, σ2I) with σ = 0.5.

√

√

6.1 Estimation and Confidence Interval

In this subsection, we investigate the estimation accuracy and confidence interval cov-
erage of our algorithm for coordinate-wise inference. Namely, we consider the following
scenarios:

• Fix number of machines m = 15, ϵ = 0.8, δ = 1/(2mn), d = 800, s∗ = 15
and s0 = 6. Set the number of samples on each machine to be 4000, 5000, 6000,
respectively.

• Fix number of samples on each machine n = 4000, ϵ = 0.8, δ = 1/(2mn), d = 800,

s∗ = 15 and s0 = 6. Set the number of machines m to be 5, 10, 15,

• Fix number of machines m = 15, number of samples on each machine n = 4000,
ϵ = 0.8, δ = 1/(2mn), d = 800. Set, s∗ = 15, s0 = 6, s∗ = 10, s0 = 4, s∗ = 20, s0 =
8, respectively.

• Fix number of machines m = 15, number of samples on each machine n = 4000,

ϵ = 0.8, δ = 1/(2mn), d = 800, s∗ = 15. Set s0 = 6, 8, 10, respectively.

• Fix number of machines m = 15, number of samples on each machine n = 4000,

δ = 1/(2mn), d = 800, s∗ = 15 and s0 = 6. Set ϵ = 0.3, 0.5, 0.8 respectively.

• Fix number of machines m = 15, number of samples on each machine n = 4000,
ϵ = 0.8, δ = 1/(2mn), s∗ = 15 and s0 = 6. Set d = 600, 800, 1000, respectively.

For each setting, we report the average estimation error ∥ ˆβ −β∗∥2
among 50 replications.
2
Also, in each setting, we calculate the confidence interval with α = 0.95 for each index

23

of β∗ using our proposed algorithm. To evaluate the quality of confidence interval, we
define cov as the coverage of the confidence interval:

cov := d−1

d
(cid:88)

i=1

P[β∗

i ∈ Ji(α)].

We also define the coverage for non-zero and zero entries of β∗ by covS and covSc,
respectively, where S is the set of non-zero indices in β∗.

covS = |S|−1 (cid:88)

P[β∗

i ∈ Ji(α)]

,

covSc = |S c|−1 (cid:88)

P[β∗

i ∈ Ji(α)].

i∈S

i∈Sc

We report the estimation error, coverage of true parameter and length of confidence

interval for each configuration listed above in Table 2:

Simulation Results

(n, m, d, s∗, s0, ϵ)

Estimation
(Sd)

Error

cov

covS

covSc

length

(3000,15,800,15,8,0.8)

0.0213 (0.0028)

0.940

0.929

0.940

0.0532

(4000,15,800,15,8,0.8)

0.0170 (0.0032)

0.945

0.960

0.944

0.0437

(5000,15,800,15,8,0.8)

0.0141 (0.0021)

0.940

0.945

0.940

0.0378

(4000,10,800,15,8,0.8)

0.0218 (0.0047)

0.945

0.945

0.945

0.0437

(4000,20,800,15,8,0.8)

0.0126 (0.0025)

0.944

0.941

0.944

0.0437

(4000,15,600,15,8,0.8)

0.0162 (0.0031)

0.946

0.933

0.946

0.0436

(4000,15,1000,15,8,0.8)

0.0191 (0.0027)

0.940

0.933

0.940

0.0439

(4000,15,800,15,4,0.8)

0.0188 (0.0032)

0.952

0.945

0.953

0.0420

(4000,15,800,15,12,0.8)

0.0137 (0.0016)

0.944

0.937

0.944

0.0462

(4000,15,800,10,8,0.8)

0.0105 (0.0017)

0.946

0.947

0.946

0.0389

(4000,15,800,20,8,0.8)

0.0243 (0.0036)

0.941

0.932

0.941

0.0497

(4000,15,800,15,8,0.5)

0.0240 (0.0038)

0.940

0.949

0.940

0.0550

(4000,15,800,15,8,0.3)

0.0943 (0.0281)

0.928

0.941

0.928

0.0792

Table 2: Table for Simulation Results of the private federated linear regression

From Table 2, we observe a consistent result with our theory. Namely, for the esti-
mation error, the error becomes small as ϵ gets larger as we require less level of privacy.

24

Also, more data points on each machine, more number of machines, smaller sparsity level
lead to better estimation accuracy. For confidence intervals, we observe that the coverage
is close to 0.95 for cov, covS, and covSc, is and stable in different settings. To further
illustrate our claim, we pick the setting of (n, m, d, s, s0, ϵ) = (4000, 15, 800, 15, 8, 0.8)
and plot the confidence intervals versus the true value among 50 replications in Figure 3.
We randomly select 60 out of 800 coordinates.

Fig 3: Confidence intervals for βk for each coordinate k randomly selected from 800
coordinates. vertical axis stands for the value of βk. Red points stand for the true βk
while black points stand for the estimated βk. We mention that the result averaged over
50 iterations.

We also summarize our results in Figure 6.1, where we plot the estimation error
against the change in the number of samples, sparsity, and number of machines. For
the figure, we fixed m = 15, d = 800, s∗ = 16, s0 = 8, for the middle figure, we fixed
n = 4000, m = 15, d = 800, ϵ = 0.5, and for the right figure, we fixed d = 800, s∗ = 16,
s0 = 8, ϵ = 0.5. The error is averaged over 200 replications.

25

Fig 4: Plot for the estimation results. Left: Log estimation error with different number
of samples n, Middle: Log estimation error with different sparsity s∗, Right: Log
estimation error with different number of machines m.

From the left figure in Figure 6.1, we observe the decreasing error when we increase
n. When the privacy parameter ϵ is large, we have better estimation error. From the
middle figure, we observe that as the sparsity level s grows, the estimation error also
increases. Also, when the sparsity for the shared signal s0 becomes large, the estimation
error also becomes large. In the right figure, we observe a consistent decrease of error
when we increase the number of machines. All these figures support Theorem 7.

6.2 Simultaneous Inference

In this subsection, we investigate our proposed algorithms for simultaneous inference
problems. We aim to build a simultaneous confidence interval when α = 0.05 under
three settings: G = {1, 2, . . . , d}, G = S, and G = Sc. For each setting, we repeat 50
simulations and report the coverage and length of the confidence intervals. The results
are shown in Table 5.

From simulation results, we can observe that our proposed simultaneous confidence
interval mostly exhibit over-coverage for G = S c, and under-coverage for G = S. This
pattern has also been observed in previous works addressing simultaneous inference [38,
42]. Therefore, this could be attributed to the inherent nature of simultaneous inference
rather than to algorithmic reasons.

7 Discussions and Future Work

In this paper, we study the high-dimensional estimation and inference problems within
the context of federated learning. In scenarios involving an untrusted central server, our
findings reveal that accurate estimation is infeasible, as the rate of convergence is ad-
versely proportional to the dimension d. Conversely, in the trusted central server setting,
we developed algorithms that achieve an optimal rate of convergence. We also explored

26

Simulation Results for Simultaneous Inference

(n, m, d, s∗, s0, ϵ)
(3000,15,800,15,8,0.8)

cov

0.981

(4000,15,800,15,8,0.8)

0.985

(5000,15,800,15,8,0.8)

0.987

(4000,10,800,15,8,0.8)

0.989

(4000,20,800,15,8,0.8)

0.983

(4000,15,600,15,8,0.8)

0.993

(4000,15,1000,15,8,0.8) 0.994

(4000,15,800,15,4,0.8)

0.983

(4000,15,800,15,12,0.8) 0.975

(4000,15,800,10,8,0.8)

0.986

(4000,15,800,20,8,0.8)

0.974

(4000,15,800,15,8,0.5)

0.940

(4000,15,800,15,8,0.3)

0.953

covS

0.883

0.910

0.875

0.894

0.898

0.878

0.878

0.772

0.957

0.976

0.850

0.882

0.789

covSc

0.983

0.987

0.990

0.991

0.986

0.995

0.997

0.993

0.974

0.985

0.982

0.940

0.975

len(cov)

0.091

0.079

0.071

0.079

0.079

0.077

0.080

0.079

0.079

0.079

0.078

0.103

0.127

len(covS) len(covSc)
0.066

0.091

0.057

0.051

0.057

0.057

0.057

0.057

0.057

0.058

0.055

0.059

0.083

0.097

0.079

0.071

0.079

0.079

0.077

0.080

0.079

0.079

0.079

0.078

0.102

0.126

Table 5: Simulation results of the private simultaneous inference in different settings.

27

inference challenges, detailing methodologies for both point-wise confidence intervals and
simultaneous inference.

There are several extensions for further research. Currently, our models presume that
each machine operates under a linear regression framework. We can possibly expand
our algorithm to accomodate more complex models, such as generalized linear models,
classification models, or broader machine learning models. Moreover, an interesting
extension would be to refine our understanding of model similarity across machines.
Although Section 5 currently bases model similarity on L0 norms, reflecting non-sparse
patterns, future studies could explore Lp norm-based similarities, particularly focusing on
L1 and L2 norms, to enhance our approach to heterogeneous federated learning settings.

References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings
of the 2016 ACM SIGSAC conference on computer and communications security,
pages 308–318, 2016.

[2] Jayadev Acharya, Clément L Canonne, and Himanshu Tyagi. General lower bounds
for interactive high-dimensional estimation under information constraints. arXiv
preprint arXiv:2010.06562, 2020.

[3] Naman Agarwal, Ananda Theertha Suresh, Felix Yu, Sanjiv Kumar, and H Brendan
cpsgd: Communication-efficient and differentially-private distributed

Mcmahan.
sgd. arXiv preprint arXiv:1805.10559, 2018.

[4] Leighton Pate Barnes, Wei-Ning Chen, and Ayfer Özgür. Fisher information under
local differential privacy. IEEE Journal on Selected Areas in Information Theory,
1(3):645–659, 2020.

[5] Leighton Pate Barnes, Yanjun Han, and Ayfer Ozgur. Lower bounds for learning
distributions under communication constraints via fisher information. Journal of
Machine Learning Research, 21(236):1–30, 2020.

[6] Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth Raghu-
nathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and Bernhard
Seefeld. Prochlo: Strong privacy for analytics in the crowd. In Proceedings of the
26th symposium on operating systems principles, pages 441–459, 2017.

[7] Mark Braverman, Ankit Garg, Tengyu Ma, Huy L Nguyen, and David P Woodruff.
Communication lower bounds for statistical estimation problems via a distributed
data processing inequality. In Proceedings of the forty-eighth annual ACM sympo-
sium on Theory of Computing, pages 1011–1020, 2016.

28

[8] T Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates
of convergence for parameter estimation with differential privacy. arXiv preprint
arXiv:1902.04495, 2019.

[9] T Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy in gen-
eralized linear models: Algorithms and minimax lower bounds. arXiv preprint
arXiv:2011.03900, 2020.

[10] Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approxima-
tions and multiplier bootstrap for maxima of sums of high-dimensional random
vectors. The Annals of Statistics, 41(6):2786–2819, 2013.

[11] John Duchi and Ryan Rogers. Lower bounds for locally private estimation via
communication complexity. In Conference on Learning Theory, pages 1161–1191.
PMLR, 2019.

[12] Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. In Conference

On Learning Theory, pages 1693–1702. PMLR, 2018.

[13] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise
to sensitivity in private data analysis. In Theory of cryptography conference, pages
265–284. Springer, 2006.

[14] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential
privacy. Foundations and Trends in Theoretical Computer Science, 9(3-4):211–407,
2014.

[15] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a
survey of attacks on private data. Annual Review of Statistics and Its Application,
4:61–84, 2017.

[16] Cynthia Dwork, Weijie J Su, and Li Zhang. Differentially private false discovery

rate control. arXiv preprint arXiv:1807.04209, 2018.

[17] Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Shuang
Song, Kunal Talwar, and Abhradeep Thakurta. Encode, shuffle, analyze privacy re-
visited: Formalizations and empirical evaluation. arXiv preprint arXiv:2001.03618,
2020.

[18] Ankit Garg, Tengyu Ma, and Huy Nguyen. On communication cost of distributed
statistical estimation and dimensionality. Advances in Neural Information Process-
ing Systems, 27:2726–2734, 2014.

[19] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learn-

ing: A client level perspective. arXiv preprint arXiv:1712.07557, 2017.

29

[20] Antonious Girgis, Deepesh Data,

and
Ananda Theertha Suresh. Shuffled model of differential privacy in federated learn-
ing. In International Conference on Artificial Intelligence and Statistics, pages 2521–
2529. PMLR, 2021.

Suhas Diggavi, Peter Kairouz,

[21] Yanjun Han, Ayfer Özgür, and Tsachy Weissman. Geometric lower bounds for
distributed parameter estimation under communication constraints. In Conference
On Learning Theory, pages 3163–3188. PMLR, 2018.

[22] Torsten Hothorn, Frank Bretz, and Peter Westfall. Simultaneous inference in gen-
eral parametric models. Biometrical Journal: Journal of Mathematical Methods in
Biosciences, 50(3):346–363, 2008.

[23] Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personal-
ized federated learning with differential privacy. IEEE Internet of Things Journal,
7(10):9530–9539, 2020.

[24] Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis test-
ing for high-dimensional regression. The Journal of Machine Learning Research,
15(1):2869–2909, 2014.

[25] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Ben-
nis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. Advances and open problems in federated learning. Foun-
dations and Trends® in Machine Learning, 14(1–2):1–210, 2021.

[26] Jakub Konecy, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communica-
tion efficiency. arXiv preprint arXiv:1610.05492, 2016.

[27] Jason D Lee, Qiang Liu, Yuekai Sun, and Jonathan E Taylor. Communication-
efficient sparse regression. The Journal of Machine Learning Research, 18(1):115–
144, 2017.

[28] Mengchu Li, Ye Tian, Yang Feng, and Yi Yu. Federated transfer learning with

differential privacy. arXiv preprint arXiv:2403.11343, 2024.

[29] Sai Li, T Tony Cai, and Hongzhe Li. Transfer learning for high-dimensional lin-
ear regression: Prediction, estimation, and minimax optimality. arXiv preprint
arXiv:2006.10593, 2020.

[30] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learn-
ing: Challenges, methods, and future directions. IEEE signal processing magazine,
37(3):50–60, 2020.

[31] Andrew Lowy and Meisam Razaviyayn. Private federated learning without a trusted
server: Optimal algorithms for convex losses. arXiv preprint arXiv:2106.09779, 2021.

30

[32] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,

and
Blaise Aguera y Arcas. Communication-efficient learning of deep networks from
decentralized data. In Artificial intelligence and statistics, pages 1273–1282. PMLR,
2017.

[33] Brendan McMahan and Abhradeep Thakurta. Federated learning with formal dif-

ferential privacy guarantees. Google AI Blog, 2022.

[34] H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning
differentially private recurrent language models. arXiv preprint arXiv:1710.06963,
2017.

[35] Thomas Steinke and Jonathan Ullman. Tight lower bounds for differentially pri-
vate selection. In 2017 IEEE 58th Annual Symposium on Foundations of Computer
Science (FOCS), pages 552–563. IEEE, 2017.

[36] Kunal Talwar, Abhradeep Thakurta, and Li Zhang. Nearly-optimal private lasso. In
Proceedings of the 28th International Conference on Neural Information Processing
Systems-Volume 2, pages 3025–3033, 2015.

[37] Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei. Ldp-
fed: Federated learning with local differential privacy. In Proceedings of the Third
ACM International Workshop on Edge Systems, Analytics and Networking, pages
61–66, 2020.

[38] Sara Van de Geer, Peter Bühlmann, Ya’acov Ritov, and Ruben Dezeure. On asymp-

totically optimal confidence regions and tests for high-dimensional models. 2014.

[39] Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,
Tony QS Quek, and H Vincent Poor. Federated learning with differential privacy:
Algorithms and performance analysis. IEEE Transactions on Information Forensics
and Security, 15:3454–3469, 2020.

[40] Manuel Wiesenfarth, Tatyana Krivobokova, Stephan Klasen, and Stefan Sperlich.
Direct simultaneous inference in additive models and its application to model un-
dernutrition. Journal of the American Statistical Association, 107(500):1286–1296,
2012.

[41] Yang Yu, Shih-Kang Chao, and Guang Cheng. Distributed bootstrap for simulta-
neous inference under high dimensionality. Journal of Machine Learning Research,
23(195):1–77, 2022.

[42] Xianyang Zhang and Guang Cheng. Simultaneous inference for high-dimensional
linear models. Journal of the American Statistical Association, 112(518):757–768,
2017.

31

[43] Yuchen Zhang, John C Duchi, Michael I Jordan, and Martin J Wainwright.
Information-theoretic lower bounds for distributed statistical estimation with com-
munication constraints. In NIPS, pages 2328–2336. Citeseer, 2013.

[44] Yuchen Zhang, Martin J Wainwright, and John C Duchi. Communication-efficient
algorithms for statistical optimization. Advances in neural information processing
systems, 25, 2012.

[45] Zhe Zhang. Differential privacy in statistical learning. ProQuest Dissertations and

Theses, page 156, 2023.

[46] Zhe Zhang and Linjun Zhang. High-dimensional differentially-private em algorithm:
Methods and near-optimal statistical guarantees. arXiv preprint arXiv:2104.00245,
2021.

32

A Proof of main results

A.1 Proof of Theorem 1

We show the proof of the lower bound of the estimation. The main idea of the proof is
as follows, we will first assume that in the general case where each data point on each
machine follows a general distribution pθ, then we will further assume some conditions
of this distribution, and prove that the lower bound of the mean estimation could be
attained under these conditions. Finally, we will show that under the assumptions that
the data points follow the normal distribution, the specific conditions hold, thus we could
finish the proof.

To start this proof, we first introduce the perturbation space A = {−1, 1}k, where
k is a pre-chosen constant and associate each parameter θ with a ∈ A and refer the
distribution pθ as pa. We characterize the distance between two parameters θ and θ′ by
the hamming distance of z and z′, such approach will be compatible with the Assouad’s
method, as will be shown later in the proof. We note that when the hamming distance of
a and a′ get smaller, it indicts that the distance between θ and θ′ becomes closer. Also,
for each a ∈ A, we further denote a⊕i ∈ A as the vector which flips the sign of the i-th
coordinate of a. Then, we state below conditions:

Condition 1 For every a ∈ A and i ∈ [k], it holds that pa⊕i ≪ pa. Further, there exist
qa,i and measurable functions ϕa,i : X → R such that |qa,i| ≤ α, which q is a constant
and:

dpa⊕i
dpa

= 1 + qa,iϕa,i.

Condition 2 For all a ∈ A and i, j ∈ [k], Epa[ϕa,iϕa,j] = 1i=j.

Condition 3 There exists some σ ≥ 0 such that, for all a ∈ A, the random vector
ϕa(X) = (ϕa,i(X))i∈[k] ∈ Rk is σ2-sub-Gaussian for X ∼ pa with independent coordi-
nates.

The above conditions characterize the distribution pa, we will later verify that the Gaus-
sian distribution could satisfy the above conditions in the later proof. Then, we state
our first claim.

Corollary 1 For each coordinate of the A, for any i = 1, 2, . . . , k, fix τ = P(Ai =
1) ∈ (0, 1/2]. Let X1, . . . , Xm be the inputs on the local servers, i.i.d. with common
distribution p⊗n
. Let Zm be the information sent from all the local servers to the central
A
machine generated through the channel W. Then, if the condition 1 satisfies, there exists
a constant c, we have:

(cid:32)

1
k

k
(cid:88)

i=1

(cid:33)2

dTV(pZm

+i , pZm
−i )

≤

c
k

q2mn2 max
a∈A

k
(cid:88)

(cid:90)

i=1

Y

E

[ϕa,i(X)W(z|X)]2
p⊗n
a
E
[W(z|X)]

p⊗n
a

dµ,

where pZm

+i = E[pZm

A |Ai = +1], pZm

−i = E[pZm

A |Ai = −1].

33

The proof of the above corollary is in appendix B.1. The above corollary characterizes
the difference between the distribution of pZm
, which is the difference between
+i
the distribution of the information about the each coordinate of A, which could be seen
as the information between Y and A, namely, the information between the information
and the parameters.

and pZm
−i

In the precious corollary, we just assumed a general channel W, in the following
corollary, we could further specifies the above corollary when the channel W, be a ϵ-
differentially private constraint channel W priv and we could further simplify the upper
bound in Corollary 1.

Corollary 2 If W priv be a privacy constraint channel and for any family of distributions
{pa, a ∈ {−1, 1}k} satisfying condition 1 and condition 2. With the same notations as
Corollary 1 we have:

(cid:32)

1
k

k
(cid:88)

i=1

(cid:33)2

dT V (pZm

+i , pZm
−i )

≤

7
k

mn2q2(enϵ2

− 1)

The proof of the above corollary could be found in appendix B.2. The above corollary
+i , pZm
−i ), in the next corollary, we will focus on
focus on the upper bound of 1
k
the lower bound, which is an Assouad-type bound. We first introduce another condition:

i=1 dT V (pZm

(cid:80)k

Condition 4 Fix p ∈ [1, ∞). Let ρ be the ℓp loss between the true parameter and the
estimation. Then, for every a, a′ ∈ A ∈ {−1, +1}k, the below inequalities hold:

lp(θa, θa′) ≥ 4ρ

(cid:18) dHam(a, a′)
τ k

(cid:19)1/p

,

where dHam(a, a′) denotes the Hamming distance with definition dHam(a, a′) = (cid:80)k
and τ = P(ai = 1) ∈ (0, 1/2] for each coordinate ai.

i=1 1(ai ̸= a′

i),

(cid:80)k

−i ):

+i , pZm

i=1 dT V (pZm

The above condition characterizes the connection between θ with the perturba-
tion space. With the above assumption, we could further obtain the lower bound of
1
k
Corollary 3 Let p ≥ 1 and assume that {pa, a ∈ A}, τ ∈ [0, 1/2] satisfy Condition 4.
Let A be a random variable on {−1, 1}k with distribution Rad(τ )⊗k. Suppose that ˆθ
constitutes an (n, ρ)-estimator of the true parameter θ∗ under lp loss and P[pA ∈ PΘ] ≥
1 − τ /4. Then the below inequality holds:

1
k

k
(cid:88)

i=1

dT V (pZm

+i , pZm

−i ) ≥

n
4

,

where pZm

+i = E[pZm

A |Ai = +1], pZm

−i = E[pZm

A |Ai = −1].

34

The proof of the above corollary could be found in appendix B.3. In the following
proof, we are going to verify that the Gaussian distribution satisfies all the above con-
ditions, thus the result in Corollary 2 and Corollary 3 holds. Then, according to these
two corollaries, we will present the lower bound for the mean estimation in the high-
dimensional federated learning setting.

For the parameters, we could fix p = 2, k = d, A = {−1, +1}d. For the probability
where τ = P(ai) = 1, we fix τ = s
. Let φ denote the probability density function
2d
of the standard Gaussian distribution N (0, I). We first suppose that, for some ρ ∈
(0, 1/8], there exists an (n, ρ)-estimator for the true parameter µ under ℓp loss. Then,
if we have ρ2 ≥ s/n, then we could finish the proof. Otherwise, we fix a parameter
γ = 4ρ√
∈ (0, 1/2], this is possible with a choice of s, the sparsity level. We could
s/2
design the parameter, the mean of the Gaussian distribution µ and A by the formula:
µa = γ(a + 1d), where a ∈ A. Then, we could verify that P[∥µa∥0 ≤ 2τ d] ≥ 1 − τ /4,
where ∥µa∥0 = (cid:80)d
i=1 1ai=1 = ∥a∥+. From the definition of Gaussian density, for a ∈ A,
we have:

pa(x) = e−γ2∥µa∥2

2/2 · eγ⟨x,a+1d⟩ · φ(x).

Therefore, for a ∈ A and i ∈ [d], we have

pa⊕i(x) = e−2γxiaie2γ2ai · pa(x) = (1 + q · ϕa,i(x)) · pa(x),
e4γ2 − 1 and ϕa,i(x) = 1−e−2γxiai e2γ2ai
where q =
generating function, we could verify that, for i ̸= j,

e4γ2 −1

(cid:112)

√

. By using the Gaussian moment-

Epa[ϕa,i(X)] = 0, Epa[ϕa,i(X)2] = 1, and Epa[ϕa,i(X)ϕa,j(X)] = 0,

so that the condition 1 and condition 2 are satisfied. Here, notice that in the proof
of Corollary 1, we require that |q · ϕa,i(x)| = C/n where C is a constant, we could
√
verify that since ρ ≤ c · (cid:112)s/n, then γ ≤ c
n from the definition of γ, Then we have
|q · ϕz,i(x)| ≤ c0|γ2 − γxi| ≤ c0/n, which could verify the condition for corollary 1. Also,
by the choice of γ and ρ, it is easy to verify that condition 4 also holds with:

ℓ2(µ(pa), µ(pa′)) = 4ρ ·

(cid:114)

dham(a, a′)
τ d

.

Thus, all the conditions mentioned above have been verified. Then, we could finish
the proof of our lower bound. Combining the result of corollary 2 and corollary 3, we get
the result below:

n2d ≤ cmn2q2(enϵ2

− 1),

where c is a constant. Also, notice that q2 = e4γ2 − 1 ≤ 8γ2 holds since γ ≤ 1/2, we
could find a constant c0, it follows that

ρ2 ≥ c0 ·

s · d
mnϵ2 ,

35

From the choice of ρ, we could claim that ρ ≥ Ω(
lower bound, which finished the proof.

(cid:113) sd

mnϵ2 ∧ 1), then we could obtain our
□

A.2 Proof of Theorem 2

In the proof of Theorem 2, we will design a mechanism to get an estimation of the
parameter and then we obtain the upper bound of ∥µ − ˆµ∥2
. The overall mechanism is
2
designed as follows: we first calculate the mean for n data points on each machine. Then,
we transform the Gaussian mean to Bernoulli mean according to the sign of the Gaussian
mean motivated by the Algorithm 2 discussed in [2], l-bit protocol for estimating product
of Bernoulli family.

Then, we could use the ϵ-local differentially private mechanism to achieve mean es-
timation for the product of Bernoulli family in the federated learning setting. After
obtaining the estimation, we could convert the estimated Bernoulli mean back to Gaus-
sian mean estimation.

First, for each data point on the machine, it follows the distribution of N (µ, Id) .
Then for the mean on i-th machine, the mean ¯Xi follows a distribution of N (µ, 1/nI).
Then, we could convert it to a Bernoulli variable Z, where Zi = 1 when ¯Xij > 0 and
Zi = −1 when ¯Xij ≤ 0. Then the mean of Z, which denote as v is:

vi = 2P(Xi > 0) − 1 = Erf

(cid:18) √

nµi√
2

(cid:19)
,

for each coordinate of v. Suppose the estimation of v is denoted by ˆv, then suppose the
estimation ˆµ is given by ˆµi =

erf−1( ˆvi), we could find such relationship:

√
2√
n

∥µ − ˆµ∥2

2 =

d
(cid:88)

i=1

|µi − ˆµi|2 =

2
n

·

d
(cid:88)

i=1

|Erf−1(vi) − Erf−1(ˆvi)|2 ≤ c ·

1
n

·

d
(cid:88)

i=1

|vi − ˆvi|2,

(A.1)

where c is a constant. The last inequality comes from the Lipschitz condition of a Erf
function. Then, we could get the upper bound for the Bonoulli mean estimation directly
from Theorem 3 in [2], where

∥v − ˆv∥2

2 ≤ c ·

d · s
mϵ2 ,

where ϵ is the privacy parameter, m is the number of machines. Combining the last
two inequalities (A.1) and (A.2), we could get the upper bound for the mean Gaussian
estimation:

∥µ − ˆµ∥2

2 ≤ c ·

s · d
mnϵ2 ,

which finished the proof.

□

36

A.3 Proof of Theorem 3

It is not difficult to observe that the convergence rate would be the same as in the
non-federated learning setting. We denote Ln as the sample loss function and L be the
population level.
and Ln is the sample
version.
k ΣΘk − ⟨ej, Θk⟩. We start from the
estimation of β and the estimation of Θ is the same. In this proof, we use n0 to refer
the total number of samples n0 = m · n. Then, it holds that:

In the estimation of β, L(β) = ∥Y − Xβ∥2
2

In the estimation of Θk, L(Θk) = 1

2 Θ⊤

Lemma A.1 Under assumptions of Theorem 5, it holds that:

8νs∥βt − ˆβ∥2

2 ≤ ⟨∇Ln(βt) − ∇Ln( ˆβ), βt − ˆβ⟩ ≤ 8µs∥βt − ˆβ∥2
2.

(A.2)

Proof: From direct calculation, we could obtain that:

⟨∇Ln(βt) − ∇Ln( ˆβ), βt − ˆβ⟩ = 2(βt − ˆβ)T ˆΣ(βt − ˆβ) ≤ 2µs+s∗∥βt − ˆβ∥2

2 ≤ 2µ2s∥βt − ˆβ∥2
2
The last inequality is according to the choice of s such that s∗ ≤ s. Then, we also have
µ2s ≤ 4µs. Thus we have obtained the right hand side of the inequality. By a similar
approach, we could also obtain the left hand side.

Lemma A.2 Under assumptions of Theorem 5, it holds that there exists an absolute
constant ρ such that

Ln(βt+1) − Ln( ˆβ) ≤

(cid:18)

1 −

(cid:19) (cid:16)

νs
24µs

Ln(βt) − Ln( ˆβ)

(cid:17)



+ c3



(cid:88)

i∈[s]

∥wt

i∥2

∞ + ∥ ˜wt

St+1∥2
2



 ,

where c3 is a constant number such that c3 = max(µs(72 · 8µs + 13), 68µs + 2/3)

(A.3)

Notice that wi, w are injected from the NoisyHT algorithm. The proof of the above
lemma follows from the result in Lemma 8.3 from [8]. Then, we could start the proof by
iterating (A.3) over t. Denote Wt = c3

to obtain

(cid:16)(cid:80)

∞ + ∥ ˜wt

i∈[s] ∥wt

i∥2

St+1∥2
2

(cid:17)

Ln0(βT ) − Ln0( ˆβ) ≤

(cid:18)

1 −

(cid:18)

≤

1 −

νs
24µs

νs
24µs

(cid:19)T (cid:16)

Ln0(β0) − Ln0( ˆβ)

T −1
(cid:88)

(cid:18)

(cid:17)

+

1 −

(cid:19)T −k−1

Wk

1
ρL2

(cid:19)T

4µc2

0 +

T −1
(cid:88)

(cid:18)

1 −

k=0

νs
24µs

k=0
(cid:19)T −k−1

Wk.

(A.4)

The second inequality is a consequence of the upper inequality in (A.2) and the ℓ2 bounds
of β0 and ˆβ. We can also bound Ln0(βT ) − Ln0( ˆβ) from below by the lower inequality
in (A.2):

Ln0(βT ) − Ln0( ˆβ) ≥ Ln0(βT ) − Ln0(β∗) ≥ 4νs∥βT − β∗∥2

2 − ⟨∇Ln0(β∗), β∗ − βT ⟩.

(A.5)

37

Now (A.4) and (A.5) imply that, with T = (ρL2) log(cid:0)8c2

0Ln0

(cid:1),

4νs∥βT − β∗∥2

2 ≤ ∥∇Ln0(β∗)∥∞

≤ ∥∇Ln0(β∗)∥∞

Thus,

√

√

s + s∗∥β∗ − βT ∥2 +

s + s∗∥β∗ − βT ∥2 +

1
n0

1
n0

T −1
(cid:88)

(cid:18)

+

1 −

k=0

νs
24µs

+

24µs
νs

max
k

Wk.

(cid:19)T −k−1

Wk.

(A.6)

(A.7)

2 ≤ k ·

∥βT − β∗∥2

s∗ log d
n0
In the above inequality, k is a constant. Then, we could calculate the upper bound of
Wk. From the result of tail bound of Laplace random variables, we could find that with
high probability that Wk ≤ c4s2 log2 d log(1/δ) log n3/n2ϵ2), where c4 = max(µs(9µs +
1/4), 17/16µs + 1/96). Then, we have with high probability:

max
k

µs
ν2
s

Wk.

+

∥βT − β∗∥2

2 ≤ k ·

s log d
n0

+

6c4µs
ν2
s

s2 log2 d log(1/δ) log n3

0/n2

0ϵ2.

Similarly, we could obtain the same result for the estimation of ˆΘk, which finishes the
proof.

A.4 Proof of Theorem 4

The structure of the proof consist of three part, the first part is to show that our algorithm
provides an (ϵ, δ)-differentially private confidence interval. In the second part, we will
show that ˆβk is a consistent estimator of true βk, which is unbiased. In the last part, we
will show that the (1 − α) confidence interval is asymptotically valid. Before we start the
first part, let us first analyze cx:
According to the assumptions of the theorem, we have learnt that for each row of X,
xΣ−1/2 is sub-Gaussian with κ = ∥Σ−1/2x∥ψ2
. Then according to the properties of
sub-Gaussian random variables, we have: ∥xΣ−1/2∥∞ ≤ 3(cid:112)2κ2 log d with probability
1 − d−2. Then for each element of xi, i = 1, 2, . . . , d, we have:

xi = e⊤

j x = e⊤

j Σ1/2Σ−1/2x

Thus,

xi ≤ ∥e⊤

j Σ1/2∥1∥Σ−1/2x∥∞ ≤ ∥Σ1/2∥2∥Σ−1/2x∥∞

Then, with probability 1 − d−2, we have xi ≤ 3(cid:112)2Lκ2 log d. By a union bound, we could
have with probability 1−d−1, ∥x∥∞ ≤ 3(cid:112)2Lκ2 log d. By the choice of cx in the theorem,
we have ∥x∥∞ ≤ cx with a high probability.
Then, we could verify that the confidence interval is (ϵ, δ)-differentially private. From
[8], we could obtain that the output ˆβu is (ϵ, δ)-DP. In a similar manner, we could also

38

verify that the output ˆΘk is also (ϵ, δ)-DP. Thus, for two adjacent data sets (X, Y ) and
(X ′, Y ′) which differ by one data (xij, yij) and (x′

ij), we have:

ij, y′

|

1
n0

( ˆΘ⊤

k xijΠR(yij) − ˆΘ⊤

k xijx⊤
ij

ˆβu)| ≤

≤

≤

1
n0
1
n0
1
n0

| ˆΘ⊤

k xijΠR(yij)| +

| ˆΘ⊤

k xijx⊤
ij

ˆβu|

| ˆΘ⊤

k xij||ΠR(yij)| +

| ˆΘ⊤

k xij||x⊤
ij

ˆβu|

1
n0
1
n0
sc0c1c2
x

√

sc1cxR +

1
n0

Thus,

|

1
n0

( ˆΘ⊤

k xijΠR(yij) − ˆΘ⊤

k xijx⊤
ij

= |

≤

1
n0
2
n0

( ˆΘ⊤

k xijΠR(yij) − ˆΘ⊤
2
n

sc0c1c2
x

sc1cxR +

√

ˆβu) −

1
n0
ˆβu)| + |

( ˆΘ⊤

k x′
1
n0

k xijx⊤
ij

ijΠR(y′

ij) − ˆΘ⊤

k x′

ijx′⊤
ij

ˆβu)|

( ˆΘ⊤

k x′

ijΠR(y′

ij) − ˆΘ⊤

k x′

ijx′⊤
ij

ˆβu)|

√

Denote ∆1 =
(ϵ, δ)-DP. For the term ˆΘ⊤
k

sc1cxR + sc0c1c2
x

ˆΣ ˆΘk, we could obtain that:

. Thus, if Ek follows N (0, 8∆2

1/n2

0ϵ2 log(1.25/δ)), ˆβj is

ˆΘ⊤
k

ˆΣ ˆΘk =

1
n0

n
(cid:88)

i=1

ˆΘ⊤

k xijx⊤
ij

ˆΘk =

1
n0

n
(cid:88)

i=1

( ˆΘ⊤

k xij)2

Thus, for two adjacent data sets X and X ′ differ by one data xij and x′
ij

, we have:

| ˆΘ⊤
k

ˆΣ ˆΘk − ˆΘ⊤
k

ˆΣ′ ˆΘk| ≤

1
n0

( ˆΘ⊤

k xij)2 +

1
n0

( ˆΘ⊤

k x′

ij)2

By Holder inequality and Cauchy inequality, we have | ˆΘ⊤

k xij| ≤

√

sc1cx, thus we have:

| ˆΘ⊤
k

ˆΣ ˆΘk − ˆΘ⊤
k

ˆΣ′ ˆΘk| ≤

√

(

2
n0

sc1cx)2 =

2
n0

sc2

1c2
x

1c2
x

. Then, let E′ follows a Gaussian distribution of N (0, 8∆2

Denote ∆2 = sc2
We could claim that ˆΘ⊤
ˆΣ ˆΘk + E′ is (ϵ, δ)-differentially private.
k
We start the second part of the proof. First, with probability 1 − k0 exp(−k1n0), we have
ΠR(yi) = yi for each i = 1, 2, . . . , d, so we could decompose ˆβk by the following approach:

2/n2m2ϵ2 log(1.25/δ)).

ˆβk = ˆβu

k +

= ˆβu

k +

1
n0
1
n0

ˆΘ⊤

k X ⊤(Xβ + W − X ˆβu) + Ek

ˆΘ⊤

k X ⊤X(β − ˆβu) +

ˆΘ⊤

k X ⊤W + Ek

1
n0
1
n0

ˆΘ⊤

k X ⊤W + Ek

= βk + ( ˆΘ⊤
k

ˆΣ − ek)(β − ˆβu) +

39

Thus, we have:

√

n0( ˆβj − βj) =

n0( ˆΘ⊤
k

√

(cid:124)

ˆΣ − e⊤
(cid:123)(cid:122)
A.8.1

k )(β − ˆβu)
(cid:125)

+

1
√
n0

(cid:124)

ˆΘ⊤

k X ⊤W
(cid:123)(cid:122)
(cid:125)
A.8.2

+

√

n0Ek
(cid:124) (cid:123)(cid:122) (cid:125)
A.8.3

(A.8)

We will analyze the three terms in (A.8) one by one. For the first term, we could further
decompose this term as:

√

n0( ˆΘ⊤
k

ˆΣ − e⊤

k )(β − ˆβu) =
=

√

√

n0( ˆΘ⊤
k
n0( ˆΘ⊤
k

ˆΣ − Θ⊤
k
ˆΣ − Θ⊤
k

ˆΣ + Θ⊤
k
ˆΣ)(β − ˆβu) +

k )(β − ˆβu)
ˆΣ − e⊤
√
n0(Θ⊤
k

ˆΣ − e⊤

k )(β − ˆβu)
(A.9)

For the first term in (A.9), we could further decompose this term from ˆΣ = 1
mn

(cid:80)m

i=1

(cid:80)n

j=1 xijx⊤
ij

:

√

n0( ˆΘ⊤
k

ˆΣ − Θ⊤
k

ˆΣ)(β − ˆβu) =

≤

√

√

k − Θ⊤

n0( ˆΘ⊤
n0λs( ˆΣ)∥ ˆΘk − Θk∥2|β − ˆβu∥2

k ) ˆΣ(β − ˆβu)

(A.10)

In the last inequality, we use λs to denote the largest s-restricted eigenvalue of the
covariance matrix ˆΣ. From Theorem 3, we could obtain that there exists a constant c
such that:

(s∗ log d)2 log(1/δ) log3 n0
n2
0ε2
Also, for the output ˆΘk, we could have the similar result:

(cid:18) s∗ log d
n0

∥β − ˆβu∥2

2 ≤ c · σ2

+

(cid:19)

∥ ˆΘk − Θk∥2

2 ≤ c · σ2

(cid:18) s∗ log d
n0

+

(s∗ log d)2 log(1/δ) log3 n0
n2
0ε2

(cid:19)

Combining (A.4) and (A.4), we could obtain that

√

n0( ˆΘ⊤
k

ˆΣ − Θ⊤
k

ˆΣ)(β − ˆβu) = o

(cid:19)

(cid:18) s∗ log d
√
n0

= o(1)

Then, we could focus on the second term of (A.9). We first introduce the following
lemma:

Lemma A.3 (Lemma 6.2 in [24]) For the vector Θ⊤
k
then with probability 1 − 2d1−a2/24e2κ4L2, we have:

ˆΣ − ek. Denote κ = ∥Σ−1/2X1∥ϕ2

,

∥Θ⊤
k

ˆΣ − ek∥∞ ≤ a

(cid:114) log d
n0

Thus, for the second term of (A.9), we have:

√

n0(Θ⊤
k

ˆΣ − e⊤

j )(β − ˆβu) ≤

√

n0∥Θ⊤
k

ˆΣ − e⊤

k ∥∞∥β − ˆβu∥1

40

√

n0

≤ k

(cid:114) log d
n0

√

≤ k · (cid:112)s∗ log d ·

s∗∥β − ˆβu∥2
(cid:114) s∗ log d
n0

= o(1)

(A.11)

Combine the result from (A.4), (A.11) to (A.9), we could obtain that the first term of
(A.8) is o(1). We could also analyze the third term of (A.8),√
Then, by the definition of ∆1, we have 8∆2
from the assumption. Also, we notice that E′ = N (0, c · s∗2 log2 d log(1.25/δ)
n2
0ϵ2
concentration of Gaussian distribution, we also have that E′ = o(1).

1 log(1.25/δ)/n0ϵ2 ∼ s∗2 log2 d log(1.25/δ)

1 log(1.25/δ)/n0ϵ2).
= o(1)
). By the

n0Ek ∼ N (0, 8∆2

n0ϵ2

Finally, we analyze the term 1√

ˆΘ⊤
random noise. Then, from the central limit theorem, we could conclude that:

j X ⊤W . From our definition, W is sub-Gaussian

n0

1
√
n0

ˆΘ⊤

j X ⊤W → N (0, σ2 ˆΘ⊤
j

ˆΣ ˆΘj)

Thus, √

n0( ˆβj − βj) = 1√
n0

ˆΣ ˆΘj + o(1)). Also, from
lemma 4.1, we could claim that under our assumptions, ˆσ2 = σ2 + o(1). We could get
the result where with high probability,

n0Ek ∼ N (0, σ2 ˆΘ⊤
j

j X ⊤W +

ˆΘ⊤

→ N (0, 1).

√

√
n0( ˆβj −βj )
√
⊤ ˆΣ ˆwj

ˆwj

ˆσ

Therefore, we could claim that [ ˆβj − Φ−1(1 − α/2) ˆσ√
n0

ˆΣ ˆΘj, ˆβj + Φ−1(1 −
ˆΣ ˆΘj] is asymptotically 1 − α confidence interval for βj. Therefore, we
□

α/2) ˆσ√
n0
have finished the proof of theorem.

(cid:113) ˆΘ⊤

j

j

(cid:113) ˆΘ⊤

A.5 Proof of Theorem 5

The proof is similar to the proof of Theorem 4, the difference is that we need to consider
the case where the privacy cost is not dominated by the statistical error. Then, for the
proof of Theorem 5, we follow the proof of Theorem 4 until (A.8). The analysis for the
second term and the third term for (A.8) stays the same. On the other hand, for the
first term of (A.8), we have: We will analyze the three terms in (A.8) one by one. For
the first term, in the same manner, we could decompose this term as:

√

n0( ˆΘ⊤
k

ˆΣ − e⊤

k )(β − ˆβu) =
=

√

√

n0( ˆΘ⊤
k
n0( ˆΘ⊤
k

ˆΣ − Θ⊤
k
ˆΣ − Θ⊤
k

ˆΣ + Θ⊤
k
ˆΣ)(β − ˆβu) +

k )(β − ˆβu)
ˆΣ − e⊤
√
n(Θ⊤
k

ˆΣ − e⊤

k )(β − ˆβu)
(A.12)

For the first term in (A.12), we could further decompose this term from ˆΣ = 1
n

(cid:80)n

i=1 xix⊤
i

:

√

n0( ˆΘ⊤
k

ˆΣ − Θ⊤
k

ˆΣ)(β − ˆβu) =

≤

≤

√

√

k − Θ⊤

n0( ˆΘ⊤
n0λs( ˆΣ)∥ ˆΘk − Θk∥2|β − ˆβu∥2

k ) ˆΣ(β − ˆβu)

kµ2
s
ν2
s

s2 log2 d log(1/δ) log3 n0/n3/2

0

ϵ2

(A.13)

41

Thus, for the second term of (A.12), by Lemma A.3, we have:

√

n0(Θ⊤
k

ˆΣ − e⊤

k )(β − ˆβu) ≤

√

k ∥∞∥β − ˆβu∥1

ˆΣ − e⊤
√

n0∥Θ⊤
k
(cid:114) log d
√
≤ k
n0
≤ k · (cid:112)s∗ log d · ∥β − ˆβu∥2

n0

s∗∥β − ˆβu∥2

(A.14)

When the privacy cost is not dominated by the statistical error and also s∗ log d/
n =
o(1), we can observe that the equation (A.14) has smaller convergence rate that (A.13).
Then, combining (A.13) and (A.14), there exists a constant k1, such that:

√

√

n( ˆΘ⊤
k

ˆΣ − e⊤

j )(β − ˆβu) ≤

γµ2
s
ν2
s

s2 log2 d log(1/δ) log3 n0
n3/2
0

ϵ2

Then, insert the result into (A.8), we have:

√

(cid:32)

n0( ˆβj − βj) = O

−

µ2
s
ν2
s

s2 log2 d log(1/δ) log3 n0
n3/2
0

ϵ2

(cid:33)

+

1
√
n0

ˆΘ⊤

k X ⊤W +

√

n0Ek (A.15)

Notice that for the first term on the right hand, the constant could be set to 1 because
it comes from the tail bound of Laplace random variable. From the result in (A.15), we
could also apply the central limit theorem to show that the second term is asymptoti-
cally Gaussian, notice that in the right hand side, the second term and the third term
asymptotically follows a distribution of N (0, σ2 ˆΘ⊤
). Also, by the con-
k
centration of Gaussian distribution, we have with high probability, E′ ≤ 8∆2
1 log(1/δ)
.Thus,
n0ϵ2
the privacy conditions are satisfied. Therefore, we have:

ˆΣ ˆΘk + 8∆2

1 log(1/δ)
n0ϵ2

√

n0[ ˆβj − βj −

√

n0(Θ⊤
k

ˆΣ − e⊤

k )(β − ˆβu)]/

σ2 ˆΘ⊤
k

ˆΣ ˆΘk +

(cid:115)

8∆2

1 log(1/δ)
n0ϵ2

∼ N (0, 1)

Then, also by our assumptions and the result in Lemma 4.1, we could claim that σ2 =
2
ˆsigma

+ o(1). Thus, finally, the confidence interval is given by:

Jj(α) =

(cid:20)
ˆβj −

ˆβj +

2

γ ˆµs
2
ˆνs

2

γ ˆµs
2
ˆνs

s2 log2 d log(1/δ) log3 n0
n2
0ϵ2
s2 log2 d log(1/δ) log3 n0
n2
0ϵ2

) − Φ−1(1 − α/2)

(cid:115)

ˆΘ⊤
k

ˆΣ ˆΘk +

σ
√
n0

) + Φ−1(1 − α/2)

(cid:115)

ˆΘ⊤
k

ˆΣ ˆΘk +

σ
√
n0

8∆2

1 log(1/δ)
n0ϵ2

,

(cid:21)

8∆2

1 log(1/δ)
n0ϵ2

which finishes our proof.

42

A.6 Proof of Theorem 6

Let us first show that our algorithm is ϵ, δ private. The major proof lies in the choice of
noise level B3. To decompose, we can find:

ˆΘ

1
√
m

√

ei

m
(cid:88)

i=1

n(gi − ¯g) = ˆΘ

1
√
m

m
(cid:88)

i=1

√

ei

ngi − ˆΘ

√
√

n
m

(cid:32) m
(cid:88)

(cid:33)

ei

¯g

i=1

Then, suppose in an adjacent data set, the different data in denoted as (xij, yij) and
(x′

√

ei

ng′

i − ˆΘ

m
(cid:88)

i=1

√
√

n
m

(cid:32) m
(cid:88)

(cid:33)

ei

¯g′)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

ij, y′

ij). Then, we calculate:
√
√

ngi − ˆΘ

m
(cid:88)

√

ei

(cid:13)
(cid:13)
( ˆΘ
(cid:13)
(cid:13)
(cid:13)

≤

i=1

1
√
m
√
√

(cid:13)
(cid:13)
ˆΘ
(cid:13)
(cid:13)
(cid:13)

n
m

ei(gi − g′

i) − ˆΘ

≤

(cid:13)
(cid:13)
ˆΘ
(cid:13)
(cid:13)
(cid:13)max
(cid:13)

√
√

n
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ (

(cid:13)
ˆΘ − Θ
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)max

ei(gi − g′

i) −

(cid:13)
(cid:13)
(cid:13)
+ ∥Θ∥max)
(cid:13)
(cid:13)

√
√

n
m

(cid:33)

(cid:32) m
(cid:88)

ei

i=1
(cid:32) m
(cid:88)

ei

n
m
√
√

n
m

¯g) − ( ˆΘ

1
√
m
(cid:13)
(cid:13)
(¯g − ¯g′)
(cid:13)
(cid:13)
(cid:13)∞

(cid:33)

i=1

√
√

n
m

(cid:32) m
(cid:88)

i=1

(cid:33)

ei

ei(gi − g′

i) −

≤ (

(cid:13)
ˆΘ − Θ
(cid:13)
(cid:13)

4

≤ L

≤ (o(1) + L)
√
√
√
√

log m
mn
log m
mn

≤ L

4

√
√

n
m

(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)
+ ∥Θ∥2)
(cid:13)
(cid:13)
√
√

(cid:13)(gi − g′

(cid:112)log m(cid:13)

n
m
(cid:13)
(cid:13)
(cid:13)xij(πR(yij) − xij ˆβ)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
ei(gi − g′
i)
(cid:13)
(cid:13)∞
√
√

i)(cid:13)
(cid:13)∞ +

+

n
m

cx(R + c0cx

√

s∗)

(cid:13)
(cid:13)
(¯g − ¯g′)
(cid:13)
(cid:13)
(cid:13)∞
(cid:32) m
(cid:88)

√
√

n
m

n
m

√
√

i=1
(cid:32) m
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
m(cid:112)log m(cid:13)

i=1

(cid:33)

ei

(cid:33)

ei

(cid:13)
(cid:13)
(¯g − ¯g′)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:13)
(¯g − ¯g′)
(cid:13)
(cid:13)
(cid:13)∞

)

(cid:13)¯g − ¯g′(cid:13)

(cid:13)∞)

Thus, the privacy could be guaranteed. Then, let us start the proof of consistency.
Throughout the proof, we define n0 = m · n, (X, Y ) be the whole data set where
X ∈ Rn0∗d and Y ∈ Rd. U ′ = maxk∈G
n(gi − ¯g). Let us define another
multiplier bootstrap statistic:

ˆΘ 1√
m

i=1 ei

(cid:80)m

√

U ∗ = max
k∈G

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

Σ−1xij(yij − xijβ)eij,

where eij are all standard Gaussian variables. At the same time, we also define:

M0 = max
k∈G

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

Σ−1xij(yij − xijβ)

43

k xij(yij − xijβ)]2 = σ2ΘT

The proof consists three major steps, we start from the first step and measure supα∈(0,1) |P(M0 ≤
CU ∗(α)) − α|. This measurement is quite straightforward, we could apply Theorem 3.1
from [10]. However, we need to verify Corollary 2.1 from [10]. Notice that for any
k, E[ΘT
k ΣΘk ≥ σ2/L. Also, it is not difficult to verify that
k xij(yij − xijβ) is sub-exponential. Since from assumption D1, we have xij is sub-
ΘT
Gaussian and from the linear model, we know that (yij − xijβ) is also, sub-Gaussian.
Then, the condition could be verified. Thus, by applying Theorem 3.1 and also under
the condition where there exists a constant k, k0, k1 such that log7(dmn)/mn ≤ 1
we
could have:

(mn)k

|P(T0 ≤ CU ∗(α)) − α| ≤ k0 ·

sup
α∈(0,1)

1
(mn)k1

+ k2v1/3(max(1, log(d/v)))2/3 + P (∆ > v)

≤ k2v1/3(max(1, log(d/v)))2/3 + P (□ > v) + o(1), (A.16)

where □ represents the maximum element between the two matrix Ω1 and Ω2, denote as
∥Ω1 − Ω2∥max, where Ω1 and Ω2 are defined as:

[Ω1]k,l =

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

and

Θ⊤

k xij(yij − xT

ijβ)Θl

Ω2 = σ2Θ
Then, from Corollary 3.1 in [10] and Lemma E.2 in [41], we could verify that ∥Ω1 −
). With a proper choice of v, e.g, there exists a constant
Ω2∥max = O(

+ log2(dn0) log d
n0

)1−κ, we have k2v1/3(max(1, log(d/v)))2/3 + P (□ >
κ and let v = (
v) = o(1). Next, we would like to associate M with M0. Similarly, from Theorem 3.2 in
[10] and (A.16), we could have:

+ log2(dn0) log d
n0

(cid:113) log d
n0
(cid:113) log d
n0

|P(M ≤ CU ∗(α)) − α| ≤ o(1) + v1

(cid:112)max(1, log(d/v1)) + P(∥M − M0∥ > v1),

sup
α∈(0,1)

From the definition of M and M0, we have:

√

n0(M − M0) = max
1≤k≤d

1
√
n0

⊤

|( ˆΘk

− Θ⊤

k )X ⊤W |

Then, for any k in 1, . . . , d, by Holder inequality and Cauchy–Schwarz inequality, we
have:

1
√
n0

|( ˆwk

⊤ − w⊤

k )X ⊤W | ≤ ∥ ˆwk

⊤ − w⊤

k ∥1∥

X ⊤W ∥∞ ≤

√

1
√
n0

s∗∥ ˆwk

⊤ − w⊤

k ∥2∥

1
√
n0

X ⊤W ∥∞

On one hand, from previous proof, we obtain that ∥ ˆwk
when the
privacy cost is dominated by statistical error uniformly for k. On the other hand, by the

k ∥2 = c ·

⊤ − w⊤

(cid:113) s∗ log d
mn

44

fact that Σ have bounded maximum eigenvalue and traditional linear regression model,
).
we could apply Bernstein inequality and also obtain that ∥ 1√
n0
Combine these two results, we could claim that there exist constants k0 such that:

X ⊤W ∥∞ is O(

(cid:113) log d
n0

1
√
n0

⊤

|( ˆΘk

− Θ⊤

k )X ⊤W | = k0 · (s∗ log d/

√

n0)

uniformly for all k, then we can choose v1 properly such that supα∈(0,1) |P(M ≤ CU ∗(α))−
α| = o(1). At last, we need to relate U ∗ with U . Our major goal is to prove that CU (α)
and CU ∗ are close to each other for any α ∈ (0, 1). We first associate U with U ′. From
the design of private max algorithm, from Lemma 3.4 in [8], suppose l1 is the element
chosen from U ′ and l2 is from U without noise injection, we use w to represent the noise
injected when we pick the largest value privately, we find that, for any c > 0:

2 ≤ l2
l2

1 ≤ (1 + c)l2

2 + 4(1 + 1/c)∥w∥2
∞

From Lemma A.1 in [8], we can verify that there exists constant k0, k1 such that ∥w∥2
∞ ≤
k0 · s∗ log4 d log m
, then from the conditions,
we could claim that l1 = l2 + o(1), also notice that the scale of noise we injected is small,
it is easy to verify that U = U ′ + o(1). The following discussions will be between U ′ and
U ∗. Denote ⊖ as the symmetric difference, then we have:

. When we choose c = o(1), e.g, c = k1

s∗ log d
n0

n0

P(T ≤ CU (α) ⊖ T ≤ CU ∗(α))
≤ 2P(CU ∗(α − π(u)) < T ≤ CU ∗(α + π(u))) + P(CU ∗(α − π(u)) > CU (α)) + P(CU ∗(α + π(u)) < CU (α))

(A.17)

For the first term in (A.17), define π(u) = u1/3 max(1, log(d/u))2/3, then exist a constant
k0, such that:

P(CU ∗(α−π(u)) < M ≤ CU ∗(α+π(u))) ≤ P(M ≤ CU ∗(α+π(u)))−P(M ≤ CU ∗(α−π(u))) ≤ k·π(u)+o(1)

Then, for the second term and third term in (A.17), from Lemma 3.2 in [10], we have:

P(CU ∗(α − π(u)) > CU (α)) + P(CU ∗(α + π(u)) < CU (α)) ≤ 2P(∥Ω1 − Ω3∥max > u),

where Ω3 is defined as:

[Ω3]k,l =

1
m

m
(cid:88)

i=1

n ˆΘk(gi − ¯g)(gi − ¯g)⊤ ˆΘl,

and Ω1 is defined the same as we defined before. Then, our major focus is to analyze
∥Ω1 − Ω3∥max, by triangle inequality, we have ∥Ω1 − Ω3∥max ≤ ∥Ω1 − Ω2∥max + ∥Ω3 −
Ω2∥max. Since we have analyzed ∥Ω1 − Ω2∥max before, we will focus on ∥Ω3 − Ω2∥max.

∥Ω3 − Ω2∥max ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

m
(cid:88)

i=1

(cid:13)
(cid:13)
n ˆΘ(gi − ¯g)(gi − ¯g)⊤ ˆΘ − σ2 ˆΘΣ ˆΘ
(cid:13)
(cid:13)
(cid:13)max

+ ∥σ2 ˆΘΣ ˆΘ − σ2Θ∥max

(A.18)

45

We will analyze the two terms separately. We start from the second term in (A.18), we
have:

∥ ˆΘΣ ˆΘ − Θ∥max
≤ ∥( ˆΘ − Θ + Θ)Σ( ˆΘ − Θ + Θ) − Θ∥max
≤ ∥ ˆΘ − Θ∥2
s∗2 log d
n0

1∥Σ∥max + 2∥ ˆΘ − Θ∥1
(cid:114) log d
n0

+ k1s∗

≤ k0

On the other hand, for the first term in (A.18),notice that:

1
m

m
(cid:88)

i=1

n ˆΘ(gi − ¯g)(gi − ¯g)⊤ ˆΘ =

1
m

m
(cid:88)

i=1

n ˆΘgig⊤
i

ˆΘ − n ˆΘ¯g ¯g⊤ ˆΘ⊤

(A.19)

Denote the data set on the i-th local machine as (Xi, Yi) and in the linear model, the
random noise as Wi. Also, we can further decompose the first term by:

1
m

m
(cid:88)

i=1

ngig⊤
i

=

=

1
m

1
m

m
(cid:88)

i=1
m
(cid:88)

i=1

n[

X ⊤

i Wi + X ⊤

i (β − ˆβ)

n

][

X ⊤

i Wi + X ⊤

i (β − ˆβ)

]⊤

n[

X ⊤
i Wi
n

][

X ⊤
i Wi
n

]⊤ +

1
m

m
(cid:88)

i=1

n[

n
i (β − ˆβ)
n

X ⊤

X ⊤

i (β − ˆβ)
n

][

]⊤ +

2
m

m
(cid:88)

i=1

n[

X ⊤

i (β − ˆβ)
n
(A.20)

][

X ⊤
i Wi
n

]⊤

Then, for the equation (A.19), we have:

∥

1
m

m
(cid:88)

i=1

n ˆΘ(gi − ¯g)(gi − ¯g)⊤ ˆΘ − σ2 ˆΘΣ ˆΘ∥max

≤ ∥ ˆΘ∥max∥

≤ ∥ ˆΘ∥2

max∥

1
m

1
m

m
(cid:88)

i=1
m
(cid:88)

i=1

n(gi − ¯g)(gi − ¯g)⊤ ˆΘ − σ2Σ ˆΘ∥max

n(gi − ¯g)(gi − ¯g)⊤ − σ2Σ∥max

(A.21)

And, we could insert (A.20) into (A.21),

∥

1
m

m
(cid:88)

i=1

n(gi − ¯g)(gi − ¯g)⊤ − σ2Σ∥max

≤ ∥

1
m

m
(cid:88)

i=1

n[

X ⊤
i Wi
n

][

X ⊤
i Wi
n

]⊤ − σ2Σ∥max + n∥¯g ¯g⊤∥max + ∥

1
m

m
(cid:88)

i=1

n[

X ⊤

i (β − ˆβ)
n

X ⊤

i (β − ˆβ)
n

][

]⊤∥max

46

+ ∥

2
m

m
(cid:88)

i=1

n[

X ⊤

i (β − ˆβ)
n

][

X ⊤
i Wi
n

]⊤∥max

(A.22)

We will analyze the four terms in (A.22) one by one. For the first term, it is quite simple,
m + log2(dm) log d
from the proof of Lemma F.2 in [41], we have the first term is Op(
For the second term, we have:

(cid:113) log d

).

m

n∥¯g ¯g⊤∥max ≤ n∥¯g∥2

∞ = n∥

1
n0

X ⊤(Y − X ˆβ)∥2
∞

Also, we have:

∥

1
n0

X ⊤(Y − X ˆβ)∥∞

≤ ∥

≤ ∥

1
n0
1
n0

≤ k0(

≤ k0

X ⊤(Y − Xβ)∥∞ + ∥

1
n0

X ⊤X( ˆβ − β)∥∞

X ⊤W ∥∞ + ∥( ˆΣ − Σ)( ˆβ − β)∥∞ + ∥Σ∥max∥ ˆβ − β∥1
(cid:114) log d
n0
(cid:114) log d
n0

)∥ ˆβ − β∥1 + ∥ ˆβ − β∥1

(cid:114) log d
n0

(cid:114) log d
n0

s∗ log d
n0

+ k2s∗

) + k1(

+ k1

(A.23)

Thus, for the second term, we can obtain that n∥¯g ¯g⊤∥max ≤ k0s∗2 log d/m+k1s∗2 log2 d/m2n.
For the third term, we have:

X ⊤

∥

1
m

m
(cid:88)

i=1

n[

]⊤∥max

X ⊤

i Xi(β − ˆβ)
n
i Xi(β − ˆβ)
n

X ⊤

][

]⊤∥max

][

X ⊤

i Xi(β − ˆβ)
n
i Xi(β − ˆβ)
n
i Xi(β − ˆβ)
n

X ⊤

n∥[

n∥[

]∥2
∞

n(∥ ˆΣi − Σ∥max + ∥Σ∥max)2∥β − ˆβ∥2
1

2n(∥ ˆΣi − Σ∥2

max + ∥Σ∥2

max)∥β − ˆβ∥2
1

2n(O(

(cid:114)

log d
n

) + O(1))∥β − ˆβ∥2
1

≤

≤

≤

≤

≤

1
m

1
m

1
m

1
m

1
m

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

i=1

≤ k0s∗2 log d
m

47

(A.24)

For the fourth term, we could apply Cauchy-Schwarz inequality, which give us the result:

][

X ⊤

i Xi(β − ˆβ)
n
i Xi(β − ˆβ)
n
i Xi(β − ˆβ)
n

X ⊤

n∥[

n∥

X ⊤
i Wi
n

]⊤∥max

][

X ⊤
i Wi
n

]⊤∥max

∥∞∥

X ⊤
i Wi
n

∥max

X ⊤

∥

2
m

m
(cid:88)

i=1

n[

m
(cid:88)

i=1
m
(cid:88)

i=1
m
(cid:88)

≤

≤

≤

2
m

2
m

2
m

≤ k0n ·

≤ k0

i=1

n∥

X ⊤
i Xi
n
s∗ log d
n0
s∗ log d
m

∥max∥β − ˆβ∥1∥

X ⊤
i Wi
n

∥∞

(A.25)

We could combine the result in (A.23), (A.24), (A.25) and insert into (A.22) and into

+
m ). Insert this result into (A.17), when u is chosen properly, we could verify that

(A.21). We could finally get the first term of (A.18) has an order of O(
s∗2 log d
supα∈(0,1) |P(T ≤ CU (α)) − α| = o(1), which finishes the proof.

+ s∗ log d
n0

(cid:113) log d
n0

A.7 Proof of Theorem 7

The proof of theorem 7 is quite straight forward. We could decompose true βi = u + vi.
Then, ˆβ = ˆu + ˆvi. Thus, from the result of estimation, we could get the result that:

and

∥u − ˆu∥2

2 ≤ c0

s0 log d
mn

+ c2

s0

2 log d2 log(1/δ) log3 mn
m2n2ϵ2

,

∥ ˆvi − vi∥2

2 ≤ c1

s1 log d
n

+ c3

s1

2 log d2 log(1/δ) log3 n
n2ϵ2

Also, combing the above two results with the inequality that ∥ ˆβi − βi∥2 ≤ ∥ ˆvi − vi∥2 +
□
∥u − ˆu∥2 gives the proof of theorem 7.

A.8 Proof of Theorem 8

The proof of Theorem 8 follows the proof of and Theorem 4 and Theorem 5. We follow
the proof of Theorem 4 until (A.8). The analysis for the second term and the third term
for (A.8) stays the same. We will analyze the three terms in (A.8) one by one. For the
first term, in the same manner, we could decompose this term as:

√

n( ˆΘ⊤
j

ˆΣ − e⊤

j )(β − ˆβu) =

√

n( ˆΘ⊤
j

ˆΣ − Θ⊤
j

ˆΣ + Θ⊤
j

ˆΣ − e⊤

j )(β − ˆβu)

48

√

=

n( ˆΘ⊤
j

ˆΣ − Θ⊤
j

ˆΣ)(β − ˆβu) +

√

n(Θ⊤
j

ˆΣ − e⊤

j )(β − ˆβu)

(A.26)

For the first term in (A.26), we could further decompose this term from ˆΣ = 1
n

(cid:80)n

i=1 xix⊤
i

:

√

n( ˆΘ⊤
j

ˆΣ − Θ⊤
j

ˆΣ)(β − ˆβu) =

≤

√

√

j − Θ⊤

j ) ˆΣ(β − ˆβu)

n( ˆΘ⊤
nλs( ˆΣ)∥ ˆΘj − Θj∥2|β − ˆβu∥2

≤ o(1) +

γµ2
s
ν2
s

1 log2 d log(1/δ) log3 mn
s2
m2n3/2ϵ2

+

γµ2
s
ν2
s

0 log2 d log(1/δ) log3 n
s2
n3/2ϵ2
(A.27)

Thus, for the second term of (A.26), by Lemma A.3, we have:

√

n(Θ⊤
j

ˆΣ − e⊤

j )(β − ˆβu) ≤

√

ˆΣ − e⊤

j ∥∞∥β − ˆβu∥1

n∥Θ⊤
j
(cid:114)

√

≤ k

n

≤ k ·

√

n

≤ o(1) +

(cid:114)

√

s∥β − ˆβu∥2

log d
mn
s log d
mn
1 log2 d log(1/δ) log3 mn
s2
m2n3/2ϵ2

· ∥β − ˆβu∥2

γµ2
s
ν2
s

+

γµ2
s
ν2
s

0 log2 d log(1/δ) log3 n
s2
n3/2ϵ2

(A.28)

Then, combining (A.27) and (A.28), we have that:

√

n( ˆΘ⊤
j

ˆΣ − e⊤

j )(β − ˆβu) ≤

2γµ2
s
ν2
s

1 log2 d log(1/δ) log3 mn
s2
m2n3/2ϵ2

+

2γµ2
s
ν2
s

0 log2 d log(1/δ) log3 n
s2
n3/2ϵ2

Then, insert the result into (A.8), we have:

(cid:18)

√

n

ˆβj − βj −

2γµ2
s
ν2
s

1 log2 d log(1/δ) log3 mn
s2
m2n2ϵ2

−

2γµ2
s
ν2
s

0 log2 d log(1/δ) log3 n
s2
n2ϵ2

(cid:19)

=

ˆΘ⊤

j X ⊤W +

1
√
n
(A.29)

√

nE3

From the result in (A.29), notice that the right hand side asymptotically follows a dis-
tribution of N (0, σ2 ˆΘ⊤
). Also, by the concentration of Gaussian dis-
j
tribution, we have with high probability, E2 ≤ 8∆2

). Thus, we have:

ˆΣ ˆΘj + 8∆2

1 log(1/δ)

1 log(1/δ)

nϵ2

nϵ2

(cid:18)

√

n

ˆβj − βj −

2γµ2
s
ν2
s

1 log2 d log(1/δ)
s2
m2n2ϵ2

−

2γµ2
s
ν2
s

0 log2 d log(1/δ)
s2
n2ϵ2

(cid:19)
/

(cid:114)

σ2 ˆΘ⊤
j

ˆΣ ˆΘj +

8∆2

1 log(1/δ)

nϵ2

∼ N (0, 1)

Then, we could replace µs, νs with the estimation ˆµs, ˆνs introduced in Algorithm 5, the
constant could be scaled to one given the tail bound of Laplace random variable. Also, for
the estimation of σ, according to the assumption, we have ˆσ = σ + o(1). For simplicity,

49

we denote a = 2k ˆµ2
ˆν2
s
by:

s

1 log2 d log(1/δ) log3 mn
s2
m2n2ϵ2

+ 2k ˆµ2
ˆν2
s

s

0 log2 d log(1/δ) log3 n
s2
n2ϵ2

, the confidence is given

Jj(α) =

[ ˆβj − a −

σΦ−1(1 − α/2)
√
n

(cid:114)

ˆΘ⊤
j

ˆΣ ˆΘj +

8∆2

1 log(1/δ)

nϵ2

, ˆβj + a +

σΦ−1(1 − α/2)
√
n

(cid:114)

ˆΘ⊤
j

ˆΣ ˆΘj +

8∆2

1 log(1/δ)

nϵ2

]

A.9 Proof of Theorem 9

In this proof, we first need to show that our algorithm is (ϵ, δ) private. We assume in two
data sets, the adjacent data set is different in (xij, yij) and (x′

ij). Then, we have:

ij, y′

∥

1
√
n

ˆΘxijej −

1
√
n

ˆΘx′

ijej∥∞ ≤

≤

≤

∥ ˆΘxijej∥∞

∥ ˆΘxij∥∞∥ej∥∞

2
√
n
2
√
n
√
2
log n
√
n

∥ ˆΘ∥1∥xij∥∞

(cid:114)

≤ 2

√

log n
n

cx

sc1

According to the choice of B5, the privacy could be guaranteed. Then, let us start the
proof of consistency. In this proof specifically, we define U ′ = maxk∈G
i=1 xijej
and also, we define:

ˆΘT
k

1√
n

(cid:80)n

M0 = max
k∈G

1
√
n

n
(cid:88)

ξjk,

j=1
k ΣΘkσ2). Also, we define the α-quantile

where ξj follows a Gaussian Distribution N (0, Θ⊤
of M0 as UM0(α). Then, we could start the proof.

We are aiming at proving supα∈(0,1) |P(M ≤ CU (α)) − α| = o(1). First, we could
prove that CU (α) and CU ′ are close to each other for any α ∈ (0, 1). From the design of
private max algorithm, from Lemma 3.4 in [8], suppose l1 is the element chosen from U ′
and l2 is from U without noise injection, we use w to represent the noise injected when
we pick the largest value privately, we find that, for any c > 0:

2 ≤ l2
l2

1 ≤ (1 + c)l2

2 + 4(1 + 1/c)∥w∥2
∞

n

. When we choose c = o(1), e.g, c = k1

From Lemma A.1 in [8], we can verify that there exists constant k0, k1 such that ∥w∥2
∞ ≤
k0 · s∗ log4 d log n
, then from the conditions,
we could claim that l1 = l2 + o(1), also notice that the scale of noise we injected is small,
it is easy to verify that U = U ′ + o(1). The following discussions will be between U ′ and
UT0

.
Motivated by the proof of Theorem 3.1 and Theorem 3.2 in [10], our proof will be
and measure

divided into two major parts, to measure the closeness between U ′ and UM0

s∗ log d
n

50

the closeness between T and T0. We start from the measurement between M and M0.
√
From the definition that M ( ˆβ(i)) = maxk∈G
k ), we notice that for each k in
1, 2, . . . , d, we have:

k − β(i)

n( ˆβ(i)

√

n( ˆβ(i) − β(i)) =

1
√
n

ˆΘX ⊤

i Wi + ( ˆΘΣ − I)( ˆβ − β) +

√

nE

Then,

|M − M0| ≤ (∥

1
√
n

ˆΘX ⊤

i Wi∥∞ − ∥

1
√
n

n
(cid:88)

j=1

ξj∥∞) + ∥( ˆΘΣ − I)( ˆβ − β) +

√

nE∥∞

(A.30)

We analyze the two parts in (A.30) separately. For the first term in (A.30). First, from
Lemma 1.1 in [42], we could obtain the result that for any z, supz |P (∥ 1√
i Wi∥∞ ≤
z) − P (M0 ≤ z)| ≤ c0 · 1
nc1

, where c0 and c1 are constants. Also,

n ΘX ⊤

∥

1
√
n

ˆΘX ⊤

i Wi∥∞ − ∥

1
√
n

ΘX ⊤

i Wi∥∞ ≤

1
√
n

∥ ˆΘX ⊤

i Wi − ΘX ⊤

i Wi∥∞

≤ ∥ ˆΘ − Θ∥1 · ∥

1
√
n

X ⊤

i Wi∥∞

≤ c · s∗

(cid:114)

log d
n

(cid:112)log d ≤ c · s∗ log d

(cid:114) 1
n

= o(1)

On the other hand, we also notice that for the second term in (A.30), following the proof
in Theorem 4, we also know that the second part is o(1), hence finishes the first part of
the proof. In the second part of the proof. By the arguments in the proof of Theorem
3.2 in [10], we have for any v:

|P(T ≤ CU ′(α)) − α| ≤ c0

sup
α∈(0,1)

1
nc1

+ c2v1/3(1 ∨ log(d/v))2/3 + P (□ > v),

where □ = maxk,l ˆΘ⊤
k

ˆΣ ˆΘl − Θ⊤

k ΣΘl.Then, we have:

∥ ˆΘ⊤ ˆΣ ˆΘ − Θ⊤ΣΘ∥max
≤ ∥ ˆΘ⊤ ˆΣ ˆΘ − ˆΘ⊤Σ ˆΘ∥max + ∥ ˆΘΣ ˆΘ − Θ∥max
≤ ∥ ˆΘ∥∞∥ ˆΣ − Σ∥max∥ ˆΘ∥1 + ∥( ˆΘ − Θ + Θ)Σ( ˆΘ − Θ + Θ) − Θ∥max

+ ∥ ˆΘ − Θ∥2

1∥Σ∥max + 2∥ ˆΘ − Θ∥1

≤ L2

≤ k0

(cid:114) log d
n0
s∗2 log d
n0

+ k1s∗

(cid:114) log d
n0

,

where k0, k1 are constants. Then, with a proper choice of v, we could claim that
supα∈(0,1) |P(T ≤ CU ′(α)) − α| = o(1), which finishes the proof.

51

B Appendix

In this section, we will give proofs of the corollary in the main proof. We will introduce
them one by one:

B.1 Proof of corollary 1
Following the proof of Theorem 1 in [2], we can gain the upper bound of (cid:80)k
when we consider the central DP:

i=1 dT V (pZm

+i , pZm

−i ),

1
k

(cid:32) k

(cid:88)

i=1

(cid:33)2

dT V (pZm

+i , pZm
−i )

≤ 7

m
(cid:88)

t=1

EA





k
(cid:88)

(cid:90)

(E

p⊗n
a

[W(z | X)] − E

[W(z | X))]2

p⊗n
a⊕i
Epa[W(z | X)]



dµ



Also notice that:

E

p⊗n
a⊕i

[W(z | X))] = E

p⊗n
a

(cid:34)

dp⊗n
a⊕i
dp⊗n
a

i=1

Z

(cid:35)

W(z | X))

= Epa(1 + qa,iϕa,i(X))n · W(z | X)

The last equation is from the definition of condition 1. Also, by the inequality that we
can find constants c0, c1 that when x > 0 and x ≍ 1/n, (1 + x)n ≤ 1 + c0 · nx and
(1 − x)n ≤ 1 − c0 · nx. If we have |qa,iϕa,i(X)| ≍ 1/n. we could find a constant c2, such
that:

1
k

(

k
(cid:88)

i=1

dT V

(cid:0)pZm

+i , pZm

−i

(cid:1))2 ≤ c2q2n2

m
(cid:88)

t=1

EA

(cid:34) k

(cid:88)

(cid:90)

i=1

Z

E

[ϕa,i(X)W(z | X)]2
p⊗n
A
E
[W(z | X)]

p⊗n
A

(cid:35)

dµ

which finishes the proof of corollary 1.

B.2 Proof of Corollary 2

We continue to show the proof of Corollary 2. First, from Theorem 2 in [2], we get a
direct result when condition 2 is satisfied, given all the conditions in corollary 2 hold, we
have:

(cid:32)

1
k

k
(cid:88)

i=1

(cid:33)2

dT V (pZm

+i , pZm
−i )

≤

7
k

q2n2

m
(cid:88)

t=1

(cid:90)

Z

max
a∈A

Varpa[W(z | X)]
Epa[W(z | X)]

dµ

Then, the focus of the proof of this corollary is on the calculation of (cid:82)
Varpa [W(z|X)]
Epa [W(z|X)] dµ
when the channel W is a privacy constraint channel W priv. For simplicity, we denote
L(z, X) = logW priv(z|X), where z ∈ Rd and X ∈ Rn×d. Then, notice that W priv is
ϵ-differentially private constraint, for two adjacent dataset X and X ′, we have:

Z

|L(z, X) − L(z, X ′)| ≤ ϵ

52

By McDiarmid’s inequality, we could claim that L is
find a constant c, which satisfies that:

√

nϵ- subGaussian. So we could

Then, by Jensen inequality, we have:

E[e2L] ≤ c · e2E[L] · e2nϵ2

E[e2L] ≤ c · (eE[L])2 · e2nϵ2

Thus, we have:

Var[W priv(z|X)]
E[W priv(z|X)]2 =

E[W priv(z|X)2]
(E[W priv(z|X)])2 − 1 =

E[e2L]
(E[eL])2 − 1 ≤ e2nϵ2

− 1

Thus, we have:

(cid:32)

1
k

k
(cid:88)

i=1

(cid:33)2

dT V (pZm

+i , pZm
−i )

α2n2

m
(cid:88)

t=1

(cid:90)

Z

max
a∈A

Varpa[W(z | X)]
Epa[W(z | X)]2 · Epa[W (z | X)]dµ

α2n2(e2nϵ2

− 1)

m
(cid:88)

t=1

(cid:90)

Z

max
a∈A

q2mn2(e2nϵ2

− 1)

Epa[W(z | X)]dµ

≤

≤

≤

7
k

7
k

7
k

which finishes the proof of corollary 2.

B.3 Proof of corollary 3
For an (n, ρ)-estimator ˆθ of the true parameter under ℓp loss, we define ˆA for A as

Then, by the triangle inequality, we have

a∈A

ˆA = argmin

∥θa − ˆθ∥p.

(cid:13)
(cid:13)θA − θ ˆA

(cid:13)
(cid:13)p ≤

(cid:13)
(cid:13)
(cid:13)θA − ˆθ
(cid:13)
(cid:13)
(cid:13)p

+

(cid:13)
(cid:13)
(cid:13)θ ˆA − ˆθ
(cid:13)
(cid:13)
(cid:13)p

≤ 2

(cid:13)
ˆθ − θA
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)p

.

Because ˆθ is an (n, ρ)-estimator under ℓp loss, we have,

EZ[EpZ [(cid:13)

(cid:13)θZ − θ ˆZ

(cid:13)
p
p]] ≤ 2pρpP[pZ ∈ PΘ] + max
(cid:13)
z̸=z′

∥θz − θz′∥p
p

P[pZ /∈ PΘ]

≤ 2pρp + 4pρp 1
τ

·

τ
4

≤

3
4

4pϵp,

(B.1)

(B.2)

(B.3)

using the fact that P[pA ∈ PΘ] ≥ 1 − τ /4 and condition 4. Also, from condition 4, Next,
. Also, since the
combining condition 4 and B.3, we could have:

(cid:80)k

1
τ k

P[Ai ̸= ˆAi] ≤ 3
4

i=1

53

Markov relation Ai − X m − Zm − ˆAi holds for all i, by the standard relation between
total variation distance and hypothesis testing, and also the definition of τ to be less
than 1/2, we have:

P[Ai ̸= ˆAi] ≥ τ P[ ˆAi = −1|Ai = 1] + (1 − τ )P[ ˆAi = 1|Ai = −1]

≥ τ (P[ ˆAi = −1|Ai = 1] + P[ ˆAi = 1|Ai = −1])
≥ τ (1 − dT V (pXm
+i , pXm
−i ))
+i , pZm
≥ τ (1 − 1/n · dT V (pZm

−i ))

The last inequality uses the definition of total variation, because Zm is generated by X m
from the privacy constraint channel W priv, so for each dataset Xi, i = 1, 2, . . . , m on the
i-th machine, let Xijk be the dataset which changes the order of Xij and Xik, then for
any z ∈ Z,W priv(z|Xi) = W priv(z|Xijk). Thus, by the definition of total variation, we
could verify that dT V (pXi
−i ) = 1/n · dT V (pZi
−i). Summing over 1 ≤ i ≤ k and
combining it with the previous bound, we obtain

+i , pXi

+i, pZi

3
4

≥

1
τ k

k
(cid:88)

i=1

P[Ai ̸= ˆAi] ≥ 1 −

1
nk

k
(cid:88)

i=1

dT V (pZn

+i , pZn
−i )

which finishes the proof of corollary 3.

B.4 Proof of Lemma 4.1

Proof of Lemma 4.1: First, we would like to show that our algorithm is (ϵ, δ)-differentially
private. For two adjacent data sets, we have:

ˆβ)2 − (πR(y′

i) − x′
i

T ˆβ)2|

1
mn

≤

≤

|(πR(yi) − xT
i
2
mn
4
mn

(πR(yi) − xT
i

(πR(yi)2 + (xT
i

ˆβ)2

ˆβ)2) ≤

4
mn

(R2 + sc2

0c2
x)

From the definition of Gaussian Mechanism, we could claim that our algorithm is (ϵ, δ)-
differential private. Then, for the convergence rate of our estimated σ, from our algo-
rithm, first, we observe with the choice of R, we claim that with high prob, we have
πR(Y ) = Y . Therefore, we have:

∥Xβ + W − X ˆβ∥2

2 − σ2| + |E|

|σ2 − ˆσ2| ≤ |

≤ |

1
mn
1
mn

For the first term, we could obtain that | 1

mn ) Also, we have:
1√

W T W − σ2| + (β − ˆβ) ˆΣ(β − ˆβ) +

(β − ˆβ)X T W + |E|

1
mn
mn W T W − σ2| = O(

(β − ˆβ) ˆΣ(β − ˆβ) ≤ λs(Σ)∥β − ˆβ∥2
2

54

≤ cL

(cid:18) s log d
mn

+

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

(cid:19)

Then, from Bernstein inequality, we could obtain that:

∥

1
mn

X T W ∥∞ = c1 ·

(cid:114)

log d
mn

Therefore, we claim that:

1
mn

(β − ˆβ)X T W

≤

1
∥β − ˆβ∥1∥X T W ∥∞
mn
√

∥β − ˆβ∥2∥X T W ∥∞
(cid:32)(cid:114)

s
mn
√

s

≤ c2

≤ c3

s log d
mn

+

s log d(cid:112)log(1/δ) log3/2 mn
mnϵ

(cid:33)(cid:114)

= Op

= Op

(cid:32)

s log d
mn
(cid:18) s log d
mn

+

+

s log d(cid:112)log(1/δ) log3/2 mn
mnϵ

·

(cid:114)

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

(cid:19)

log d
mn
(cid:33)

s log d
mn

Also, from our algorithm, we have E ∼ N (0, 2B2

2 log(1.25/δ)/ϵ2). Then,

|E| =

2B2

2 log(1/δ)

ϵ2

|N (0, 1)| = c4

R4 + s2c4

x log(1/δ)

0c4
m2n2ϵ2

= c5 ·

s2 log2 d log(1/δ) log3 mn
m2n2ϵ2

,

by observing that cx = O(
conclusion. Therefore, we finish our proof.

√

log d). Combining above inequalities, we have reached our

B.5 Proof of Lemma 4.2

Proof of Lemma 4.2: It is not difficult to verify the privacy conditions. Then, from the
theory of covering number, we could find n1 vectors v1, v2, . . . , vn1
, such that for each
s-sparse unit vector v, we have ∥v − vi∥ ≤ 1/9. Thus, we have:

λs − ˆλs = v∗ ˆΣv∗ − vi ˆΣvi

≤ v∗ ˆΣ(v∗ − vi) + (v∗ − vi) ˆΣvi

≤

λ2s

2
9
≤ 8/9λs

Note that the second last inequality is a direct result of Cauchy inequality and the last
inequality holds because let v∗′ be the corresponding eigenvector of λ2s, then we could

55

1 + v′

2)T ˆΣ(v′

break this eigenvector to two s-sparse vectors v′
1
λ2s = (v′
1 + v′
Also, notice that for the noise ξ, by the concentration of Laplace distribution, we could
√
find a constant c such that ξ ≤ cs log d/
n = o(1) with high probability. By the
□
definition of λs, we conclude the proof.

such that v∗′ = v′

2) ≤ 4λs.

and v′
2

1 + v′
2

, then

56

