The Over-Certainty Phenomenon in
Modern UDA Algorithms
FinAmin Jung-EunKim∗
NorthCarolinaStateUniversity NorthCarolinaStateUniversity
samin2@ncsu.edu jung-eun.kim@ncsu.edu
Abstract
Whenneuralnetworksareconfrontedwithunfamiliardatathatdeviatefromtheir
trainingset,thissignifiesadomainshift. Whilethesenetworksoutputpredictions
ontheirinputs,theytypicallyfailtoaccountfortheirleveloffamiliaritywiththese
novelobservations. Thischallengebecomesevenmorepronouncedinresource-
constrained settings, such as embedded systems or edge devices. To address
suchchallenges,weaimtorecalibrateaneuralnetwork’sdecisionboundariesin
relationtoitscognizanceofthedataitobserves,introducinganapproachwecoin
as certaintydistillation. Whileprevailing works navigate unsuperviseddomain
adaptation(UDA)withthegoalofcurtailingmodelentropy,theyunintentionally
birthmodelsthatgrapplewithcalibrationinaccuracies-adilemmawetermtheover-
certaintyphenomenon. Inthispaper,weprobethedrawbacksofthistraditional
learningmodel. Asasolutiontotheissue,weproposeaUDAalgorithmthatnot
onlyaugmentsaccuracybutalsoassuresmodelcalibration,allwhilemaintaining
suitabilityforenvironmentswithlimitedcomputationalresources.
1 Introduction
Whenencounteringnewenvironments,humansnaturallyadoptacautiousapproach,assimilating
thenoveltytoguidetheirdecision-making. Thisinherentabilitytoassessunfamiliarityandadjust
certaintyhasnotbeenentirelyemulatedinartificialneuralnetworks. Unlikehumanswhomight
exhibithesitationinunknownsituations,manyunsuperviseddomainadaptation(UDA)algorithms
lackanexplicitmechanismtomodulatecertaintyinresponsetothenoveltyorunfamiliarityoftheir
inputs.
Deeplearninghasneverbeenastrangertothechallengesofuncertainty. Overthepastfewyears,the
miscalibrationproblemofmodernneuralnetworkshasgainedsubstantialattention,ashighlighted
by works such as [1], [2], [3], and [4]. However, there is an observed void in the landscape of
unsuperviseddomainadaptationtechniques,withmostofthemneglectingmodelcalibrationduring
adaptationprocesses. Inthispaper,weintroducetheover-certaintyphenomenonwhichharmsmodel
calibration,andproposeanalgorithmthatextendsthenotionofunfamiliarity-analogoustowhat
humansexperience-toUDA.
AprevailingstrategyamongUDAalgorithmsistheminimizationofentropy,eitherasanexplicit
target or as an inherent by-product of their methodology. And while this might bolster accuracy
metrics,ourresearchindicatesaconcerningtrend: excessiveentropyreductioncanbedetrimentalto
modelcalibration. Whatmakesthistrendmoreproblematicisthatitoccurswithinthecontextofa
newdomain,whereepistemicuncertaintyshouldtypicallybegreater.
Tofurtherframeourdiscussion,UDAisusedwhenamodel,trainedonasourcedomain,ispresented
withthechallengesofadifferentyetanalogoustargetdomain. Thenuancesbetweenthesedomains,
∗Correspondence
Preprint.Underreview.
4202
rpA
42
]GL.sc[
1v86161.4042:viXraFigure1: ThischartshowsadaptingaMobileNettothelevel5-intensitycontrastdomainviathree
modernUDAalgorithms. Minimizingentropyisacommonobjectiveinrecentwork. However,this
canhaveconsequencesonmodelcalibration.
commonly termed as domain shift, can introduce significant disruptions in model performance.
UDA,initsessence,aspirestoadapttheinsightsharvestedfromthesourcedomainandapplythem
proficientlytothetargetdomain,bypassingtheneedforlabeleddatainthelatter. Thus,withinthe
contextofthisdomainshift,itcanbeespeciallyproblematictobetoocertain.
Withthepurposeofaddressingtheseintertwinedchallenges,weintroducecertaintydistillation. This
UDAtechniqueseekstoaugmentaccuracy,improvemodelcalibration,andmaintaincompatibility
to resource-constrained devices. By interweaving calibration into the core learning process, we
produceaUDAalgorithmthatjointlyimprovesaccuracyandepistemicuncertainty. Furthermore,
we introduce a hyperparameter of our algorithm which provides a trade-off between adaptation
performanceandresourceconsumption. Tosummarize,ourcontributionsare:
• The identification of the over-certainty phenomenon in modern UDA algorithms. We
provide thorough empirical evidence which corroborates our claims. Furthermore, we
provideplausibleexplanationsastowhythishappens.
• CertaintyDistillation,anewUDAalgorithmthatachievesSOTAcalibrationinallofthe
fourdatasetsandSOTAaccuracyupliftsinthemajorityofdomainshifts. Additionally,our
algorithmprovidesfavorablememory-consumptionvs. performancetrade-offs.
2 RelatedWorkandState-Of-The-Arts
Wedivideourliteraturesurveyintothreesections. Thefirstsectioncoversmethodologiescatered
towards updating a neural network on unlabeled data. For the sake of brevity, we will refer to
unlabeled data as “observations." This section gives an overview of the work done to improve
networksonthefly. Westartbyintroducingearlierwork,suchasdictionarylearningtechniques,and
leadourwayintorecentdevelopments. Thesecondsectioncovershowwemeasureneuralnetwork
calibrationandcertainty. ThethirdsectioncoversOODdetection.
2.1 Self-TaughtLearningandUDA
Thephrase“self-taughtlearning"wascoinedby[5]. Inthiswork,theauthorsutilizeobservations
to find an optimal sparse representation of said observations. More precisely, the authors utilize
observationstofindasetofbasisvectorsandcorrespondingactivations. Bydoingthis,theauthors
createamethodologyforfindingasparserepresentationofinputs. Uponfindingthebasisvectors,
theauthorsthensolvefortheactivationsusingthetrainingset. Finally,thissparserepresentationis
usedtotraintheirmodelinlieuoftheordinarytrainingset.
Work in this field has extended to a variety of approaches. For example, [6] formulate a similar
methodologyforutilizingtheobservationsmadefromedgedevices. Specifically,theyproposea
methodology using SVMs to find so-called “domain invariant features.” These are features that
maximizethemarginacrossvariousdomains. Otherexamplesincludetheuseofpseudo-labeling
toexploittheexistingmodel’spredictionsastargetlabels[7]. Pseudolabelingcanbethoughtof
undertheguiseofKnowledgedistillation(KD)[8,9]. KDisatransferlearningparadigmwherea
2largeneuralnetwork,knownastheteacher,transfers“knowledge”toasmaller“student”network.
Succinctly,thestudentistrainedtomatchtheoutputoftheteacherwhengiventhesameinputasthe
teacher[10]. Anothertechniqueisensemblingvarioussource-specificnetworks[11,12].
MorerecentadvancementsincludeTENT[13],EATA/ETA[14],andT3A[15]. TheTENTalgorithm
focusesontest-timeentropyminimization. Inotherwords,thealgorithmworksbyusinggradient
descenttominimize:
(cid:88)
L =− f (y|x)logf (y|x) (1)
TENT Θ Θ
y∈C
toupdatethemodel’sbatch-normalizationparameters. ETA2advancesonTENTbymakingsurethat
observationsarereliableandnon-redundantbeforetheyareusedforupdatingthebatch-normalization
parameters. Todothis,theycomputeasampleadaptiveweight,S(x),foreachobservationbefore
minimizingentropy:
(cid:88)
L =−S(x) f (y|x)logf (y|x) (2)
ETA Θ Θ
y∈C
WhereS(x)isafunctionoftheentropyofthemodeltowardsthebatchsample(i.e.,thereliability)
andthesimilaritytowhatithasseenbefore(i.e.,non-redundancy).
TheT3Aalgorithm[15]differsfromtheprevioustwoasitfocusesonupdatingtheprototypes[16]of
eachclassduringtesttime:
(cid:26)
St−1∪{f (x)}, ifyˆ=y
St = k θ k (3)
k St−1, else.
k
1 (cid:88)
c = z (4)
k |S |
k
z∈Sk
wherec representsthecentroidoftheprototypesofaclassk. Toinference,T3Acomputes:
k
exp(z·c )
argmax ykγ c(Y =y k|f θ(x))= (cid:80) exp(z·k
c )
(5)
j j
where z is the output of the feature extractor.3 Unlike TENT or ETA, T3A does not use back
propagation. However,similartoETA,thisalgorithmfilterslessreliablesamplesduringequation
3 by only keeping the M most reliable (low-entropy) prototypes for each class. Therefore, the
algorithmstoresC·M prototypes,whereC isthenumberofclasses.
2.2 NeuralNetworkCalibration
Neuralnetworkcalibrationhasbeenofintenseinterestinrecentyears.Theaccuracyoftheconfidence
ofaneuralnetworkisextremelyimportant—asconfidencevalues,reflectingtheprobabilityassigned
to a prediction, are used in a variety of domains. For example, the authors of BranchyNet [17]
utilizeneuralnetworkconfidencevaluestoallowanearlyexitforfasterinference,bankingonhigh
confidenceatintermediatelayers. Conversely,[18]remarksonthelackofcertaintycalibrationin
mostdeepnetworks,wherecertaintyencompassesnotjustconfidencebutalsothemodel’soverall
assuranceinitspredictions. Inourwork,wemeasurecertaintyasH−1 =(Entropy )−1. Themost
2
remarkableobservationtheyfoundwasthatmostmodelsareeithertooconfidentornotconfident
enough,possiblyduetooverfittingduringtraining.
Theauthorsof[19]explorethisconceptfurther. Theymeasureamodel’sexpectedcalibrationerror
(ECE)withrespecttochanges(rotation,translation,etc)inthetestset. ECEisdefinedas:
2WhiletheauthorsofEATA/ETAintroducetwosimilaralgorithms,forourpaper,wefocusonETA.
3Wedefinethefeatureextractorasallthelayersofthebackbonebeforethefinaldenselayer.Thefinaldense
layer,whichhasasizethatisafunctionofthenumberofclasses,iswhatwerefertoastheclassifier.
3Figure2: Apotentialcauseoftheover-certainyphenomenonisthatpriorworkaimsatincreasing
certaintytowardsobservationsdespitethedomainshift(corruptionintensity)increasing. Ouralgo-
rithm,certaintydistillation,achievesstate-of-the-artaccuracyandcalibrationonTinyImageNet-C.
Theresultsareaveragedover15uniquedomainshifts;variancesacrossthedomainsareshownby
theerrorbars.
N
ECE=(cid:88)|B i|
|accuracy(B )−confidence(B )| (6)
N i i
i=1
Theynoticemodelscalibratedonthevalidationsettendtobewellcalibratedonthetestset,butare
notproperlycalibratedtoshifteddata.Recentworkhasinvestigatedthisphenomenon.[1]approaches
theissuethroughamethodknownastemperaturescalingwhile[20]approachesthisproblemby
regularizingthelogitnorm. Moreclassicalsolutionstothisproblemexistaswell; [21]and[22]
considerlabelsmoothingtoaddressthisissue.
2.3 DetectingOutofDistributionData
Therehasbeenintenseinterestinrecentyearsintheproblemofdeterminingif,andtowhatdegree
an observation is similar to what a model was trained on. The authors of [23] observe that if an
autoencoderweretrainedtoreconstructinliers,itwouldhaveagreaterreconstructionerrorwhen
reconstructingOODdata. [24]and[25]approachthisissuebyobservingthatthediscriminatorof
aGANlearnswhetherornotagiveninputisaninlier. Manyotherworksdelveintothisdomain;
OpenMax[26]analyzesmeanactivationvectors(MAV),and[26]investigatestheoptimalrecognition
error. Otherexamplesinclude[27,28,29,30,31]. Literaturesuchas[32]aimsatdeterminingunder
whatconditionsOODdetectionispossible.
3 ProposedApproach
3.1 TheOver-CertaintyPhenomenon(OCP)
Inthiswork,wepresentevidenceforwhatwedubtheover-certaintyphenomenon(OCP)ofcon-
temporaryUDAapproaches. ThisphenomenonisthatUDAalgorithmstendtomiscalibratetheir
underlyingbackbonenetworksbycausingtheirpredictionstobeexcessivelycertain. ModernUDA
algorithms often strive to decrease test-time entropy. However, as shown in Fig. 1, this entropy
reductionmayincreaseECEbecausethemodelsbecomeoverlycertainontheirpredictions.
Thisphenomenonofexistingalgorithmscausingmodelstobecomeoverlycertainpresentsitself
acrossmanyotherdatasets. Forexample,inTable1a,T3Areducesentropybyafactorofabout4in
theArt,ClipartandProductdomains. Asbefore,itcausesECEtoworsencomparedtothebaseline.
Inadditiontotheresultsweshowinthispaper,weprovideextensiveevidenceonthisphenomenon
intheAppendix. WedonotclaimthatUDAalgorithmsshouldalwaysstrivetoincreasebackbone
uncertainty;poorcalibrationcanalsobecausedbyunder-certainty. Infact,thereexistcaseswhere
reducingentropycomparedtobaselineimprovescalibration. However,wefindthattheresulting
4calibrationisstillsub-optimal. Despitethesecomplexities, ourinvestigationrevealsaconsistent
pattern: theover-certaintyphenomenoncausessub-optimalmodelcalibration,asignificantconcern
forsafety,robustness,andreliability.
3.2 TowardsBetterCalibrationinUDA:DiscussiononSOTAAlgorithms
WeidentifytwoplausiblecausesoftheOCP,thefirstissueisthatmodernUDAalgorithmsaimat
minimizingbackboneentropytooaggressively. InthecaseofTENTandETA,theirlossfunctions,
equations(1)and(2),explicitlyaimatreducingamodel’sentropy. RegardingTENT,thereisno
regularizationofthisprocess. InthecaseofETA,thealgorithmusesareliabilityscore,S(x),which
aimsatweighingobservationsdifferentlybutdoesnotregularizethedistributionsofthepseudo-labels.
T3Adoesnotexplicitlyreduceentropyasitdoesnotusealossfunction,buttheauthorsclaimthisas
aneffectofusingtheiralgorithm. Infact,theyshowincertaindatasetsT3Areducesentropymore
thanTENTdoes.
Anotherissueishowexistingmethodsevaluateobservationreliability,thesuitabilityofamodel’s
prediction for use for adaptation. Previous works, ETA and T3A, tap into the power of model
certainty,usingittoweightheinfluenceofobservations. ETAassessesreliabilitybyensuringthat
observationsmeetacertainentropythreshold;similarly,T3Ausesentropytosorttheimportance
ofclassprototypes. However,therearedrawbacksinusingentropyasaproxyforreliabilityinthis
manner [20]. To illustrate our point, we give a toy example of how using entropy can lead to a
misleadingconclusion:
Example1 Supposethatweanalyzetheclassifierwhileclassifyingbetweentwoclasseswithclass
centroids, c and c . This is done by taking the output of the feature extractor, f(x) = z, and
0 1
computingthedotproductbetweenthecentroidsandz.
g =[z·c ,z·c ] (7)
0 1
Consider g and g ,g as vectors representing the inner products related to a specific training
s t1 t2
sampleandobservation, respectively. Specifically, g correspondstotheinnerproductswiththe
s
trainingsamplewheref(x )=z ,andg ,g correspondtotheinnerproductswiththeobservation
s s t1 t2
wheref(x )=z . Forthesakeofanexample,let’sassume:
t t
g =[8.0,7.29]; g =[.9199,.00019]; g =[6.1,6.5];
s t1 t2
Ifwetakethesoftmaxofthesevectorsandcomputetheentropy,wegetEntropy (SM(g))forg ,g
2 s t1
andg ,as0.92bits,0.86bitsand0.97bits,respectively.
t2
Notice that if we consider the entropy of these three vectors as a proxy for reliability, we would
consider x to be more reliable than x , despite x having considerably greater average inner
t1 t2 t2
productwiththeclasscentroids. Itisfarmorelikelythatthevaluesofg occurredduetospurious
t1
featurecorrelationsbetweenx andtheclasscentroids. Infact,inthescenarioabove,x wouldbe
t1 t1
deemedtobemorereliablethanthegenuinesourcedomainobservationx . Furthermore,thereis
s
noconsiderationofthesourcedomain’scertainty. Byonlyevaluatingthetargetdomain’scertainty
withoutjuxtaposingitagainstthesourcedomaincertainty,thereisalackofreferenceintermsof
assessingthereliabilityoftheobservation.
3.3 TheCertaintyDistillationAlgorithm
Toamelioratetheover-certaintyphenomenon,weintroducecertaintydistillation(CD)(Algorithm
2). The CD algorithm employs a novel adaptation technique to strategically manipulate model
certaintytotheunknowntoimprovecalibration. CDrefinesthemodel’scertaintylevels,aligning
themmorecloselywithitsactualaccuracy,byselectivelyadjustingthetemperatureparameterduring
thedistillationprocess. Thisisachievedwithoutdirectlyalteringgroundtruthlabels,insteadfocusing
onthetemperingoflogitsthroughtemperatureadjustments. Thealgorithmemploysatwo-model
approach,usingateachermodeltoguidethecalibrationofastudentmodel,withanemphasison
preventingover-certaintyandachievingbettermodelcalibration.
Thisprocessinvolvesiterativeadjustmentsofthestudentmodel’spredictions,guidedbythecompar-
ativeanalysisofentropyandlogitnorms,therebyfosteringamoreaccurateandreliablepredictive
5model. TheinputsN ,N ,H ,andX correspondwiththeteachermodel,thestudentmodel,the
te s 0
student’saverageentropyonthetrainingset,andunlabeledobservations,respectively. Animportant
detail of our methodology is the student and teacher are two copies of the same model. The H
0
parameteristhebackbone’saverageentropyonthetrainingset. Itisusedasareferencepoint;the
ideaistocomparethemodel’scertaintyonX withrespecttothecertaintyofwhatitwastrained
on. Theinputκisthemedianl normofthetraining-setlogits;thisgivesusacontextintermsof
2
logitnorms. TheCompute_T(Algorithm1)returnsatemperatureforeachobservation,T ,with
vec
respecttoobservationentropyandrelativelogit-norm. ParametersT ,T ,andH areused
min max max
toscaletheresultingtemperatures;forourexperimentsweuse1.2,5.0,andlog (C)respectively
2
unlessstatedotherwise. Lastly,theλparameteristhelearningrateforSGD,whichwesetto0.001
forallexperiments.
Algorithm1Compute_τ
Input:N (X),H ,H T ,T ,κ
te 0 max min max
Output:T
vec
1: z =N (X)
logits te
2: H =Entropy (z ){entropyforeachsample}
vec 2 logits
3: scaled_H =sigmoid(H −H )
vec vec 0
4: Init.T
vec
5: fore ∈scaled_H do
i (cid:16) vec (cid:17)
6: t =T + ei ·(T −T )
i min Hmax max min
7: τ =[1+ 2(1.5−1.5·sigmoid(|zlogits|2))]·t
i 5 κ i
8: StoreT ←τ
vec i
9: endfor
10: returnT
vec
Tooptimizememoryefficiency,wefreezealargesubset,β,oftheweightsofourstudentsothatwe
onlyhavetostoretheweightsoftheteachernetworkplustheweightswhichwechoosenottofreeze.
The Freeze_b_Layers(N ,β) function works by freezing the parameters of N . For notational
s s
purposes,wedefineβ asthepercentageofthebackbonemodelthatisfrozen. Wefoundempirically
thatfreezinglast(i.e.,thelayersclosesttotheoutput)blayersworksthebest. Weselectbsothat
β percentofourbackbonemodelisfrozen. Thereforethebvaluewillvaryacrossbackbonesfora
targetβ value. Inotherwords,toselectb,oneshouldcomputethenumberofparametersineach
layerandselectblayerssuchthattargetpercentageofnetworkparameters,β isfrozen.
Figure3:Memoryconsumptiontrade-off. ResultswereproducedbyvaryingtheβandM parameters
ofCDandT3AandallowingthemodelstoadaptusingEfficientNet. Wefounddomain-to-domain
σ =52.07MBforthememoryconsumptionofT3AwithrespecttoM.
max
Aninterestinginterpretationastohowourmodelimprovesaccuracyisthroughtheworksof[33]
and[34]. Althoughtheformer’sworkconcernsitselfinthesemi-supervisedlearningsetting,we
foundtheirobservationstoberelevant. Thatis,theyintroducethenoisystudent,anetworkthathas
beennoised bydropoutandstochastic-depth. Theyfindthattheirnoisystudentcanevenlearnto
outperformtheteacherwhichinitiallyproducedthepseudo-labels. Forthelatterwork,theyestablish
thatlabelsmoothingmitigateslabelnoise,whichisadesirablepropertywithrespecttounsupervised
adaptation. Specifically, they find that label smoothing can be thought of as a regularizer. This
motivatesustosmoothmoreaggressivelywhenwenoticethatanobservationmightbelessreliable.
6Algorithm2Certainty_Distillation
Input:N ,N ,H ,X,κ
te s 0
Parameter:T ,T ,H ,β,λ
min max max
Output:N+
s
1: N =Freeze_b_Layers(N ,β)
s s
2: T =Compute_τ(N (X),H ,κ,H ,T ,T )
vec te 0 max min max
3: T =avg(T )
avg vec
4: Initloss
5: forx ∈Xandτ ∈T do
i i vec
6: s =SoftMax (N (x ))
si T=1 s i
7: s =SoftMax (N (x ))
ti T=τi t i
8: l =T2 ·BCE(s ,s )
CD avg si ti
9: Storeloss←l
CD
10: endfor
11: L =avg(loss)
CD
12: N′ ←θ −λ∇L (θ )
s s CD s
13: N+ =Temperature_Scale(N′,T )
s s avg
14: returnN+
s
Algorithm Art Clipart Product RealWorld Algorithm Art Clipart Product RealWorld
NoAdaptation 0.9902 0.9992 0.5663 0.0113 NoAdaptation 0.3133 0.2818 0.1940 0.0024
CD(ours) 2.2642 2.3637 1.5137 0.0892 CD(ours) 0.1107 0.0715 0.0599 0.0072
T3A 0.2527 0.2638 0.1424 0.0837 T3A 0.4330 0.3666 0.2245 0.0427
ETA 0.7716 0.6305 0.4086 0.0118 ETA 0.3341 0.3219 0.2136 0.0023
TENT 0.7820 0.6328 0.4164 0.0077 TENT 0.3281 0.3049 0.2065 0.0025
(a)ShannonEntropyonHomeOffice. (b)ECEonHomeOffice(lowerisbetter).
Table 1: Our investigation reveals a pattern of existing UDA algorithms achieving sub-optimal
calibration. Wesuspectthisiscausedbyexcessiveentropyminimization. Experimentdoneusingthe
EfficientNet backboneontheHomeOfficedataset. NotethatECEvalueslessthan0.01are
β=0%
consideredalreadywellcalibrated[1].
Theτ parameterreturnedbyAlgorithm1playsapivotalrole,wenameitthecertaintyregularizer.
Itregulatesthe“sharpness"ofpredictedprobabilitiesandsmoothensthepredictionsproducedby
theteacher. Bypreventingthemodelfrombecominginappropriatelycertaininitspredictions,we
produceamodelthatisbettercalibrated—itspredictioncertaintymorecloselyalignswithitstrue
accuracy. InCD,wedonotlabelsmoothdirectly,butinsteadadjustthetemperatureparameterofour
teacherduringthedistillationprocess. ToshowhowCDregularizesobservationsappropriately,we
continuefromexample1:
Example2 Given the same g ,g and g from example 1, we input these into our Compute_τ
t1 t2 s
algorithm. We set H = H(g ),κ = |g | ,T = 1.2 and T = 4.0. Our algorithm first
0 s s 2 min max
computesascaledentropy,scaled gwithrespecttothesourcedomainentropyforg ,g andg ,
H s t1 t2
as0.50,0.49,and0.27,respectively.
Instep6ofCompute_τ,thisentropyistransformedintoapreliminaryregularizer,t . Afterwards,
i
step7adjustst byconsideringlogitnormwithrespecttothesource-domainlogitnorm.
i
τ =2.39; τ =2.43; τ =2.57
gs gt2 gt1
Notice that, unlike purely entropy-based methods, the Compute_τ algorithm correctly assigns
greater regularization to the less reliable samples. Namely, step 7 ensures that samples that are
low-entropyduetodegeneratereasonsareproperlyregularizedbyconsideringlogitnorm. Further-
more,uniquefromexistingalgorithms,ourregularizerdirectlyaddressesmodelcertainty. Theimpact
ofCompute_τ isanalyzedinFig.4.
74 Experiments
4.1 ExperimentalSetting
InordertoevaluateCD,weconductaseriesofexperimentsusingthreedifferentbackbonemodels
acrossfivedatasets. OurprimaryevaluationmetricswillbemodelaccuracyandECE onthe
bins=15
observations,allowingustoexamineboththepredictiveperformanceandthecalibrationqualityof
themodels. Byusingvarieddomainsanddifferentbackbonearchitectures,weaimtodemonstrate
therobustnessandadaptabilityofouralgorithminhandlingdiverseandchallengingunsupervised
domainadaptationscenarios. Allexperimentsarerunfourtimesusingrandom_seed=0,1,2,3,
respectively. Run-to-runvariancesareverylow;theoneswhichwedonotshowinthemainpaper
arereportedintheappendix. Notethattheβ parameterpresentedisonlyrelevanttoCD;allother
algorithmsusedthevanillaversionoftherespectivebackbones.
4.2 Datasets
ThefollowingpubliclyavailableUDAdatasetsareusedinourexperiments;weselectedthesebecause
theyarecommonlyusedinexistingworksandprovideavarietyofdomainshifts.Intotal,weevaluate
ouralgorithmover26domainshifts. Furthermore,15ofourdomainshiftshave5levelsofcorruption
attributed to them. Dataset preprocessing steps are written in more detail in the appendix. For
somedatasets,wetestedusingthe“leaveoneout”(LOO)paradigm;forexample,inPACS,totest
generalization to pictures, we first trained our backbone networks on art, cartoon, sketch before
adapting.ForTIN-C,wefirsttrainedonasource/corruption-freedomainandadaptedtosomedomain
shift.
1. PACS[35]has4domains: pictures,art,cartoon,sketchwith7classes. TestedusingLOO.
2. HomeOffice[36]has4domains: art,clipart,product,realwith65classes. Testedusing
LOO.
3. Digitsisacombinationof3“numbers”datasets: USPS[37],MNIST[38],andSVHN[39].
Thereare10classes. TestedusingLOObytrainingonthesourcedomains’trainingsetsand
adaptingtotargetdomain’stestset. Forthisexperiment,wesetT andT parameters
min max
to1.05and4.0respectively.
4. TinyImageNet-C(TIN-C)[40],has15domainswith200classes. Backbonesaretrainedon
corruption-free(source)trainingset,adaptedtoandevaluatedoncorrupted(target)domains.
Foreachtargetdomain,thereare5tiersofcorruption.
4.3 BackBonesandTrainingDetails
WetestallbuttheDigitsdatasetontwopopularlow-resourceclassifiers,EfficientNetB0[41]and
MobileNet[42]pre-trainedforImageNet[43]. Weflattentheoutputofbothnetworksandaddafinal
denselayerwithanoutputshapeequivalenttothenumberofclasses.
WeevaluatetheDigitsdatasetusingSmallCNN,acustomlightweightnetworktailoredtohandle
grayscaleimages,servingasarepresentativeofmorecompactandstraightforwardarchitecturesfor
less complex datasets. SmallCNN encompasses a variety of essential building blocks, including
2Dconvolutionallayersequippedwithdifferentfilters,batchnormalization,ReLUactivation,and
max-poolinglayers. Thenetworkalsointegratesdenselayersandafinalclassifierlayertomake
predictions for the given number of classes. The specific details and orderings of the layers in
SmallCNNareelaboratedonintheappendix. Notethatallthreemodelsusebatchnormalization
layers.
4.4 State-Of-The-ArtApproachesforComparison
WecompareagainstTENT,T3AandETA,thethreemostrecentUDAalgorithms,andalsoabaseline
withnoadaptation,(No Adapt). ForETA,wesetE_0=0.4·ln(C),asthiswastheirrecommended
value,andϵ={0.6,0.1,0.4,0.125}foreachenumerateddataset,respectively. Theseϵvalueswere
empiricallychosentohelptheirperformance. ForT3A,wesetthenumberof supportstoretain,
M = ∞, as this provides the lowest calibration error. We do a single iteration of adaptation for
8Table 2: Average accuracy, ECE, and entropy on the Digits dataset using the SmallCNN
β=0%
backbone. Domain-to-domainσ2 =[0.22,0.18,0.20]foraccuracy,ECEandentropy,respectively
max
Algorithm Accuracy ECE Entropy
NoAdaptation 0.5894 0.3014 0.4386
CD(ours) 0.6475 0.1685 1.2195
T3A 0.6247 0.2713 1.8729
ETA 0.6463 0.2698 0.3561
TENT 0.6445 0.2617 0.3980
all algorithms unless stated otherwise. We use a batchsize=50 for CD and use the authors’
recommendedbatchsizesfortherest.
4.5 Results
We present our accuracy and ECE measurements on the four aforementioned datasets. To show
evidenceoftheover-certaintyphenomenon,wealsoreportpredictionentropy. Morecomprehensive
figurescanbefoundintheappendix. Wealsoinvestigatetheimpactofparameterβ,theamountof
thestudentfrozenfortheCDalgorithm,andcompareittotheimpactofparameterM oftheT3A
algorithminFig. 3. Doingthisallowsustoinvestigatethetrade-offsbetweendomainadaptability
andresourceconsumption. Furthermore,weperformanablationstudyonouralgorithm. Wemeasure
theimpactofourCompute_τ function,whichproducesourcertaintyregularizerτ.
Figure4: OurablationstudyhighlightstheeffectivenessoftheCompute algorithmcomparedtoa
τ
fixedτ optimizedforminimalECEonthesource-domaintrainingset. Therightmostfiguredisplays
theτ valuesgeneratedbyCompute_τ,witherrorbarsindicatingdomain-to-domainvariance.
4.6 CDReducesCalibrationError
Duetoouralgorithmaddressingtheover-certaintyphenomenon,wesignificantlyimprovecalibration
performance; CD had the lowest average ECE in all tested datasets and in nearly all individual
domainshifts. Werecognizethatreducingentropydidimprovecalibrationcomparedtobaselinein
somecasesinTables 2and 3,buttheresultingcalibrationwasstillsub-optimal. Fig 4empirically
validatesourfindingthatanadaptivecertaintyregularizeraidsinreducingECE.
4.7 CDAugmentsAccuracy
In addition to strong calibration performance, CD provides consistent accuracy uplifts while not
necessitatinganytransformationsonobservations. AccordingtoFig.2ouralgorithmgivessignificant
accuracy improvements on a variety of domain shifts without requiring any alterations to initial
9Table 3: Average accuracy, ECE, and entropy on the PACS dataset using the MobiletNet
β=0%
backbone. Domain-to-domainσ2 =[0.01,0.02,0.12]foraccuracy,ECEandentropyrespectively.
max
Algorithm Accuracy ECE Entropy
NoAdaptation 0.8410 0.1110 0.1768
CD(ours) 0.8482 0.0754 0.3660
T3A 0.8567 0.1160 0.0960
ETA 0.8541 0.1011 0.1692
TENT 0.8483 0.1079 0.1618
source-domaintraining. Acrossnearlyalldomainshifts,backbonemodels,andcorruptionintensities
withinTIN-C,CDhasthehighestaccuracyandlowestECE.Furthermore,itperformscompetitively
againsttheexistingstateoftheartintheremainingdatasetswhileaddressingcalibration.
4.8 CDisSuitableforLowResourceScenarios
OurexperimentswithSmallCNNinTable2showouralgorithm’srelevanceforlow-resourcescenarios.
We achieve strong accuracy while maintaining low calibration error. In our experiments using
EfficientNet, we were able to freeze the majority of the parameters of our backbone while still
achieving significant performance improvements in both accuracy and ECE. Furthermore, our
methodologydoesnotrequirecomputingdistancemetricsbetweenoursourcedomainandtarget
domain-whichwouldincreasecomputationalcomplexity. Themodelweightswhicharetrainable
arewhatpredominantlyconsumememoryforouralgorithm. Therefore,thememoryoverheadofCD,
P ,iscontrolledbyβ andisatmostthetrainableparametersofthebackbonemodel. Thememory
CD
overheadofT3A,P ,isafunctionoftheirhyperparameterM andthesizeofthefinalclassifier.
T3A
Moreformally,ifwedefinef ,c asthenumberofparametersinthefeatureextractorandclassifier
ω ω
respectively:
P =M ·c (8)
T3A ω
P =(1−β)·(f +c ) (9)
CD ω ω
P =P =NumBNParams (10)
ETA TENT
InFig. 3weshowthatourmethodismemoryefficientwhilemaintainingcompetitivecalibrationand
accuracy. Infact,wecanachievesignificantperformanceupliftsusing≤1MBofextramemory.
5 Discussion
In this work, we identify the over-certainty phenomenon of state-of-the-art UDA methodologies
whichcauseharmtomodelcalibration. Toamelioratethisissue,weintroduceacertaintyregularizer,
τ, which adapts the sharpness of self-labels and persuades overall model entropy. The resulting
algorithm, CD, jointly improves model accuracy and reduces calibration error while remaining
memory efficient. Another merit of our methodology is its compatibility with the majority of
backbonenetworks. CDdoesnotrequirebatchnormalizationlayerslikeTENTandETAdo. This
modelagnosticapproachpermitsgreaterfreedomwhenchoosingasuitablebackbone. Furthermore,
CDiscompatiblewithexistingprototypicallearningapproachessuchasT3Aandtheworkdoneby
[16]. Intheinterestofreproducibility,weprovidethecodeforouralgorithminthesupplementary
materials.
References
[1] ChuanGuo,GeoffPleiss,YuSun,andKilianQWeinberger. Oncalibrationofmodernneural
networks. InInternationalconferenceonmachinelearning,pages1321–1330.PMLR,2017.
[2] MoloudAbdar, FarhadPourpanah, SadiqHussain, DanaRezazadegan, LiLiu, Mohammad
Ghavamzadeh,PaulFieguth,XiaochunCao,AbbasKhosravi,U.RajendraAcharya,Vladimir
10Makarenkov,andSaeidNahavandi. Areviewofuncertaintyquantificationindeeplearning:
Techniques,applicationsandchallenges. InformationFusion,76:243–297,2021.
[3] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-
distributionimagedetectioninneuralnetworks. arXivpreprintarXiv:1706.02690,2017.
[4] Anusri Pampari and Stefano Ermon. Unsupervised calibration under covariate shift. arXiv
preprintarXiv:2006.16405,2020.
[5] RajatRaina,AlexisBattle,HonglakLee,BenjaminPacker,andAndrewYNg. Self-taught
learning: transfer learning from unlabeled data. In Proceedings of the 24th international
conferenceonMachinelearning,pages759–766,2007.
[6] FeiMa,ChengliangWang,andZhuoZeng. Svm-basedsubspaceoptimizationdomaintransfer
methodforunsupervisedcross-domaintimeseriesclassification. KnowledgeandInformation
Systems,pages1–29,112022.
[7] Dong-HyunLeeetal. Pseudo-label: Thesimpleandefficientsemi-supervisedlearningmethod
for deep neural networks. In Workshop on challenges in representation learning, ICML,
volume3,page896.Atlanta,2013.
[8] GeoffreyHinton,OriolVinyals,andJeffreyDean. Distillingtheknowledgeinaneuralnetwork.
InNIPSDeepLearningandRepresentationLearningWorkshop,2015.
[9] JianpingGou,BaoshengYu,StephenJMaybank,andDachengTao. Knowledgedistillation: A
survey. InternationalJournalofComputerVision,129:1789–1819,2021.
[10] SamuelStanton,PavelIzmailov,PolinaKirichenko,AlexanderAAlemi,andAndrewGWilson.
Doesknowledgedistillationreallywork? AdvancesinNeuralInformationProcessingSystems,
34:6906–6919,2021.
[11] Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Best sources
forward: domaingeneralizationthroughsource-specificnets. In201825thIEEEinternational
conferenceonimageprocessing(ICIP),pages1353–1357.IEEE,2018.
[12] JindongWang,CuilingLan,ChangLiu,YidongOuyang,TaoQin,WangLu,YiqiangChen,
WenjunZeng,andPhilipYu. Generalizingtounseendomains: Asurveyondomaingeneraliza-
tion. IEEETransactionsonKnowledgeandDataEngineering,2022.
[13] DequanWang,EvanShelhamer,ShaotengLiu,BrunoOlshausen,andTrevorDarrell. Tent:
Fullytest-timeadaptationbyentropyminimization. arXivpreprintarXiv:2006.10726,2020.
[14] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and
MingkuiTan. Efficienttest-timemodeladaptationwithoutforgetting. InInternationalconfer-
enceonmachinelearning,pages16888–16905.PMLR,2022.
[15] YusukeIwasawaandYutakaMatsuo. Test-timeclassifieradjustmentmoduleformodel-agnostic
domaingeneralization. AdvancesinNeuralInformationProcessingSystems,34:2427–2440,
2021.
[16] JakeSnell,KevinSwersky,andRichardZemel. Prototypicalnetworksforfew-shotlearning.
Advancesinneuralinformationprocessingsystems,30,2017.
[17] SuratTeerapittayanon,BradleyMcDanel,andH.T.Kung. Branchynet: Fastinferenceviaearly
exitingfromdeepneuralnetworks. CoRR,abs/1709.01686,2017.
[18] WeichengZhu,MatanLeibovich,ShengLiu,SreyasMohan,AakashKaku,BoyangYu,Laure
Zanna,NargesRazavian,andCarlosFernandez-Granda. Deepprobabilityestimation,2022.
[19] YanivOvadia,EmilyFertig,JieRen,ZacharyNado,DSculley,SebastianNowozin,JoshuaV.
Dillon,BalajiLakshminarayanan,andJasperSnoek. Canyoutrustyourmodel’suncertainty?
evaluatingpredictiveuncertaintyunderdatasetshift,2019.
[20] HongxinWei,RenchunziXie,HaoCheng,LeiFeng,BoAn,andYixuanLi. Mitigatingneural
network overconfidence with logit normalization. In International Conference on Machine
Learning,pages23631–23644.PMLR,2022.
[21] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-
MingCheng. Delvingdeepintolabelsmoothing. IEEETransactionsonImageProcessing,
30:5984–5996,2021.
[22] RafaelMüller,SimonKornblith,andGeoffreyE.Hinton. Whendoeslabelsmoothinghelp?
CoRR,abs/1906.02629,2019.
11[23] KaiTian,ShuigengZhou,JianpingFan,andJihongGuan. Learningcompetitiveanddiscrimina-
tivereconstructionsforanomalydetection. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume33,pages5167–5174,2019.
[24] ThomasSchlegl,PhilippSeeböck,SebastianM.Waldstein,UrsulaSchmidt-Erfurth,andGeorg
Langs. Unsupervisedanomalydetectionwithgenerativeadversarialnetworkstoguidemarker
discovery. CoRR,abs/1703.05921,2017.
[25] HoussamZenati,ChuanShengFoo,BrunoLecouat,GauravManek,andVijayRamaseshan
Chandrasekhar. Efficientgan-basedanomalydetection,2018.
[26] AbhijitBendaleandTerranceEBoult. Towardsopensetdeepnetworks. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pages1563–1572,2016.
[27] YifeiMing,ZiyangCai,JiuxiangGu,YiyouSun,WeiLi,andYixuanLi. Delvingintoout-of-
distributiondetectionwithvision-languagerepresentations. AdvancesinNeuralInformation
ProcessingSystems,35:35087–35102,2022.
[28] JaewooPark,JackyChenLongChai,JaehoYoon,andAndrewBengJinTeoh. Understanding
thefeaturenormforout-of-distributiondetection.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages1557–1567,2023.
[29] Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han.
Detectingout-of-distributiondatathroughin-distributionclassprior.InInternationalConference
onMachineLearning,pages15067–15088.PMLR,2023.
[30] XinhengWu,JieLu,ZhenFang,andGuangquanZhang. Metaoodlearningforcontinuously
adaptiveooddetection. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages19353–19364,2023.
[31] WenjunMiao,GuansongPang,TianqiLi,XiaoBai,andJinZheng.Out-of-distributiondetection
inlong-tailedrecognitionwithcalibratedoutlierclasslearning.arXivpreprintarXiv:2312.10686,
2023.
[32] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, and Feng Liu. Is out-of-distribution
detectionlearnable? AdvancesinNeuralInformationProcessingSystems,35:37199–37213,
2022.
[33] QizheXie,Minh-ThangLuong,EduardHovy,andQuocVLe. Self-trainingwithnoisystudent
improvesimagenetclassification. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10687–10698,2020.
[34] MichalLukasik,SrinadhBhojanapalli,AdityaMenon,andSanjivKumar. Doeslabelsmoothing
mitigatelabelnoise? InHalDauméIIIandAartiSingh,editors,Proceedingsofthe37thInter-
nationalConferenceonMachineLearning,volume119ofProceedingsofMachineLearning
Research,pages6448–6458.PMLR,13–18Jul2020.
[35] DaLi,YongxinYang,Yi-ZheSong,andTimothyMHospedales. Deeper,broaderandartier
domaingeneralization. InProceedingsoftheIEEEinternationalconferenceoncomputervision,
pages5542–5550,2017.
[36] HemanthVenkateswara,JoseEusebio,ShayokChakraborty,andSethuramanPanchanathan.
Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,pages5018–5027,2017.
[37] J.J.Hull. Adatabaseforhandwrittentextrecognitionresearch. IEEETransactionsonPattern
AnalysisandMachineIntelligence,16(5):550–554,1994.
[38] YannLeCun, CorinnaCortes, andCJBurges. Mnisthandwrittendigitdatabase. ATTLabs
[Online].Available: http://yann.lecun.com/exdb/mnist,2,2010.
[39] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
Readingdigitsinnaturalimageswithunsupervisedfeaturelearning. NeuripsWorkshopon
DeepLearning,2011.
[40] YaLeandXuanYang. Tinyimagenetvisualrecognitionchallenge. CS231N,7(7):3,2015.
[41] MingxingTanandQuocV.Le. Efficientnet: Rethinkingmodelscalingforconvolutionalneural
networks. CoRR,abs/1905.11946,2019.
12[42] AndrewG.Howard,MenglongZhu,BoChen,DmitryKalenichenko,WeijunWang,Tobias
Weyand, MarcoAndreetto, andHartwigAdam. Mobilenets: Efficientconvolutionalneural
networksformobilevisionapplications,2017.
[43] JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-
scalehierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpattern
recognition,pages248–255.Ieee,2009.
[44] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
GregS.Corrado,AndyDavis,JeffreyDean,MatthieuDevin,SanjayGhemawat,IanGoodfellow,
AndrewHarp,GeoffreyIrving,MichaelIsard,YangqingJia,RafalJozefowicz,LukaszKaiser,
Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek
Murray, ChrisOlah, MikeSchuster, JonathonShlens, BenoitSteiner, IlyaSutskever, Kunal
Talwar,PaulTucker,VincentVanhoucke,VijayVasudevan,FernandaViégas,OriolVinyals,Pete
Warden,MartinWattenberg,MartinWicke,YuanYu,andXiaoqiangZheng. TensorFlow:Large-
scalemachinelearningonheterogeneoussystems,2015. Softwareavailablefromtensorflow.org.
[45] Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a
runningaverageofitsrecentmagnitude. COURSERA:Neuralnetworksformachinelearning,
4(2):26–31,2012.
[46] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2014.
13A Appendix
A.1 ExperimentalSetupDetails
WeuseTensorFlow2.9[44]withNvidiaCUDNNversion11.3onanRTX308016GBlaptopGPU
with32GBofsystemmemory.
1. PACS [35] has 4 domains: pictures, art, cartoon, sketch with 7 classes. All images are
resizedto(227,227,3)andscaledbetween[0,255]. TestedusingLOO.
2. HomeOffice[36]has4domains: art,clipart,product,realwith65classes. Allimagesare
resizedto(128,128,3)andscaledbetween[0,255]. TestedusingLOO.
3. Digitsisacombinationof3“numbers”datasets: USPS[37],MNIST[38],andSVHN[39].
The images are resized to (32,32,1) and scaled between [0,255]. There are 10 classes.
TestedusingLOObytrainingonthesourcedomains’trainingsetsandadaptingtotarget
domain’stestset. Forthisexperiment,wesetT andT parametersto1.05and4.0
min max
respectively.
4. TinyImageNet-C(TIN-C)[40],has15domainswith200classes. Allimagesareresizedto
(256,256,3)andscaledbetween[0,255]. Backbonesaretrainedoncorruption-free(source)
trainingset,adaptedtoandevaluatedoncorrupted(target)domains. Foreachtargetdomain,
thereare5tiersofcorruption.
WedomostinitialtrainingonthesourcedomainusingRMS_Prop(lr=2e−4)[45]tominimize
cross-entropylossforepochs={15,15,5,25}foreachenumerateddataset,respectively. Small-
CNNiscompiledandinitiallytrainedwiththeAdamoptimizer[46]inlieuofRMSProp.
NotethatMobileNetexpectsinputstobeprepossessedinauniquemanner. WeuseTensorflow’s
off-the-shelfpre-processinglayerforMobileNetattheinput.
A.2 ResultsusingEffecientNet onTinyImageNet-C
β=98%
14151617A.3 ResultsusingMobileNet onTinyImageNet-C
β=0%
18192021A.4 OtherExperiments
Algorithm Accuracy ECE Entropy
NoAdaptation 0.6646 0.1979 0.6418
CD 0.6686 0.0692 1.5162
β=98%
CD 0.6720 0.0622 1.5576
β=0%
T3A 0.6863 0.2667 0.1857
ETA 0.6778 0.2180 0.4556
TENT 0.6837 0.2105 0.4597
Table 4: Average accuracy, ECE and entropy on the Home Office dataset using the EfficientNet
backbone. Domain-to-domainσ2 =[0.06,0.03,1.11]foraccuracy,ECEandentropyrespectively.
max
Algorithm Accuracy ECE Entropy
NoAdaptation 0.5669 0.2424 0.8235
CD 0.5748 0.06378 1.8699
T3A 0.6169 0.3351 0.1819
ETA 0.5776 0.2395 0.7928
TENT 0.5791 0.2412 0.7761
Table5: Averageaccuracy,ECEandentropyontheHomeOfficedatasetusingtheMobileNet
β=0%
backbone. Domain-to-domainσ2 =[0.01,0.09,1.11]foraccuracy,ECEandentropyrespectively.
max
Algorithm Accuracy ECE Entropy
NoAdaptation 0.8730 0.1054 0.0812
CD(ours) 0.8794 0.0789 0.1565
T3A 0.8973 0.0844 0.0645
ETA 0.8827 0.0975 0.0723
TENT 0.8795 0.1007 0.0719
Table 6: Average accuracy, ECE and entropy on the PACS dataset using the EfficientNet
β=98%
backbone. Domain-to-domainσ2 =[0.06,0.03,0.22]foraccuracy,ECEandentropyrespectively.
max
A.5 Run-to-RunVariances
• FortheDigitsdataset: σ2 =[4.39e−05,5.51e−04,3.85e−03]foraccuracy,ECEand
max
entropy,respectivelyacrossalltrialsusingSmallCNN.
• ForHomeOfficeusingusingtheEffecientNet backbone: andσ2 =[0.05,1.12e−
β=0% max
5,1.66e−5]forentropyandECErespectively.
• For Home Office using the EffecientNet backbone: σ2 = [1.18e−7,1.46e−
β=98% max
5,1.61e−7]foraccuracy,ECEandentropyacrossalltrials.
• ForHomeOfficeusingtheMobileNet backbone:σ2 =[5.13e−5,3.81e−5,9.54e−
β=0% max
7]foraccuracy,ECEandentropyacrossalltrials.
• For the PACS dataset using EfficientNet backbone: σ2 = [1.21e−05,1.02e−
β=98% max
13,2.09e−03]foraccuracy,ECEandentropy,respectivelyacrossalltrials.
22A.6 SmallCNN
Figure5: SmallCNN’sarchitecturehas319,498parameters.
23