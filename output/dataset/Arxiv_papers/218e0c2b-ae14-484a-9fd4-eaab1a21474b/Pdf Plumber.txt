Collaborative Heterogeneous Causal Inference Beyond
Meta-analysis
Tianyu Guo∗∗ Sai Praneeth Karimireddy†† Michael I. Jordan∗†
April 25, 2024
Abstract
Collaboration between different data centers is often challenged by heterogeneity across sites. To
account for the heterogeneity, the state-of-the-art method is to re-weight the covariate distributions in
each site to match the distribution of the target population. Nevertheless, this method could easily
fail when a certain site couldn’t cover the entire population. Moreover, it still relies on the concept of
traditional meta-analysis after adjusting for the distribution shift.
In this work, we propose a collaborative inverse propensity score weighting estimator for causal
inferencewithheterogeneousdata. Insteadofadjustingthedistributionshiftseparately,weuseweighted
propensityscoremodelstocollaborativelyadjustforthedistributionshift. Ourmethodshowssignificant
improvementsoverthemethodsbasedonmeta-analysiswhenheterogeneityincreases. Toaccountforthe
vulnerabledensityestimation,wefurtherdiscussthedoublemachinemethodandshowthepossibilityof
usingnonparametricdensityestimationwithd<8andaflexiblemachinelearningmethodtoguarantee
asymptotic normality. We propose a federated learning algorithm to collaboratively train the outcome
modelwhilepreservingprivacy. Usingsyntheticandrealdatasets,wedemonstratetheadvantagesofour
method.
1 Introduction
The booming of Federated Learning (FL) has drawn attention in medical and social sciences, where sharing
datasets between data centers is often limited. However, their research focuses more on Causal Inference, in
which prediction gets less attention, whereas valid inference is the main focus. For example, Meta-analysis
takes the weighted mean of published estimators of the average treatment effect (ate) and mainly focuses
on choosing optimal weights and making inferences.
Givenhomogeneousdata,howcouldFederatedLearninghelpCausalInference? Theestimationof atecom-
monlyincorporatesnuisancepredictionmodels,e.g.,thepropensityscoremodel. Thankstothehomogeneity,
we can use FL methods to train a shared propensity score model, then each site gets its own ate estimator,
and finally, the central server uses Meta-analysis to take the weighted mean.
Nevertheless,givenheterogeneousdata,FederatedLearningseemstoplayanegligiblerole. Sincepropensity
scoremodelsdifferbetweensites,trainingasharedmodelismeaningless. Asaresult,allmethodsfallwithin
the scope of Meta-analysis. For example, to estimate the ate for a target site, Han et al. (2022) and Han
et al. (2023b) consider using density ratio to re-weight source sites and summarizing estimates from source
sites with Meta-analysis.
We propose a novel method tailored for collaboration with heterogeneous data. Suppose we have K sites,
denote the ate as τ, the nuisance propensity model as e, the site-wise weight as η . Instead of taking the
k
weighted mean afterward, we directly take the weighted mean of nuisance models and get τˆ
((cid:80)K
η eˆ ) in
k r=1 r r
each site k, which is inconsistent. Then, we could recover a consistent estimator τˆClb by taking the average
∗DepartmentofStatistics,UCBerkeley. Email: tianyu_guo@berkeley.edu
†DepartmentofEECS,UCBerkeley.
1
4202
rpA
42
]LM.tats[
1v64751.4042:viXraacross all sites. Equations (1) and (2) summarize the previous and our estimators.
K K
(cid:88) (cid:88)
τˆ = η τˆ (eˆ ) τˆ = η τˆ (eˆ ), (1)
homo k k FL heter k k k
k=1 k=1
K
we propose: τˆClb
=(cid:88)
τˆ
k((cid:80)K
r=1η reˆ r). (2)
k=1
Our method outperforms previous ones in several ways: first, it is the first method that allows collabo-
ration across disjoint domains without additional assumptions; second, it achieves better accuracy than
Meta-analysis; third, it remains stable even as the heterogeneity between sites increases, which encourages
collaboration from a broader range. We provide theory and experiment to demonstrate these claims.
2 Problem Setup
We use S = [K] to denote the set of sites, with D(k) being the dataset of site k. Let Z be the binary
treatment, X ∈ Rd be the covariates with dimension d, Y be the outcome. Let Y(z) be the potential
outcome under treatment z ∈ {1,0}. Classical causal inference only copes with the biased sampling of Z.
However, we need to cope with multiple sites. We first present a motivating example from Meta-analysis to
model the actual data-generating procedure.
Example 1 (Collaboration of Clinical Trails). Koesters et al. (2013) reviews clinical trials of Agomelatine,
an antidepressant drug approved by the European Medicines Agency in 2009. The 13 included trials have
different data sizes and demographic distributions. One study was carried out on individuals aged 60 or
above, and the remaining is for all ages. Each study reports the mean difference in Hamilton Rating Scale
for Depression (HRSD) scores between treatment and control groups.
ThetargetgroupinExample1isthepatientswithdepression. However,eachclinicaltrialisabiasedsample
from the target population. Abstracting from this example, we propose a sampling-selecting framework for
collaborative causal inference:
1. Sampling: Sample an individual i from the target distribution, let S ∈{(k,Z)|k ∈S,Z ∈{0,1}}∪∅
i
be the selection indicator. If S = (k,1), individual i gets selected to site k and gets treated. If
S =∅, theindividualiseliminatedfromthedataset. Sample(X ,Y (1),Y (0),S )i.i.d. fromthetarget
i i i i
distribution according to Equation (3) and get the pooled dataset (4).
P(S =∅|X)=e∅(X),
P(S =(k,z)|X)=e(k,z)(X)
K
(cid:88) (cid:88)
with e∅(X)+ e(k,z)(X)=1. (3)
k=1z∈{0,1}
D ={(X ,Y (1),Y (0),S )|i∈[N]}. (4)
Meta i i i i
2. Selecting: WeuseZ(S )todenotethetreatmentindicatorcorrespondingtoS . Wefollowthepotential
i i
outcome framework and invoke the Stable Unit Treatment Value Assumption (SUTVA). Therefore,
Y =Z(S )Y (1)+{1−Z(S )}Y (0). Split D to each site and treatment/control groups according
i i i i i Meta
to S and get
D(k) ={(X ,Y ,Z )∈D |S =(k,Z )}. (5)
i i i Meta i i
Furthermore, census data commonly reflects the target distribution of covariates. Therefore, we assume
there’s a public dataset D(t) that contains covariates information.
D(t) ={(X )|X drawn i.i.d. from target distribution}. (6)
i i
2(a) Sampling-Selecting Framework.
(b) KL-MSE curve of different estimators
Target Population Selection to sites
3.5
S=(k,1) 3.0
2.5
S=(k,0) 2.0
1.5
S=(ℓ,1) 1.0
D(t)
0.5
⋮ ⋮
0.0
Census Data Discard S=∅ 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Heterogeneity
rorrE
deruaqS
naeM
Meta IPW
Meta AIPW
Clb IPW
Clb AIPW
IPW in sites
Figure 1: Visualization of the data-generating process and the comparison of proposed estimators.
Figure1avisualizesthedata-generatingprocess. Eachsiteselectsfromthetargetdistributioninadifferent
way. The selection indicator S =(k,z) describes the sampling mechanism of the Z =z group in site k. The
figureshowsdistributionshiftswithleftandrighttilting(S =(k,1),(k,0)),undercoverage(S =(ℓ,1)),and
discarded data S =∅. Xiong et al. (2022) assumes that i.e., P(S =∅)=0 and consider the pooled dataset.
In contrast, we allow S = ∅ to reflect the biased sampling of the pooled dataset ∪K D(k) from the target
k=1
population, which is neglected by Xiong et al. (2022). For example, if all sites include fewer men in the
dataset, we would have that P(S ̸=∅|men)<P(S ̸=∅|other genders).
One may question that there’s no real sampling-selecting process since each site collects data independently.
Apossibleansweristorecallthewell-acceptedquasi-experimentframework(Cooketal.,2002). Forexample,
tounderstandtheeffectofgenderonanoutcome. Thequasi-experimentframeworkimaginesthatindividual
X firstlygetsi.i.d.sampled,thengets“treated” bygenderG ,althoughthere’snoactual“gender” assignment
i i
process. The sampling-selecting framework extends the quasi-experiment to multiple-site settings.
The objective is to estimate the average causal effect on the target distribution τ = E[Y(1)−Y(0)]. A
foundation for identifying τ is Assumption 1.
Assumption 1 (Homogeneity and unconfoundedness). We have that
(Y(1),Y(0))⊥⊥S |X (7)
More than unconfounded treatment assignment, Assumption 1 also implies that the individual treatment
effectsarethesameacrosssites. InExample1,whenfixingX foranindividuali,iftheeffectofAgomelatine
stillvariesacrosssites,collaborationismeaninglessduetounmeasuredconfounders. ViolationofAssumption
1 is sometimes termed as anti-causal learning (Farahani et al., 2020).
Another foundation for identifying the causal effect is the overlapping assumption. There are two kinds of
overlappingassumptions. Givenanindividual, Assumption2requireseachsitetoselectthemwithnon-zero
probability, whereas Assumption 3 only requires the overall selection probability to be non-zero.
Assumption 2 (Individual-Overlapping). We have that
min{P(Z =1,S =1|x,A=k)}>c>0.
x,k
Assumption 3 (Overall-Overlapping). We have that
min{P(Z =1,S =1|x)}>c>0.
x
Revisiting Example 1, Assumption 1 is guaranteed by the experimental design and by the similar effect
of drugs given sufficient demographic information. Assumption 2 fails since one site only includes senior
patients. While Assumption 3 holds since other sites collect data from all ages.
3We provide a counter-example showing that Assumptions 1 and 3 might not hold.
Example 2 (Collaboration of Observational Studies with Unmeasured Confounder). Betthäuser et al.
(2023) reviews observational studies regarding the learning deficits of school-aged children during COVID-
19. Among 42 included observational studies, four were from middle-income countries, and the remaining
were from high-income countries. Over half of the studies didn’t collect covariates and took the difference
in means of grades before and after the pandemic.
Sinceoverhalfofthestudiesdidn’tcollectcovariates,thereareunmeasuredconfounders. Therefore,Assump-
tion 1 is unlikely to hold. Moreover, if the target distribution is school-aged children from the entire world,
both 2 and 3 fail since low-income countries are missing in the study. We suggest avoiding collaboration in
this case.
We have some additional notations: Denote E[Y ] as µ and E[Y ] as µ . Define N = (cid:80)K N(k) with
1 1 0 0 S k=1
N(k) = |D(k)| being the sample size of dataset k. Note that N < N since we drop the individuals with
S
S =∅.
2.1 Related Work
ThereareextensiveattemptsinMeta-analysisliteraturetocopewithheterogeneity(Borensteinetal.,2007,
2010; Higgins et al., 2009). For example, by assuming that the average treatment effect follows the normal
distribution across sites, many propose using random effects models (Hedges & Vevea, 1998; Riley et al.,
2011)insteadoffixedeffectsmodels(Tufanaruetal.,2015). Thereareotherways,suchasusingsite-specific
informationandconductMeta-regression(vanHouwelingenetal.,2002;Glynn&Quinn,2010),usingquasi-
likelihood (Tufanaru et al., 2015). More recently, Cheng & Cai (2021) propose a penalized method for
integrating heterogeneous causal effects. However, all methods need strong parametric assumptions on the
heterogeneity. It’s still necessary to rely on qualitative understandings of heterogeneity based on summary
statistics (Stroup, 2000).
CausalInferenceliteraturealsohasagrowinginterestincollaborationwithconcernsinexternalvalidity(Con-
cato et al., 2000; Rothwell, 2005; Colnet et al., 2023). Yang & Ding (2020) propose a Rao-Blackwellization
method for incorporating RCT and observational studies with unmeasured confounders to improve the es-
timation efficiency. Recently, more works try to incorporate federated learning in causal inference (Xiong
et al., 2022; Han et al., 2023a; Guo et al., 2023; Vo et al., 2023). (Vo et al., 2022) proposes adaptive kernel
methods under the causal graph model. Several focus on inference. For example, (Xiong et al., 2022; Hu
et al., 2022) assumes homogeneous models and proposes a collaboration framework that avoids direct data
merging. Han et al. (2022, 2023b) considers heterogeneous sample selection under parametric distribution
shift assumptions. Nevertheless, most new methods still fall under the framework of Meta-analysis.
As a broader interest, our work also uses double machine learning.Chernozhukov et al. (2018); Athey &
Imbens (2019). It extends the doubly robust estimator (Bang & Robins, 2005; Glynn & Quinn, 2010; Funk
etal.,2011)tonon-parametricandmachinelearningmethods(Huangetal.,2006;Sugiyamaetal.,2007b,a;
Wager & Athey, 2017; Tibshirani, 1996). We adopt it in particular to mitigate the hardness of estimating
density ratio (Farahani et al., 2020; Härdle et al., 2004).
3 Collaborative Inverse Propensity Score Weighting
Theinversepropensityscoreweighting(ipw)estimatorplaysacentralroleincausalinference. Wegeneralize
ittocollaborativesetting,thinkinge(k,z)(X)=P(S =(k,z)|X)asageneralizedversionofpropensityscore.
We begin with using the oracle propensity score models and then discuss how to estimate the models.
3.1 The Clb-IPW estimator
As a benchmark, consider the method where each site calculates its own ipw estimator for ate and takes
weightedsum, whichisthestandardmethodinMeta-analysis. Sinceweassumepropensityscoremodelsare
4correct, it’s not necessary to use L penalty as in Han et al. (2022). Define
1
K
τˆ =(cid:88) η(k)(µˆ(k) −µˆ(k) ), with
Meta Meta,1 Meta,0
k=1
µˆ(k) = 1 (cid:88) Z iY i , (8)
Meta,1 Nˆ(k) e(k,1)(X i)
Meta,1 i∈D(k)
µˆ(k) = 1 (cid:88) (1−Z i)Y i, (9)
Meta,0 Nˆ(k) e(k,0)(X i)
Meta,0 i∈D(k)
Nˆ(k) = (cid:88) Z i , and (10)
Meta,1 e(k,1)(X )
i
i∈D(k)
Nˆ(k) = (cid:88) 1−Z i . (11)
Meta,0 e(k,0)(X )
i
i∈D(k)
Equations (8), (9), (10), and (11) take the Hájek form Little & Rubin (2019), in which we use a consistent
estimator Nˆ for the sample size. It always achieves better numerical stability and smaller variance than
directly using sample size. More importantly, as we will show later, we could only identify e(k,z)(X) up to a
constant factor, Hájek form releases us from the identifiability issue.
The best choice of η(k) is the inverse variance. In specific, denoting Var(τˆ(k) ) = (σ(k) )2 , the optimal
Meta Meta
weights are η(k) ∝(σ(k) )−2 . See Cheng & Cai (2021); Ye et al. (2021) for more discussions.
Meta
Meta-ipw is designed for review studies (Borenstein et al., 2007) rather than collaboration. Each site must
be able to obtain a valid estimator. But, a single site would commonly suffer from under-coverage of the
entire population. Revisiting Example 1, one site only takes experiments for older people. Due to their
under-coverage, they can never get the valid ate estimator for the entire population, so it’s impossible to
incorporate them into the Meta-ipw estimator.
Alternatively, we introduce the Clb-ipw estimator. Clb-ipw directly takes the weighted mean of heteroge-
neous propensity score functions. In specific, to estimate µ , we use
1
µˆ(k) = 1 (cid:88) η(k)Z iY i , (12)
Clb,1 Nˆ(k) (cid:80)K η(r)e(r,1)(X )
Clb,1 i∈D(k) r=1 i
with Nˆ(k) = (cid:88) η(k)Z i
Clb,1 (cid:80)K η(r)e(r,1)(X )
i∈D(k) r=1 i
We have that
(cid:104) η(k)e(k,1)(X)Y (cid:105)
E[µˆ(k) ]=E 1 ,
Clb,1 (cid:80)K η(r)e(r,1)(X)
r=1
which means that it’s not consistent for µ . However, when we take summation of µˆ(k) across k, we get
1 Clb,1
that
(cid:104)(cid:80)K η(k)e(k,1)(X)Y (cid:105)
E[µˆClb,1]=E (cid:80)k= K1 η(r)e(r,1)(X)1 =µ 1.
r=1
It allows collaboration between disjoint domains. In Example 1, the site that only includes elders could
compute µˆ(k) without worrying about their under-coverage. Given a young patient X from other sites.
Clb,1
We have that e(k,1)(X) = 0 but e(r,1)(X) > 0 for r ̸= k, which ensures a non-zero denominator. The
estimators for µ
0
follow the same manner, which we relegate to the appendix. We could compute τˆClb in a
fully federated way, as presented in Algorithm 1.
5Algorithm 1 Clb-ipw Algorithm
Require: K datasetswithD(k) asshowninEquation(5). Eachsitepublishestheirpropensityscoremodels
e(k,1)(X) and e(k,0)(X).
1: for k =1 to K do
2: At site k, calculate µˆ( Ck l) b,1 , µˆ( Ck l) b,0, Nˆ C(k l) b,1, and Nˆ C(k l) b,1 according to Equation (12). Send them to the
central server.
3: end for
4: Central server computes
τˆClb =µˆClb,1−µˆClb,0, (13)
where µˆClb,1 is the average of µˆ( Ck l)
b,1
weighted by Nˆ C(k l) b,1, with µˆClb,0 following the same manner.
Thebestchoiceofη(k) isdata-dependentandthuscouldnotbeobtainedfromoneroundofcommunication.
Therefore, we suggest taking vanilla weights η(k) =1 for all k. Notice that
K
(cid:88)
e(k,1)(X)=P(Z(S)=1|X), (14)
k=1
which means that the vanilla weights match the propensity score for Z in the pooled dataset. More impor-
tantly, we find that the vanilla weights would already make the Clb-ipw estimator uniformly better than
Meta-ipw estimator.
Proposition 1 (Meta-ipw Estimator). Given Assumptions 1 and 2, using inverse variance weighting, as
N →∞, we have that
√
N(τˆ −τ)→d N(0,v2 ),
Meta Meta
where
−1
v2
=(cid:110)(cid:88)K E(cid:104)(Y 1−µ 1)2
+
(Y 0−µ 0)2(cid:105)(cid:111)
.
Meta e(k,1)(X) e(k,0)(X)
k=1
Theorem 2 (Clb-ipw Estimator). Given Assumptions 1 and 3, using vanilla weights for Clb-ipw, as
N →∞, we have that
√
N(τˆClb−τ)→d N(0,v C2 lb), (15)
where
(cid:104) (Y −µ )2 (Y −µ )2 (cid:105)
v2 =E 1 1 + 0 0
Clb (cid:80)K e(k)(X) (cid:80)K e(r,0)(X)
k=1 r=1
Moreover, we have that
v2 ≤v2 .
Clb Meta
There are two ways to understand why τˆClb is better: First, Meta-ipw takes the weighted mean site-wise,
whereas Clb-ipw takes the weighted mean individual-wise. Given each individual X , Clb-ipw adaptively
i
putsmoreweightsonsiteswithlargere(k)(X ). WhereasMeta-ipwusesthesameweightsforanyX . Second,
i i
Clb-ipw utilizescoarserbalancing scores Imbens&Rubin(2015). Balancing score isageneralizationofthe
propensity score. Any function of covariates is sufficient for adjusting the confoundingness between Z and
Y. The Meta-ipw uses P(S |X) as its inverse weights, and Clb-ipw uses P(Z(S)|X). Theorem 3 shows
that they are both balancing scores.
Theorem 3. We have that
(Y(1),Y(0))⊥⊥Z(S)|P(S |X), and
(Y(1),Y(0))⊥⊥Z(S)|P(Z(S)|X).
6Notice that P(S |X) has an auxiliary variable k(X) comparing to P(Z(S)|X). But k(X) is superfluous
since it doesn’t affect (Y ,Y ). As a result, Clb-ipw gets better efficiency by maintaining a smaller model.
1 0
A simpler model benefits us by maintaining fewer variables to adjust for, thus attaining better efficiency.
Similar ideas occur extensively in model selection literature Raschka (2020).
3.2 Estimation of propensity score models
We start from the identification of e(k,z)(X). Since we have no information on the dropped set D =
∅
{i|S =∅},it’simpossibletoidentifyallparameters. Forinstance,multiplyingN byafactor2anddividing
i
e(k,z)(X) by 2 would lead to the same observed distribution. However, identifiability is guaranteed up to
a constant factor. And thanks to the Hájek forms of our ipw estimators, identification up to a constant is
enough.
Proposition 4. We have that
e(k,z)(X)=r(k,z)(X)P(S =(k,z)|S ̸=∅)P(S =∅).
where r(k)(X) = p(X |S =(k,z))/p(X) is the density ratio function, which is identifiable. Meanwhile,
P(S =(k,z)|S ̸=∅) is identifiable by taking N(k)/N . Only P(S =∅) is not identifiable.
S
Wefocusonestimatingdensityratior(k,z)(X). Wesuggesttwomethodsfromthelargeliteratureondensity
ratio estimation. Han et al. (2022) applies a parametric exponential tilting model. They assumes that
r(k,z)(X) = exp(ψ(X)⊤γ(k,z)) for a given representation function ψ (such as ψ(x) = x) and unknown
parameter γ(k,z). We could estimate γ through the method of moments, i.e., finding γˆ(k,z) that solves
(cid:88) Z ψ(X )exp(ψ(X)⊤γ(k,z))
i i
i∈D(k)
= (cid:88) ψ(X )exp(ψ(X)⊤γ(k,z)),
i
i∈Dt
which is equivalent to entropy balancing Zhao & Percival (2017). Recently, motivated by Matching Abadie
& Imbens (2016) and K-Nearest Neighbour Zhang et al. (2018), Lin et al. (2021) propose a minimax non-
parametric way to estimate the density ratio. Using their method, we have that
N(t) M
rˆ(k,z)(x)= , (16)
(cid:80)
Z W(x;Dt,D(k,z))
i∈D(k) i
where W(x;Dt,D(k,z)) means the total number of units in Dt that x is close to X than its M-nearest
i
neighbour in D(k,z). See Lin et al. (2021) for more detail. We have the following convergence rates for them
Proposition 5 (Point-wise error of density estimation). Given x ∈ Rd, if the exponential tilting model is
correctly specified, we have that
(cid:104) (cid:105)
E |exp(ψ(x)⊤γˆ(k,z))−r(k,z)(x)| =O(N−1/2). (17)
For the nonparametric method, we have that
(cid:104) (cid:105)
E |rˆ(k,z)(x)−r(k,z)(x)| =O(N−1/(2+d)). (18)
4 Incorporating Outcome Models
Density ratio estimation is challenging and can easily fail under mis-specification or due to the curse of
dimensionality. Therefore, it is essential to incorporate outcome models to mitigate the errors caused by
densityratioestimation. TomaintainconsistentstructurewithSection3,wefirstdiscusshowtoincorporate
outcome models in the estimator and then discuss how to learn the outcome models.
74.1 Decoupled AIPW estimator
Theaugmentedinversepropensityscoreweighted(aipw)estimatorBang&Robins(2005)employsNeyman
orthogonality to construct an asymptotically normal estimator even if nuisance models converge at slower
rates. We introduce their idea to the collaboration setting.
How to use outcome models? Due to the biased selection of S, directly taking the mean across all source
data renders the estimator inconsistent. A natural idea is to use the inverse propensity score to adjust the
distribution and get that
τˆ =
1 (cid:88) (cid:104) mˆ 1(X i)
−
mˆ 0(X i) (cid:105)
.
adjust N eˆ(k,1)(X ) eˆ(k,0)(X )
i i
Si̸=∅
This is the choice of Han et al. (2022). However, the consistency of τˆ substantially depends on the
adjust
density ratio function, making the regression model useless. Alternatively, we make use of the public census
dataset D(t). As discussed in Section 2, D(t) provides public information for X in the target distribution.
Utilizing it, we propose a decoupled aipw estimator.
N(t) K
τˆaipw = N1
(t)
(cid:88)(cid:104) mˆ 1(X i(t))−mˆ 0(X i(t))(cid:105) +(cid:88) δˆ a(k ip) w, (19)
i=1 k=1
with δˆ a(k ip) w having two versions:
K
δˆ(k) =(cid:88) η(k)(cid:104) δˆ(k) −δˆ(k) (cid:105) ,
Meta−aipw Meta−aipw,1 Meta−aipw,0
k=1
K K
δ(k) =(cid:88) wˆ(k) δˆ(k) −(cid:88) wˆ(k) δˆ(k) ,
Clb−aipw Clb,1 Clb−aipw,1 Clb,0 Clb−aipw,0
k=1 k=1
with wˆ(k) ∝Nˆ(k) , wˆ(k) ∝Nˆ(k) .
Clb,1 Clb,1 Clb,0 Clb,0
Here δˆ(k) and δˆ(k) are residual versions of the corresponding ipw estimators, changing all Y to
Meta−aipw Clb−aipw
Y −m(X) in the formula. We only present the formula for the δˆ ’s and relegate δˆ ’s to the appendix.
1 0
δˆ(k) = 1 (cid:88) Z i[Y i−m 1(X i)] ,
Meta−aipw,1 Nˆ(k) e(k,1)(X i)
Meta,1 i∈D(k)
δˆ(k) = 1 (cid:88) Z i[Y i−m 1(X i)] .
Clb−aipw,1 Nˆ(k) (cid:80)K e(r,1)(X )
Clb,1 i∈D(k) r=1 i
The proposed estimator computes the difference in mean of outcome models only in D(t) and the correction
terms only in D(k)’s. Though being decoupled, it preserves the robustness of the aipw estimator. We
summarize its properties in Theorem 6.
Theorem 6. Suppose that
1. The estimated models mˆ , mˆ and eˆare independent1 with D(t) and D(k)’s.
1 0
2. They have convergence rates
E[∥mˆ −m ∥ ],E[∥mˆ −m ∥ ]=O(1/N−ξm), (20)
1 1 2 0 1 2
and E[∥eˆ−e∥ ]=O(1/N−ξe), (21)
2
with ξ ξ >1/2.
m e
1Wecouldachieveindependencebyusingsamplingsplitting,seeChernozhukovetal.(2018)formoredetaileddiscussion.
83. The models eˆ, mˆ, e, and m are bounded.
Further supposing that N(t)/N →λ, we have that
S
√
N(τˆClb−aipw−τ)→d N(0,v C2 lb−aipw), (22)
with
(cid:104) (cid:105)
v2 = λ−1E [m (X)−m (X)]2 −λ−1τ2
Clb−aipw 1 0
(cid:104) (Y −m (X))2 (Y −m (X))2 (cid:105)
+E 1 1 + 0 0 .
P(Z(S)=1|X) P(Z(S)=0|X)
The assumptions in Theorem 6 are standard in the literature (Chernozhukov et al., 2018; Athey & Wager,
2020). If we use the K-NN density ratio estimation (Lin et al., 2021), we get that ξ =2/(2+d). Therefore,
e
takinganyoutcomemodelwithξ
m
≥1/2−2/(2+d)wouldguaranteetheasymptoticnormalityofτˆClb−aipw.
4.2 Estimation of outcome models
It’s worth noting the convergence rates in Equation (20) are taking average over the target population. To
achievelowexcessriskinthetargetpopulation,weadoptthedomainadaptationpartfromorthogonalstatis-
tical learning Foster & Syrgkanis (2020). Consider the loss function re-weighted through inverse propensity
scores:
K
(cid:88)
L(m ;{D(k)} )= L(k)(m ;D(k))
1 k∈S 1
k=1
with L(k) =
(cid:88) Z iℓ(Y i,m 1(X i))
. (23)
(cid:80)K eˆ(r,1)(X )
i∈D(k) r=1 i
We want to compare it with training directly on the target distribution, i.e., using loss function L˜
N
L˜(m ;D)=(cid:88) ℓ(Y (1),m (X )). (24)
1 i 1 i
i=1
Theorem 7. Suppose that
1. The estimated propensity score model eˆ(X) satisfies Equation (21).
2. Using loss function (24), mˆ satisfies Equation (21).
1
Then, using loss function (23), we have that
E[∥mˆ (X)−m (X)∥ ]≤O(1/N−ξm)+O(1/N−4ξe). (25)
1 1 2
4.3 Federated Learning Algorithm
The estimation of the outcome model requires federated learning. We could optimize the loss function by
using FedAvg Li et al. (2020) or SCAFFOLD Karimireddy et al. (2020). We present the process, including
computing τˆaipw in Algorithm 2.
As a result, using Algorithm 2, if we combine Theorems 6 and 7, we could get that τˆClb−aipw is asymptotic
normal given that ξ ξ < 1/2 and ξ5 < 1/2. Using Proposition 5, it suffices to utilize the K-NN density
m e e
ratio estimation method with d≤8 and find an outcome model with ξ ≥1/2−2/(2+d). This avoids the
m
problem of the misspecification of the exponential tilting model.
It is worth noting that our discussion of aipw is is from the point of view of learning theory. If we adopt
the classical double robustness framework, when the outcome model is correctly specified, there’s no need
to adjust the distribution of the covariates. The aipw estimator is asymptotically normal even when the
propensity score model completely fails. We would demonstrate its robustness in the simulation.
9Algorithm 2 Clb-aipw Algorithm
Require: K datasets {D(k)} and D(t).
k∈S
1: (Locally) estimate eˆ(k,z)(X).
2: while not converged do
3: Train model m 1 and m 0 using the FedAvg Algorithm with Loss function in (23).
4: end while
5: (Locally) update Y i’s by Y i →Y i−m Zi(X i).
6: Use Algorithm 1 to get (cid:80)K k=1δ C(k l) b−aipw.
7: Construct the Clb-aipw estimator using Equation (19).
5 Experiments
5.1 Synthetic Dataset
We conduct the experiment using synthetic dataset. Echoing the discussion in Section 2, to show that
the sampling-selecting procedure is not necessarily to truly happen, we fix sample sizes and generate the
covariates using different distributions. Consider three source datasets, with N(k) =1000, 2000, 3000. The
target dataset contains N(t) = 10000 data points. In specific, we generate the target distribution through
X ∼N(µ(t),σ2I ) with µ(t) =−0.1 and σ =2.
3
In the source dataset, we fix the treatment assignment mechanism and take the true propensity score as
P(Z(k) =1|X(k))=1/[1+exp([1.2;0.3;−1.2]⊤X(k))].
Take the true potential outcomes as
Y(1)=[1.2;1.8;1.4]⊤X(k)
and Y(0)=[0.6;0.7;0.6]⊤X(k).
Wealsochoosenormaldistributionforsourcedatasets. SupposethatX(k) ∼N(µ(k),σ2),withσ =2. Weuse
the mean KL−divergence between source datasets to the target dataset as a measure for the heterogeneity
across sites, which is given by
3
d (D(t),{D(k)} )=(cid:88) 1 (µ(k)−µ)2 .
KL k∈[3] 2σ2
k=1
We increase d from 0 to 4. Fixing each d , we choose µ(k) uniformly and randomly assign negative
KL KL
sign to one of them. In the estimation process, we use the exponential tilting model for density ratio
estimation and the linear model for outcome regression. We calculate the mean squared error (MSE) of
Meta-ipw, Clb-ipw, and Meta-aipw, and Clb-aipw through 2000 Monte Carlo Simulations, with four
replications of different {µ(k)}’s. Figure 1b shows the d −MSE curve. We mark the ipw estimators in
KL
each single site with dotted line. Although outperforming each individual sites, the Meta-ipw estimator
still suffers from the increasing of heterogeneity. In contrast, both Clb-ipw and aipw remain stable when
heterogeneity increases.
We further demonstrate the robustness of the aipw estimator with four combinations of specifications of
propensity score and outcome models. We relegate the detail of how to construct mis-specified model to the
appendix. Figure 2a shows the 95% C.I. of the Meta-ipw, Clb-ipw, Meta-aipw, and Clb-aipw estimators.
We choose the case with the mean KL-distance being 3. In all cases, Clb-ipw estimator has tighter
confidence intervals. When propensity score model is misspecified, both Meta-ipw and Clb-ipw fail due to
incorrect weighting. In contrast, aipw estimators remain consistent as long as outcome model is correct.
When both models get misspecified, there is no hope to obtain consitent result.
10(a) Synthetic dataset (b) Real dataset
True PS; True OM True PS; False OM
6 6
Meta IPW
4 Clb IPW 4
Meta AIPW
2 Clb AIPW 2
3.5
0 0
2 2 3.0
4 4
False PS; True OM False PS; False OM 2.5
6 6
4 4
2.0
2 2
0 0 1.5
2 2
1 2 Meta IPW Clb IPW Meta AIPW Clb AIPW
4 4
tceffE
tnemtaerT
detamitsE
×102
Figure 2: The 95% confidence intervals for synthetic dataset and the real dataset. The red dots mark the
true effect size. In Figure 2a, Clb-ipw shows smaller variance than Meta-ipw under all scenarios. The
aipw estimator remains consistent when either of the PS or OM model is correctly specified. In Figure 2b,
τ denotes the estimated causal effect in Pennycook et al. (2020), and τ denotes Roozenbeek et al. (2021).
1 2
We find that Meta-ipw, Clb-ipw, Meta-aipw, and Clb-aipw estimators have similar performance, with
Meta-ipw and Meta-aipw showing slightly larger effect sizes.
5.2 Real world application
We present a real-world application of our method. Our data comes from two studies about preventing
sharingfakenewsduringCOVID-19. Roozenbeeketal.(2021)replicatestheexperimentofPennycooketal.
(2020) to study the effect of a nudge intervention on preventing the sharing of fake news. Both of the two
studies sample participants according to U.S. census through online platforms. The outcome is measured
bythedifferenceofsharingintentionsbetweentrueandfalseheadlinesaboutCOVID-19(truthdiscernment
score). They find that a simple accuracy reminder could increase the truth discernment score (τˆ = 0.034,
p<0.001). Usingthesamedesignandanalysisprocedures,Roozenbeeketal.(2021)replicatestheirfindings,
though with a less significant effect size (τˆ=0.015, p≈0.017).
Although two studies both try to sample from the target distribution and their heterogeneity is well-
controlled, as suggested by Jin et al. (2023), we still use exponential tilting method to adjust the covariates
shift. WeadjustthedistributionforthemeanandvarianceoftheCognitiveReflectionTest(CRT)score,the
scientificknowledgequizscore, theMedicalMaximizer-MinimizerScale(MMS),distributionofself-reported
political leanings, gender, and age. Figure 2b presents the 95% C.I.s for the two datasets and three estima-
tors. Due to that the two datasets are close, we find close results. But Clb-ipw and aipw show slightly
larger effect size, matching the conclusion of the original study.
6 Conclusion
Inthiswork,weproposeacollaborativeinversepropensityscoreestimatorthatissuitableforheterogeneous
data. Along the way, we utilize the sampling-selecting framework to describe the heterogeneity across sites.
We show that the Clb-ipw estimator outperforms Meta-analysis-based estimator both in theory and in
simulation. To account for the difficulty of density estimation, we borrow ideas from AIPW and orthogonal
statistical learning literature, and provide the necessary convergence rates for nuisance models. As a future
direction, it is worth while to explore the communication-efficient method for the optimal weighting of
propensity score models.
11References
Abadie, A. and Imbens, G. W. Matching on the Estimated Propensity Score. Econometrica, 84(2):781–807,
2016. ISSN 0012-9682. doi: 10.3982/ECTA11293. URL https://www.econometricsociety.org/doi/
10.3982/ECTA11293.
Athey, S. and Imbens, G. Machine Learning Methods Economists Should Know About, March 2019. URL
http://arxiv.org/abs/1903.10075. arXiv:1903.10075 [econ, stat].
Athey, S. and Wager, S. Policy Learning with Observational Data, September 2020. URL http://arxiv.
org/abs/1702.02896. arXiv:1702.02896 [cs, econ, math, stat].
Bang, H. and Robins, J. M. Doubly Robust Estimation in Missing Data and Causal Inference Mod-
els. Biometrics, 61(4):962–973, 2005. ISSN 1541-0420. doi: 10.1111/j.1541-0420.2005.00377.x.
URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2005.00377.x. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2005.00377.x.
Betthäuser, B. A., Bach-Mortensen, A. M., and Engzell, P. A systematic review and meta-analysis of the
evidence on learning during the COVID-19 pandemic. Nature Human Behaviour, 7(3):375–385, March
2023. ISSN 2397-3374. doi: 10.1038/s41562-022-01506-4. URL https://www.nature.com/articles/
s41562-022-01506-4. Number: 3 Publisher: Nature Publishing Group.
Borenstein, M., Hedges, L., and Rothstein, H. Introduction to Meta-Analysis. 2007.
Borenstein, M., Hedges, L. V., Higgins, J. P., and Rothstein, H. R. A basic introduction to fixed-effect and
random-effectsmodelsformeta-analysis. Research Synthesis Methods,1(2):97–111,2010. ISSN1759-2887.
doi: 10.1002/jrsm.12. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.12. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.12.
Cheng, D. and Cai, T. Adaptive Combination of Randomized and Observational Data, November 2021.
URL http://arxiv.org/abs/2111.15012. arXiv:2111.15012 [stat].
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J.
Double/debiased machine learning for treatment and structural parameters. The Econometrics Jour-
nal, 21(1):C1–C68, February 2018. ISSN 1368-4221, 1368-423X. doi: 10.1111/ectj.12097. URL
https://academic.oup.com/ectj/article/21/1/C1/5056401.
Colnet, B., Mayer, I., Chen, G., Dieng, A., Li, R., Varoquaux, G., Vert, J.-P., Josse, J., andYang, S. Causal
inference methods for combining randomized trials and observational studies: a review, January 2023.
URL http://arxiv.org/abs/2011.08047. arXiv:2011.08047 [stat].
Concato, J., Shah, N., and Horwitz, R. I. Randomized, Controlled Trials, Observational Studies, and the
Hierarchy of Research Designs. New England Journal of Medicine, 342(25):1887–1892, June 2000. ISSN
0028-4793, 1533-4406. doi: 10.1056/NEJM200006223422507. URL http://www.nejm.org/doi/abs/10.
1056/NEJM200006223422507.
Cook,T.D.,Campbell,D.T.,andShadish,W. Experimental and quasi-experimental designs for generalized
causal inference, volume 1195. Houghton Mifflin Boston, MA, 2002.
Farahani, A., Voghoei, S., Rasheed, K., and Arabnia, H. R. ABrief Review of Domain Adaptation, October
2020. URL http://arxiv.org/abs/2010.03978. arXiv:2010.03978 [cs].
Foster,D.J.andSyrgkanis,V. OrthogonalStatisticalLearning,September2020. URLhttp://arxiv.org/
abs/1901.09036. arXiv:1901.09036 [cs, econ, math, stat].
Funk, M. J., Westreich, D., Wiesen, C., Stürmer, T., Brookhart, M. A., and Davidian, M. Doubly Robust
Estimation of Causal Effects. American Journal of Epidemiology, 173(7):761–767, April 2011. ISSN 1476-
6256, 0002-9262. doi: 10.1093/aje/kwq439. URL https://academic.oup.com/aje/article-lookup/
doi/10.1093/aje/kwq439.
12Glynn, A.N.andQuinn, K.M. AnIntroductiontotheAugmentedInversePropensityWeightedEstimator.
PoliticalAnalysis,18(1):36–56,2010. ISSN1047-1987,1476-4989. doi: 10.1093/pan/mpp036. URLhttps:
//www.cambridge.org/core/product/identifier/S1047198700012304/type/journal_article.
Guo, Z., Li, X., Han, L., and Cai, T. Robust Inference for Federated Meta-Learning, January 2023. URL
http://arxiv.org/abs/2301.00718. arXiv:2301.00718 [stat].
Han, L., Hou, J., Cho, K., Duan, R., and Cai, T. Federated Adaptive Causal Estimation (FACE) of Target
TreatmentEffects,April2022. URLhttp://arxiv.org/abs/2112.09313. arXiv:2112.09313[math,stat].
Han, L., Li, Y., Niknam, B. A., and Zubizarreta, J. R. Privacy-Preserving, Communication-Efficient, and
Target-Flexible Hospital Quality Measurement, February 2023a. URL http://arxiv.org/abs/2203.
00768. arXiv:2203.00768 [stat].
Han,L.,Shen,Z.,andZubizarreta,J. MultiplyRobustFederatedEstimationofTargetedAverageTreatment
Effects, September 2023b. URL http://arxiv.org/abs/2309.12600. arXiv:2309.12600 [cs, math, stat].
Hedges, L. V. and Vevea, J. L. Fixed-and random-effects models in meta-analysis. Psychological methods, 3
(4):486, 1998. Publisher: American Psychological Association.
Higgins, J. P. T., Thompson, S. G., and Spiegelhalter, D. J. A Re-Evaluation of Random-Effects Meta-
Analysis. Journal of the Royal Statistical Society Series A: Statistics in Society, 172(1):137–159, January
2009. ISSN 0964-1998, 1467-985X. doi: 10.1111/j.1467-985X.2008.00552.x. URL https://academic.
oup.com/jrsssa/article/172/1/137/7084465.
Hu, M., Shi, X., and Song, P. X.-K. Collaborative causal inference with a distributed data-sharing manage-
ment, April 2022. URL http://arxiv.org/abs/2204.00857. arXiv:2204.00857 [stat].
Huang, J., Gretton, A., Borgwardt, K., Schölkopf, B., and Smola, A. Correcting Sam-
ple Selection Bias by Unlabeled Data. In Advances in Neural Information Processing Sys-
tems, volume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/paper/2006/hash/
a2186aa7c086b46ad4e8bf81e2a3a19b-Abstract.html.
Härdle, W., Müller, M., Sperlich, S., Werwatz, A., and others. Nonparametric and semiparametric models,
volume 1. Springer, 2004.
Imbens, G. W. and Rubin, D. B. Causal Inference for Statistics, Social, and Biomedical Sciences: An
Introduction. Cambridge University Press, 2015.
Jin, Y., Guo, K., and Rothenhäusler, D. Diagnosing the role of observable distribution shift in scientific
replications, September 2023. URL http://arxiv.org/abs/2309.01056. arXiv:2309.01056 [stat].
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. SCAFFOLD: Stochastic
Controlled Averaging for Federated Learning. In Proceedings of the 37th International Conference on
Machine Learning, pp. 5132–5143. PMLR, November 2020. URL https://proceedings.mlr.press/
v119/karimireddy20a.html. ISSN: 2640-3498.
Koesters, M., Guaiana, G., Cipriani, A., Becker, T., and Barbui, C. Agomelatine efficacy and acceptability
revisited: systematic review and meta-analysis of published and unpublished randomised trials. British
Journal of Psychiatry,203(3):179–187,September2013. ISSN0007-1250,1472-1465. doi: 10.1192/bjp.bp.
112.120196.URLhttps://www.cambridge.org/core/product/identifier/S0007125000052533/type/
journal_article.
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the Convergence of FedAvg on Non-IID Data,
June 2020. URL http://arxiv.org/abs/1907.02189. arXiv:1907.02189 [cs, math, stat].
Lin,Z.,Ding,P.,andHan,F. Estimationbasedonnearestneighbormatching: fromdensityratiotoaverage
treatment effect, December 2021. URL http://arxiv.org/abs/2112.13506. arXiv:2112.13506 [econ,
math, stat].
13Little, R. J. and Rubin, D. B. Statistical analysis with missing data, volume 793. John Wiley & Sons, 2019.
Pennycook,G.,McPhetres,J.,Zhang,Y.,Lu,J.G.,andRand,D.G. FightingCOVID-19Misinformationon
Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention. Psychological Science,
31(7):770–780, July 2020. ISSN 0956-7976. doi: 10.1177/0956797620939054. URL https://doi.org/10.
1177/0956797620939054. Publisher: SAGE Publications Inc.
Raschka, S. Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning, November
2020. URL http://arxiv.org/abs/1811.12808. arXiv:1811.12808 [cs, stat].
Riley, R. D., Higgins, J. P. T., and Deeks, J. J. Interpretation of random effects meta-analyses. BMJ,
342:d549, February 2011. ISSN 0959-8138, 1468-5833. doi: 10.1136/bmj.d549. URL https://www.bmj.
com/content/342/bmj.d549. Publisher: British Medical Journal Publishing Group Section: Research
Methods &amp; Reporting.
Roozenbeek, J., Freeman, A. L. J., and Linden, S. v. d. How Accurate Are Accuracy-Nudge Interventions?
A Preregistered Direct Replication of Pennycook et al. (2020). Psychological Science, 32(7):1169–1178,
2021. doi: 10.1177/09567976211024535. URL https://doi.org/10.1177/09567976211024535. _eprint:
https://doi.org/10.1177/09567976211024535.
Rothwell, P. M. External validity of randomised controlled trials: “To whom do the results of this trial
apply?”. The Lancet, 365(9453):82–93, January 2005. ISSN 01406736. doi: 10.1016/S0140-6736(04)
17670-8. URL https://linkinghub.elsevier.com/retrieve/pii/S0140673604176708.
Stroup,D.F. Meta-analysisofObservationalStudiesinEpidemiologyAProposalforReporting. JAMA,283
(15):2008,April2000. ISSN0098-7484. doi: 10.1001/jama.283.15.2008. URLhttp://jama.jamanetwork.
com/article.aspx?doi=10.1001/jama.283.15.2008.
Sugiyama, M., Krauledat, M., and Müller, K.-R. Covariate shift adaptation by importance weighted cross
validation. Journal of Machine Learning Research, 8(5), 2007a.
Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P., and Kawanabe, M. Direct Importance Estimation
withModelSelectionandItsApplicationtoCovariateShiftAdaptation.InAdvancesinNeuralInformation
Processing Systems, volume 20. Curran Associates, Inc., 2007b. URL https://proceedings.neurips.
cc/paper_files/paper/2007/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html.
Tibshirani, R. Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society:
Series B (Methodological), 58(1):267–288, January 1996. ISSN 0035-9246, 2517-6161. doi: 10.1111/j.
2517-6161.1996.tb02080.x. URL https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.
1996.tb02080.x.
Tufanaru, C., Munn, Z., Stephenson, M., and Aromataris, E. Fixed or random effects meta-analysis?
Commonmethodologicalissuesinsystematicreviewsofeffectiveness. JBIEvidenceImplementation,13(3):
196, September 2015. ISSN 2691-3321. doi: 10.1097/XEB.0000000000000065. URL https://journals.
lww.com/ijebh/fulltext/2015/09000/fixed_or_random_effects_meta_analysis__common.12.aspx.
van Houwelingen, H. C., Arends, L. R., and Stijnen, T. Advanced methods in meta-analysis: multivari-
ate approach and meta-regression. Statistics in Medicine, 21(4):589–624, 2002. ISSN 1097-0258. doi:
10.1002/sim.1040. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1040. _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1040.
Vershynin, R. High-dimensional probability: An introduction with applications in data science, volume 47.
Cambridge university press, 2018.
Vo,T.V.,Bhattacharyya,A.,Lee,Y.,andLeong,T.-Y. Anadaptivekernelapproachtofederatedlearningof
heterogeneous causal effects. Advances in Neural Information Processing Systems, 35:24459–24473, 2022.
Vo, T. V., lee, Y., and Leong, T.-Y. Federated Learning of Causal Effects from Incomplete Observational
Data, August 2023. URL http://arxiv.org/abs/2308.13047. arXiv:2308.13047 [cs, stat].
14Wager,S.andAthey,S.EstimationandInferenceofHeterogeneousTreatmentEffectsusingRandomForests,
July 2017. URL http://arxiv.org/abs/1510.04342. arXiv:1510.04342 [math, stat].
Xiong, R., Koenecke, A., Powell, M., Shen, Z., Vogelstein, J. T., and Athey, S. Federated Causal Infer-
ence in Heterogeneous Observational Data, December 2022. URL http://arxiv.org/abs/2107.11732.
arXiv:2107.11732 [cs, econ, q-bio, stat].
Yang,S.andDing,P.CombiningMultipleObservationalDataSourcestoEstimateCausalEffects.Journalof
the American Statistical Association, 115(531):1540–1554, 2020. ISSN 0162-1459. doi: 10.1080/01621459.
2019.1609973. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7571608/.
Ye, T., Shao, J., and Kang, H. Debiased inverse-variance weighted estimator in two-
sample summary-data Mendelian randomization. The Annals of Statistics, 49(4):
2079–2100, August 2021. ISSN 0090-5364, 2168-8966. doi: 10.1214/20-AOS2027.
URL https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-4/
Debiased-inverse-variance-weighted-estimator-in-two-sample-summary-data/10.1214/
20-AOS2027.full. Publisher: Institute of Mathematical Statistics.
Zhang, S., Li, X., Zong, M., Zhu, X., and Wang, R. Efficient kNN Classification With Different Numbers of
Nearest Neighbors. IEEE Transactions on Neural Networks and Learning Systems, 29(5):1774–1785, May
2018. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2017.2673241. URL http://ieeexplore.ieee.
org/document/7898482/.
Zhao, Q. and Percival, D. Entropy balancing is doubly robust. Journal of Causal Inference, 5(1):20160010,
September 2017. ISSN 2193-3685, 2193-3677. doi: 10.1515/jci-2016-0010. URL http://arxiv.org/abs/
1501.03571. arXiv:1501.03571 [stat].
15A Proofs
A.1 Preliminaries
Definition A.1. Given i.i.d. weights w and outcomes Y , take their weighted sum Gˆ =(cid:80)n w Y . We call
i i i=1 i i
an estimator is “Hájek" type if it uses
((cid:80)n
w
)−1
to normalize, and “Horvitz-Thompson" (HT) type if it
i=1 i
uses (nE[w])−1, i.e.,
1 1
µˆ = Gˆ µ = Gˆ.
Hájek (cid:80)n w HT nE[w]
i=1 i
WebeginwithrelatingtheasymptoticbehaviourofHájek-typeipwestimatorwiththeHT-type. Inspecific,
we have that
Lemma A.2. The “Hajek"-type weighted mean estimator is asymptotically equivalent to the centralized
“Horvitz-Thompson"-type weighted mean estimator
n
1 (cid:88)
µˆ =µ+ w (Y −µ), (26)
HT nE[w] i i
i=1
i.e., we have that
√
n(µˆ −µˆ )=o (1).
Hájek HT P
Proof. We subtract µ from µˆ and get that
Hájek
√
√ n (cid:88)n
n(µˆ −µ)= w (Y −µ)
Hájek (cid:80)n
w
i i
i=1 i i=1
n
1 1 (cid:88)
= √ w (Y −µ)
(cid:80)n w /n n i i
i=1 i i=1
n
1 1 (cid:88)
= √ w (Y −µ)+o (1)
E[w] n i i P
i=1
√
= n(µˆ −µ)+o (1).
HT P
√
Thesecondtothethirdlineisbycombiningthefactthat(cid:80)n w /n=E[w]+o (1)and(cid:80)n w (Y −µ)/ n=
i=1 i P i=1 i i
O (1), through law of large numbers and CLT.
P
A.2 Proof of Proposition 1
We first define several useful intermediate values. We use Gˆ to denote un-normalized ipw summations and
Nˆ to denote the estimated data sizes.
Gˆ(k) = (cid:88) Z iY i − (1−Z i)Y i, Gˆ(k) = (cid:88) Z iY i , and Gˆ(k) = (cid:88) (1−Z i)Y i.
Meta e(k,1)(X) e(k,0)(X) Meta,1 e(k,1)(X) Meta,0 e(k,0)(X)
i∈D(k) i∈D(k) i∈D(k)
In the main paper, we use that
1 1
µˆ(k) = Gˆ(k) µˆ(k) = Gˆ(k) .
Meta,1 Nˆ(k) Meta,1 Meta,0 Nˆ(k) Meta,0
Clb,1 Clb,0
Proof. We first re-write Gˆ(k) as
Meta
N
Gˆ(k)
=(cid:88)1{S
i
=(k,1)}Y
i −
1{S =(k,0)}Y
i.
Meta e(k,1)(X) e(k,0)(X)
i=1
16Use Lemma A.2, we only need to consider
1 (cid:110)1{S =(k,1)}(Y −µ ) 1{S =(k,0)}(Y −µ )(cid:111)
τˆ(k) = i i 1 − i 0 +τ
Meta−HT N e(k,1)(X) e(k,0)(X)
Note that
(cid:110)1{S =(k,1)}(Y −µ ) 1{S =(k,0)}(Y −µ )(cid:111)
E 1 − 0
e(k,1)(X) e(k,0)(X)
(cid:104) (cid:104)P{S =(k,1)|X}(Y −µ ) P{S =(k,0)|X}(Y −µ ) (cid:105)(cid:105)
= E E 1 1 − 0 0 |X
e(k,1)(X) e(k,0)(X)
= E[Y −µ −(Y −µ )]=0.
1 1 0 0
We also have that
(cid:110)1{S =(k,1)}(Y −µ ) 1{S =(k,0)}(Y −µ )(cid:111)
Var 1 − 0
e(k,1)(X) e(k,0)(X)
(cid:104)(cid:104)1{S =(k,1)}(Y −µ ) 1{S =(k,0)}(Y −µ )(cid:105)2(cid:105)
= E 1 1 − 0 0
e(k,1)(X) e(k,0)(X)
(cid:104)1{S =(k,1)}(Y −µ )2 1{S =(k,0)}(Y −µ )2(cid:105)
= E 1 1 + 0 0
e(k,1)(X)2 e(k,0)(X)2
(cid:104)1{S =(k,1)}(Y −µ )2 1{S =(k,0)}(Y −µ )2(cid:105)
= E 1 1 + 0 0
e(k,1)(X)2 e(k,0)(X)2
(cid:104)(Y −µ )2 (Y −µ )2(cid:105)
= E 1 1 + 0 0 .
e(k,1)(X) e(k,0)(X)
Therefore, using CLT, we get that
√
N(τˆ(k)−τ)→d N(0,(v(k) )2 ), (27)
Meta
with
(v(k) )2 = 1 E(cid:104)(Y 1−µ 1)2 + (Y 0−µ 0)2(cid:105) . (28)
Meta N e(k,1)(X) e(k,0)(X)
Therefore, we have that
√
N(τˆ
−τ)=(cid:88)K (cid:104) η(k)√
N(τˆ(k)
−τ)(cid:105)
→d
N(cid:16) 0,(cid:88)K (η(k))2 E(cid:104)(Y 1−µ 1)2
+
(Y 0−µ 0)2(cid:105)(cid:17)
,
Meta Meta N e(k,1)(X) e(k,0)(X)
k=1 k=1
with
(cid:88)K (η(k))2 (v(k) )2
v2 = Meta
Meta N
k=1
1
≥ ,
(cid:104) (cid:105)−1
N(cid:80)K E (Y1−µ1)2 + (Y0−µ0)2
k=1 e(k,1)(X) e(k,0)(X)
where the equality holds if and only if η(k) ∝(v(k) )−1 .
Meta
17A.3 Proof of Theorem 2
We first provide the entire formula for Clb-ipw estimator. We define
Gˆ(k) = (cid:88) Z iY i Gˆ(k) = (cid:88) (1−Z i)Y i
Clb,1 (cid:80)K e(r,1)(X ) Clb,0 (cid:80)K e(r,0)(X )
i∈D(k) r=1 i i∈D(k) r=1 i
Nˆ(k) = (cid:88) Z iY i Nˆ(k) = (cid:88) Z iY i .
Clb,1 (cid:80)K e(r,1)(X ) Clb,1 (cid:80)K e(r,1)(X )
i∈D(k) r=1 i i∈D(k) r=1 i
Then, we have that
(cid:80)K Gˆ(k) (cid:80)K Gˆ(k)
τˆClb = k=1 Clb,1 − k=1 Clb,0,
(cid:80)K Nˆ(k) (cid:80)K Nˆ(k)
k=1 Clb,1 k=1 Clb,0
where in the main paper, we use that
1 1
µˆClb,1 =
Nˆ(k)
Gˆ( Ck l) b,1, and µˆClb,0 =
Nˆ(k)
Gˆ( Ck l) b,0.
Clb,1 Clb,0
Proof. We rewrite the formula as
N N
Gˆ(k)
=(cid:88)1{S
i
=(k,1)}Y
i, and Gˆ(k)
=(cid:88)1{S
i
=(k,0)}Y
i. (29)
Clb,1 (cid:80)K e(r,1)(X ) Clb,0 (cid:80)K e(r,0)(X )
i=1 r=1 i i=1 r=1 i
As a result, we have that
K N K N
k(cid:88) =1Gˆ Clb,1 =(cid:88) i=1k(cid:88) =11 (cid:80){S
K
r=i 1= e(( rk ,1, )1 () X} iY )i =(cid:88)
i=1
P1 (Z{Z (S(S i)i) == 11 |} XY ii ),
K N K N
k(cid:88) =1Gˆ Clb,0 =(cid:88) i=1k(cid:88) =11 (cid:80){S
K
r=i 1= e(( rk ,1, )0 () X} iY )i =(cid:88)
i=1
P1 ({ ZZ (( SS )i) == 00 |} XY ii ).
Similarly, we get
N K N
Nˆ
Clb,1
=(cid:88) i=1k(cid:88) =1(cid:80)1{
K
r=S 1i e= (r( ,1k ), (1 X)}
i)
=(cid:88)
i=1
P(1 Z{ (Z
S
i( )S =i) 1= |1 X}
i)
N K N
Nˆ Clb,0 =(cid:88) i=1k(cid:88) =1(cid:80)1{
K
r=S 1i e= (r( ,0k ), (0 X)}
i)
=(cid:88)
i=1
P(1 Z{ (Z
S
i( )S =i) 0= |0 X} i).
As a result, Nˆ C− l1 b,1Gˆ Clb,1−Nˆ C− l1 b,0Gˆ Clb,0 takes the form of Hájek type ipw estimator. Therefore, we could
use Lemma A.2 and get the corresponding HT-type estimator. Since we have that
(cid:104) 1{Z(S)=1} (cid:105) (cid:104)P[Z(S)=1|X](cid:105)
E =E =1.
P(Z(S)=1|X) P(Z(S)=1|X)
Same result holds for the control group. The HT estimators are
N
(µˆClb,1,HT−τ)=
N1 (cid:88)(cid:104)1{Z P(( ZS
i
() S= )=1} 1(Y
|i
X− )µ 1)
−
1{Z P(( ZS
i
() S= )=0} 0(Y
|i
X− )µ 0)(cid:105)
i i i i
i=1
18Using central limit theorem, since we have that
(cid:104)1{Z(S)=1}(Y −µ ) 1{Z(S)=0}(Y −µ )(cid:105)
E 1 − 0
P(Z(S)=1|X) P(Z(S)=0|X)
(cid:104)P(Z(S)=1|X)E[Y −µ |X] P(Z(S)=0|X)E[Y −µ |X](cid:105)
= E 1 1 − 0 0
P(Z(S)=1|X) P(Z(S)=0|X)
(cid:104) (cid:105)
= E E[Y −µ −Y +µ |X]
1 1 0 0
= 0.
and
(cid:104)1{Z(S)=1}(Y −µ ) 1{Z(S)=0}(Y −µ )(cid:105)
Var 1 − 0
P(Z(S)=1|X) P(Z(S)=0|X)
(cid:16)(cid:104)1{Z(S)=1}(Y −µ ) 1{Z(S)=0}(Y −µ )(cid:105)2(cid:17)
= E 1 − 0
P(Z(S)=1|X) P(Z(S)=0|X)
(cid:16)P(Z(S)=1|X)E[(Y −µ )2 |X] P(Z(S)=0|X)E[(Y −µ )2 |X](cid:17)
= E 1 1 + 0 0
P(Z(S)=1|X)2 P(Z(S)=0|X)2
(cid:16) (Y −µ )2 (Y −µ )2 (cid:17)
= E 1 1 + 0 0 .
P(Z(S)=1|X) P(Z(S)=0|X)
Use that N /N →P(S ̸=∅). We get that
S
(cid:112) N S(cid:16) τˆClb−τ(cid:17) →d N(0,v C2 lb), (30)
with
(cid:16) (Y −µ )2 (Y −µ )2 (cid:17)
v2 =P(S ̸=∅)E 1 1 + 0 0 . (31)
Clb P(Z(S)=1|X) P(Z(S)=0|X)
To compare v2 and v2 , we first prove Lemma A.3.
Clb Meta
Lemma A.3. The function f(t ,...,t
)=(t−1+...+t−1)−1
with t >0, i=1,...,K is concave.
1 K 1 K i
Proof. We directly prove it by showing that its hessian matrix is negative semi-definite. Denoting ∇2f =
{H } , we have that
kj 1≤k,j≤K

2t−4 2t−3
H
kj
=
((cid:80)K
r=1k
t− r1) 23
t−−
2t−( 2(cid:80)K
r=1k
t− r1)2
if k =j
(32)

((cid:80)K
r=k 1tj
− r1)3
if k ̸=j.
By taking out the common factor we get that
1 2(cid:16)(cid:88) rK =1t− r1(cid:17)3 ∇2f(t 1,...,t K)=  t t− 1 −. . .2 2  (cid:0) t− 12 ... t− K2(cid:1) −((cid:88) rK =1t−
r1)
  
t− 13
t− 23 ...

   . (33)
K t−3
K
Thesecondtermisnegativedefinite. Thefirsttermonlygetsonenon-zeroeigenvalue,withthecorresponding
eigenvector v =(t−2,...,t−2). We only need to verify that v⊤∇2fv ≤0. We have that
1 K
3 2
K K K K
1(cid:16)(cid:88) (cid:17) (cid:16)(cid:88) (cid:17) (cid:88) (cid:88)
t−1 v⊤∇2f(t ,...,t )v⊤ = t−4 −( t−1)( t−7)
2 r 1 K k k k
r=1 k=1 k=1 k=1
(cid:88) (cid:88)(cid:16) (cid:17)
=2 t−4t−4− t−1t−7+t−7t−1 ≤0,
k j k j k j
k<j k<j
19where the last line is by using the AM-GM inequality and getting that t−1t−7 +t−1t−7 ≥ 2t−4t−4. This
k j j k k j
shows that ∇2f is negative semi-definite, which means that f is concave.
We use Jensen inequality and get that
2
v2 =
Meta (cid:110) (cid:104) (cid:105) (cid:104) (cid:105)(cid:111)−1
N(cid:80)K E (Y1−µ1)2 +E (Y0−µ0)2
k=1 2e(k,1)(X) 2e(k,0)(X)
1 1
≥ +
(cid:110) (cid:104) (cid:105)(cid:111)−1 (cid:110) (cid:104) (cid:105)(cid:111)−1
N(cid:80)K E (Y1−µ1)2 N(cid:80)K E (Y0−µ0)2
k=1 e(k,1)(X) k=1 e(k,0)(X)
(cid:104) 1 (cid:105) (cid:104) 1 (cid:105)
≥E +E
(cid:110) (cid:111)−1 (cid:110) (cid:111)−1
N(cid:80)K (Y1−µ1)2 N(cid:80)K (Y0−µ0)2
k=1 e(k,1)(X) k=1 e(k,0)(X)
(cid:104) (Y −µ )2 (cid:105) (cid:104) (Y −µ )2 (cid:105)
=E 1 1 +E 0 0
N(cid:80)K e(k,1)(X) N(cid:80)K e(k,0)(X)
k=1 k=1
=v2 ,
Clb
where we use Jensen twice at the second and the third lines.
A.4 Proof of Theorem 3
Proof. The proof relies on the definition of independence and Assumption 1. Using p to denote the joint
y,s
density function for (Y(1),Y(0)) and S, and p , P as their marginal distributions, we have that
y s
p {(Y(1),Y(0)),S |X}=p {(Y(1),Y(0))|X}P {S |X}.
y,s y s
Take expectation conditional on P(S =(k,z)|X) = e(k,z)(X), and use the tower property of cognitional
expectation, we get that, for the L.H.S.,
(cid:104) (cid:12) (cid:105) (cid:110) (cid:111)
E p {(Y(1),Y(0)),S |X} (cid:12) e(k,z)(X) =p (Y(1),Y(0)),S |e(k,z)(X) ;
y,s (cid:12) y,s
for the R.H.S.,
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E p {(Y(1),Y(0))|X}P {S |X} (cid:12) e(k,z)(X) =E p {(Y(1),Y(0))|X} (cid:12) e(k,z)(X) e(k,z)(X)
y s (cid:12) y (cid:12)
(cid:110) (cid:111)
=p (Y(1),Y(0))|e(k,z)(X) e(k,z)(X)
y
(cid:110) (cid:111) (cid:110) (cid:111)
=p (Y(1),Y(0))|e(k,z)(X) P S |e(k,z)(X) .
y s
This shows that
(Y(1),Y(0))⊥⊥S |e(k,z)(X).
For the second part, similarly, using tower property, we have that
(cid:104) (cid:12) (cid:105) (cid:110) (cid:111)
E p {(Y(1),Y(0)),Z(S)|X} (cid:12) P[Z(S)|X] =p (Y(1),Y(0)),S |P[Z(S)|X] ;
y,z (cid:12) y,s
for the R.H.S.,
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E p {(Y(1),Y(0))|X}P {Z(S)|X} (cid:12) P {Z(S)|X} =E p {(Y(1),Y(0))|X} (cid:12) e(k,z)(X) e(k,z)(X)
y z (cid:12) z y (cid:12)
(cid:110) (cid:111)
=p (Y(1),Y(0))|P {Z(S)|X} P {Z(S)|X}
y z z
(cid:110) (cid:111) (cid:110) (cid:111)
=p (Y(1),Y(0))|P {Z(S)|X} P Z(S)|P {Z(S)|X} .
y z z
This shows that
(Y(1),Y(0))⊥⊥Z(S)|P{Z(S)|X}.
20A.5 Proof of Proposition 4 and 5
We only provide the proof for Proposition 4. For the proof of Proposition 5, see Zhao & Percival (2017) and
Lin et al. (2021).
Proof. Suppose that another distribution (S′,X′,Y′,Y′) generates the same observed distribution p(x),
1 0
p(x|S =(k,1)), and p(x|S =(k,0)) for all k. Using Bayes’ theorem,
p(x′ |S′ =(k,1))P(S′ =(k,1))
P(S′ =(k,1)|X′)=
p(x′)
p(x|S =(k,1))P(S′ =(k,1))
=
p(x)
P(S′ =(k,1))
=P(S =(k,1)|X) .
P(S =(k,1))
This shows that e(k,z)(X) is identifiable up to a constant, but P(S =∅) is not identifiable.
A.6 Proof of Theorem 6
We first give the formulas for µˆ(k):
0
δˆ(k) = 1 (cid:88) (Z i)[Y i−m 1(X i)] , δˆ(k) = 1 (cid:88) (1−Z i)[Y i−m 1(X i)] .
Meta−aipw,0 Nˆ M(k e)
ta,0 i∈D(k)
e(k,1)(X i) Clb−aipw,1 Nˆ C(k l)
b,1 i∈D(k)
(cid:80)K r=1e(r,1)(X i)
Proof. Consider the following estimator using the true outcome and propensity score models:
K
τ˜Clb−aipw = N1
(t)
(cid:88) (cid:104) m 1(X i(t))−m 0(X i(t))(cid:105) +
Nˆ
C1
lb
(cid:88) Nˆ C(k l) bδ˜ C(k l) b, (34)
i∈D(t) k=1
with
δ˜(k) = 1 (cid:88) (cid:104)Z(S i)(Y i−m 1(X i)) − (1−Z(S i))(Y i−m 0(X i))(cid:105) , (35)
Clb−dr Nˆ(k) (cid:80)K e(r,1)(X ) (cid:80)K e(r,0)(X )
Clb i∈D(k) r=1 i r=1 i
where m , m , and e are true models. We first prove Lemma A.4.
1 0
Lemma A.4. We have that
(cid:112) d
N S(τˆClb−aipw−τ˜Clb−aipw)→0. (36)
Proof. Similar to the proof of Theorem 2, using Lemma A.2, we have that
√ √
d d
N(τˆClb−aipw−τˆClb−aipw−HT)→0 and N(τ˜Clb−aipw−τ˜Clb−aipw−HT)→0, (37)
with
τˆClb−aipw−HT
N
=
1 (cid:88) (cid:104)
mˆ (X)−mˆ
(X)(cid:105)
+
1 (cid:88)(cid:104)Z(S i)(Y i−mˆ 1(X i))
−
(1−Z(S i))(Y i−mˆ 0(X i))(cid:105)
,
N(t) 1 0 N (cid:80)K eˆ(r,1)(X ) (cid:80)K eˆ(r,0)(X )
i∈D(t) i=1 r=1 i r=1 i
and
τ˜Clb−aipw−HT
N
=
1 (cid:88) (cid:104)
m (X)−m
(X)(cid:105)
+
1 (cid:88)(cid:104)Z(S i)(Y i−m 1(X))
−
(1−Z(S i))(Y i−m 0(X))(cid:105)
.
N(t) 1 0 N P(Z(S )=1|X ) P(Z(S )=0|X )
i i i i
i∈D(t) i=1
21We decompose τˆClb−aipw−HT−τ˜Clb−aipw−HT
τˆClb−aipw−HT−τ˜Clb−aipw−HT
N
=
1 (cid:88)(cid:104) Z(S i)
−
Z(S i) (cid:105)
(Y −mˆ (X ))
N
i=1
P(Z(S i)=1|X i) (cid:80)K r=1eˆ(r,1)(X i) i 1 i
N
−
1 (cid:88)(cid:104) 1−Z(S i)
−
1−Z(S i) (cid:105)
(Y −mˆ (X ))
N
i=1
P(Z(S i)=0|X i) (cid:80)K r=1eˆ(r,0)(X i) i 0 i
N
+
1 (cid:88)(cid:110)(cid:104) Z(S i) (cid:105)(cid:104)
m (X )−mˆ (X
)(cid:105) −E(cid:104)
m (X )−mˆ (X
)(cid:105)(cid:111)
N P(Z(S )=1|X ) 1 i 1 i 1 i 1 i
i i
i=1
N
−
1 (cid:88)(cid:110)(cid:104) 1−Z(S i) (cid:105)(cid:104)
m (X )−mˆ (X
)(cid:105) −E(cid:104)
m (X )−mˆ (X
)(cid:105)(cid:111)
N P(Z(S )=0|X ) 0 i 0 i 0 i 0 i
i i
i=1
+ 1 (cid:88) (cid:110)(cid:104) mˆ (X(t))−m (X(t))(cid:105) −E(cid:104) mˆ (X)−m (X)(cid:105)(cid:111)
N(t) 1 i 1 i 1 1
i∈D(t)
+ 1 (cid:88) (cid:110)(cid:104) mˆ (X(t))−m (X(t))(cid:105) −E(cid:104) mˆ (X)−m (X)(cid:105)(cid:111) .
N(t) 0 i 0 i 0 0
i∈D(t)
Denote the above terms as ∆ ,...,∆ . We bound each of them.
1 6
(cid:118)
(cid:117) (cid:104) (cid:105)2(cid:118)
(cid:117) N Z(S )2 P(Z(S )=1|X )−(cid:80)K eˆ(r,1)(X ) (cid:117) N
(cid:117) 1 (cid:88) i i i r=1 i (cid:117) 1 (cid:88)(cid:104) (cid:105)2
|∆ 1|≤ (cid:116)
N P(Z(S )=1|X )2[(cid:80)K eˆ(r,1)(X )]2
(cid:116)
N
Y i(1)−m 1(X i)
i=1 i i r=1 i i=1
(cid:118) (cid:118)
(cid:117) (cid:117)c−2 (cid:88)N (cid:104) (cid:88)K (cid:105)2(cid:117) (cid:117) 1 (cid:88)N (cid:104) (cid:105)2
≤ (cid:116) P(Z(S )=1|X )− eˆ(r,1)(X ) (cid:116) Y (1)−m (X ) ,
N i i i N i 1 i
i=1 r=1 i=1
By taking expectation and applying Jensen inequality, we get that
(cid:118)
√ √ (cid:117) (cid:117) (cid:104) (cid:88)K (cid:105)2(cid:114) (cid:110)(cid:104) (cid:105)2(cid:111)
E[ N|∆ |]≤ N(cid:116)c−2E P(Z(S)=1|X)− e(r,1)(X) E Y(1)−m (X)
1 1
r=1
≤ N1/2−ξm−ξe →0,
√ √
P P
as N →∞. This shows that N|∆ |→0. We could prove that N∆ →0 with the same manner. Using
1 2
the Bernstein Inequality for bounded random variables (Vershynin, 2018), we have that
(cid:110)√ (cid:111) (cid:110)√ (cid:12)(cid:88)N (cid:104) (cid:105) (cid:104) (cid:105)(cid:12) (cid:111)
P N|∆ |≥t/c ≤ P N(cid:12) m (X )−mˆ (X ) −E m (X )−mˆ (X ) (cid:12)≥t
3 (cid:12) 1 i 1 i 1 i 1 i (cid:12)
i=1
(cid:16) t2/2 (cid:17)
≤ 2exp − √
(cid:80)N
Var[mˆ (X )−m (X )]/N +M t/(3 N)
i=1 1 i 1 i Y
(cid:16) t2/2 (cid:17)
≤ 2exp − √
E{[mˆ (X)−m (X)]2}+M t/(3 N)
1 1 Y
(cid:16) t2/2 (cid:17)
≤ 2exp − →0,
N−2ξm +N−1/2M Yt/3
√ √
P P
for any t > 0 and N → ∞. This proves that N∆ → 0. We could prove that N∆ → 0 with the same
3 4
22manner. At last, for ∆ , we have that
5
(cid:110)√ (cid:111) (cid:16) t2/2 (cid:17)
P N|∆ |≥t ≤ 2exp − √
3 (cid:80) Var[mˆ (X )−m (X )]/N(t)+M t/(3 N(t))
i∈D(t) 1 i 1 i Y
(cid:16) t2/2 (cid:17)
≤ 2exp − √
E{[mˆ (X)−m (X)]2}+M t/(3 N(t))
1 1 Y
(cid:16) t2/2 (cid:17)
≤ 2exp − →0,
(N(t))−2ξm +(N(t))−1/2
M t/3
Y
√ √
P P
for any t > 0 and N → ∞. This proves that N∆ → 0. We could prove that N∆ → 0 with the same
5 6
manner. Combining ∆ ,...,∆ with Equation (37) together, we finish the proof of Lemma A.4.
1 6
By Lemma A.4, we only need to consider τ˜Clb−aipw. Using CLT, we have that
√
N(τ˜Clb−aipw−τ)→d N(0,v C2 lb−aipw), (38)
since
(cid:104) (cid:105) (cid:104)Z(S)(Y −m (X)) (1−Z(S))(Y −m (X))(cid:105)
E(τ˜Clb−aipw)= E m 1(X(t))−m 0(X(t)) +E P(Z(S1 )=11
|X)
− P(Z(S)=0 0|X0
)
= E[Y −Y ],
1 0
and with
(cid:104)√ (cid:105)
v C2
lb−aipw
= Var N(τ˜Clb−aipw−τ)
N (cid:104) (cid:105)
= Var m (X(t))−m (X(t))
N(t) 1 0
N (cid:104)Z(S)(Y −m (X)) (1−Z(S))(Y −m (X))(cid:105)
+ Var 1 1 − 0 0
N P(Z(S)=1|X) P(Z(S)=0|X)
(cid:104) (cid:105) (cid:104) (Y −m (X))2 (Y −m (X))2 (cid:105)
= λ−1E [m (X)−m (X)]2 −λ−1τ2+E 1 1 + 0 0 .
1 0 P(Z(S)=1|X) P(Z(S)=0|X)
This proves Theorem 6.
A.7 Proof of Theorem 7
It is a direct result from Appendix B.2 in Foster & Syrgkanis (2020).
23B Experiments
B.1 Extra Details
Fortheincorrectscenario,usingsubscriptitodenotedifferentdimensionsofX,weletX′ =X X ,X′ =X2,
1 1 2 2 2
and X′ =X /max{1,X′}. Using X′ as the regressors for misspecified propensity and outcome models.
3 3 1
B.2 Ablations
We provide the KL−MSE plots with misspecified models in Figure 3. All experiment settings are the same
with Figure 1b, but we perturb the models. We construct false models also with X′. The results show the
sametrendwithFigure1b. ItisworthnotingthatinFigure3a,theaipwestimatorhassimilarvariancewith
Meta-ipw when KL distance is large. We attribute this result to numerical instability, as we find there are
occasionally divergent learned parameters due to extreme heterogeneity. The Clb-ipw estimator maintains
low MSE against heterogeneity.
(a) True PS; False OM
3.5
3.0
2.5
2.0 1.5
1.0
0.5
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Heterogeneity
rorrE
deruaqS
naeM
(b) False PS; True OM
Meta IPW 4.0
Meta AIPW 3.5
Clb IPW 3.0 Clb AIPW
IPW in sites 2.5
2.0 1.5
1.0
0.5
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Heterogeneity
rorrE
deruaqS
naeM
(c) False PS; False OM
4.00
3.75
Meta IPW 3.50
Meta AIPW 3.25
Clb IPW 3.00 Clb AIPW IPW in sites 2.75
2.50
2.25
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Heterogeneity
rorrE
deruaqS
naeM Meta IPW Meta AIPW
Clb IPW
Clb AIPW
IPW in sites
Figure3: Themeansquarederrorchangingwithheterogeneity. WeuseX′ forallmisspecifiedmodels. When
both models fail to fit the data, there’s no theoretical guarantee and all estimators have huge mean squared
error. The better performance of Meta-ipw there is meaningless.
24