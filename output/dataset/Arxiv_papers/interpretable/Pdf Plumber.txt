INTERPRETABLE CLUSTERING WITH THE DISTINGUISHABILITY
CRITERION
APREPRINT
AliTurfah XiaoquanWen∗
DepartmentofBiostatistics DepartmentofBiostatistics
UniversityofMichigan UniversityofMichigan
AnnArbor,MI,48105 AnnArbor,MI,48105
aturfah@umich.edu xwen@umich.edu
April24,2024
ABSTRACT
Clusteranalysisisapopularunsupervisedlearningtoolusedinmanydisciplinestoidentifyheteroge-
neoussub-populationswithinasample. However,validatingclusteranalysisresultsanddetermining
thenumberofclustersinadatasetremainsanoutstandingproblem. Inthiswork,wepresentaglobal
criterioncalledtheDistinguishabilitycriteriontoquantifytheseparabilityofidentifiedclustersand
validateinferredclusterconfigurations. OurcomputationalimplementationoftheDistinguishability
criterioncorrespondstotheBayesriskofarandomizedclassifierunderthe0-1loss. Weproposea
combinedlossfunction-basedcomputationalframeworkthatintegratestheDistinguishabilitycriterion
withmanycommonlyusedclusteringprocedures,suchashierarchicalclustering,k-means,andfinite
mixturemodels. Wepresentthesenewalgorithmsaswellastheresultsfromcomprehensivedata
analysisbasedonsimulationstudiesandrealdataapplications.
Keywords Unsupervisedlearning·Clusteranalysis·k-means·Hierarchicalclustering·Mixturemodels
1 Introduction
Clusteranalysisisaubiquitousunsupervisedlearningapproachtouncoverlatentstructuresandpatternsinobserved
data. Clusteringalgorithmshavebeenusedinawidevarietyofscientificapplications,suchasanimalbehaviorstudies
[1],weatheranomalydetection[2],diseasediagnosis[3,4,5],andnovelcelltypeidentification[6,7,8,9]. Often,the
identifiedclustersareinterpretedtorepresentdistinctpopulationsfromwhichthecorrespondingsamplesoriginate.
Manychallengeswithclusteranalysis,suchasdeterminingthenumberofclusters,arisefromaninabilitytorigorously
quantify desired cluster characteristics. While the precise definition of a “meaningful” cluster is usually context-
dependent,itisgenerallyacceptedthattheclustersshoulddisplay“internalcohesion”(i.e.,objectswithinaclusterare
similartooneanother)and“externalisolation”(i.e.,theclustersarewell-separated)[10,11,12]. Despitethisalmost
universallyagreed-uponprinciple,quantifyingtheseparabilityoftheclusters,i.e.,thelevelofexternalisolationwith
respecttointernalcohesion,remainsanopenprobleminclusteranalysis.
Inthispaper,weintroducetheDistinguishabilitycriteriontomeasuretheseparabilityofasetofassumedclusters. The
criterionismotivatedbythissimpleintuition: ifallclustersarewellseparatedfromeachother,thentheoriginating
clustersforalldatapoints(whetherobservedornot)shouldbeeasilytraceable. ToimplementtheDistinguishability
criterion,weformulatelabelingthegeneratingclusterforanarbitrarydatapointasaprobabilisticclassificationproblem.
Naturally,thedifficulty(orlackthereof)ofthisclassificationproblemcanbedescribedbyanoverallmisclassification
probabilityaveragedoverallpossibledatapoints.
WeemployastatisticalviewpointtodefinetheDistinguishabilitycriterionforclusteranalysis. Thepartitionedobserved
dataaretakentoberealizationsfromcluster-specificdatagenerativedistributions,whichareessentialforcomputingthe
∗CorrespondingAuthor
4202
rpA
52
]LM.tats[
2v76951.4042:viXraInterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
proposedmisclassificationprobability. Althoughnotallclusteringalgorithmsmakeexplicitdistributionalassumptions
forthepresumedclusters,manycommonlyappliedheuristics-basedalgorithmsachieveoptimalperformanceunder
specificprobabilisticgenerativemodels[13,14]. Furthermore,theidentifiedclusterstructuresfromclusteranalysisare
typicallyexpectedtobereplicatedinfuturedatasets—animplicitassumptionforconsistentdatagenerativedistributions.
Asaresult,statisticalinferenceproceduresbasedonexplicitdistributionalassumptionshavebecomemorepopularfor
theirabilitytonotonlyenhanceclusteringperformancebutalsotoexamineandinterpretclusterstructuresidentifiedby
bothmodelandheuristics-basedclusteringmethods[15,16,17,18].
Theremainderofthispaperisorganizedasfollows. Wefirstprovidethemathematicaldefinitionandpropertiesofthe
Distinguishabilitycriterion. Wethendiscussitsusagewithexistingclusteringalgorithms. Finally,weillustratethe
applicationsoftheDistinguishabilitycriterionusingbothsyntheticandrealdatafromvariousscientificapplications.
Our implementation of the Distinguishability criterion and the analyses presented in this paper can be found at
https://github.com/aturfah/distinguishability-criterion.
2 Results
2.1 TheDistinguishabilityCriterion
The proposed Distinguishability criterion measures the overall separability of a given cluster configuration and is
derivedbyquantifyingthemisclassificationprobabilityfromamulti-classclassificationproblem.
GivenaclusterconfigurationwhereeachoftheK clusterscorrespondstoadistinctclass,wedenotetheclasslabelfor
anobservationxbyθ ∈{1,2,...,K}. Weassumeapre-definedclassifier,δ(x):Rp →{1,...,K},andevaluatethe
classificationperformanceusingthe0-1lossfunction,i.e.,
L(δ(x),θ)=1{δ(x)̸=θ}.
Theoverallmisclassificationprobabilityundertheassumedclusterconfiguration,denotedbyP ,isdefinedasthe
mc
Bayesriskoftheclassifierδ(x),i.e.,
(cid:104) (cid:0) (cid:1)(cid:105)
P
mc
=Ex E
θ
L(δ(x),θ)|x (1)
Usingthe0-1lossensuresthattheresultingBayesriskisavalidprobabilitymeasurement,rangingfrom0to1. Itis
naturallyinterpretedastheprobabilityofmisclassifyingadatapointunderthegivenclusterconfiguration,marginalizing
allpotentialxvaluesandtheirrespectivetruegeneratingclusters. Forinstance,aP valuecloseto0signifiesahigh
mc
degreeofclusterseparation,indicatedbyaminimalprobabilityoferroneouslyassigninganarbitrarydatapointtoan
incorrectgeneratingcluster(SupplementaryFigure2).
As we are primarily interested in assessing different cluster configurations, the selection of the required classifier
is flexible. However, the choice of classifier can impact the computational efficiency of the P evaluation. Our
mc
implementationfocusesonthesetofclassifiersworkingdirectlywiththeprobabilities
π (x):=Pr(θ =k |x), fork =1,...,K.
k
Thissetincludestheoptimalclassifierunderthe0-1loss,δ ,i.e.,
o
δ (x)=argmaxπ (x)
o k
k
OurdefaultclassifierforcomputingP isarandomizeddecisionfunction,δ ,whichassignsalabeltoanobservation
mc r
xbysamplingfromacategoricaldistributionbasedontheprobabilitydistributionπ(x)=(π (x),...,π (x)),i.e.,
1 K
δ (x)∼Categorical(π(x)).
r
InadditiontoyieldinghighlycomparableP valuestotheoptimalclassifierwithinthedecision-criticalrangesof
mc
clusterseparation(SupplementaryFigure1),therandomizedclassifier’scomputationalpropertiesenablehighlyefficient
clusteranalysisprocedures.
Theπ ’sarethekeyquantitiesbridgingtheobservedclusteringdataandP . TheyarecalculatedusingBayesrule,
k mc
2InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
π (x) ∝ α (X )p(x|θ(X )=k).
k k c c
Thenotationemphasizesthatboththeprior,α (X ),andthelikelihoodfunction,p(x|θ(X )),aredirectlyinformedby
k c c
andestimatedfromtheclusteringdata. Morespecifically,thepriorquantifiestherelativefrequencyoftheobservations
arisingfromeachassumedcluster,whilethelikelihoodfunctionencodesthecharacteristicsofthecorrespondingcluster
population, such as its centroid and spread information. Computing π values is straightforward for model-based
k
clusteringalgorithmssuchasGaussianmixturemodels(GMMs)[14]. Fornon-model-basedclusteringalgorithms,
explicitdistributionalassumptionsspecifyingtheparametricfamilyoflikelihoodfunctionsarerequired. Weillustrate
theseproceduresfork-meansandhierarchicalclusteringalgorithmsinsubsequentsections.
Insummary,P isaprobabilitymeasurementofglobalseparabilityacrossinferredclusters. Itcanaccommodatea
mc
widerangeofdistributionalassumptions,makingitcompatiblewithadiversesetofclusteringproceduresanddata
modalities. Moreover,asafunctionofclusteringdata,X ,theestimateofP itselfisavalidlossfunctionsuitablefor
c mc
selectingoptimalclusterconfigurationsinclusteranalysis.
2.2 CombinedLossFunctionforClusterAnalysis
Inclusteranalysis,thedesiredclustercharacteristicsareoftendefinedbymultiplecriteria[19]. Asinglecriterionon
itsown,includingtheproposedDistinguishabilitycriterion,isinsufficienttodefineapracticallyoptimalclustering
solution. Alternatively,combiningmultiplelossfunctionstargetingdifferentdesiredclusterpropertiescanresultin
morebalancedandholisticclusteringsolutions. ThisobservationleadstoaprincipledwaytoincorporateP with
mc
otherestablishedclusteringcriteriaandalgorithms.
Specifically, let L denote a loss function associated with existing clustering algorithms. Formally, we consider a
1
compoundloss,L,asaweightedlinearcombinationofL andP ,i.e.,
1 mc
L=L +λP , λ>0. (2)
1 mc
BecauseofthescaleofP ,itisoftenconvenienttosolvethefollowingequivalentconstrainedoptimizationproblem,
mc
MinimizeL ,subjecttoP ≤τ, (3)
1 mc
whereτ isapre-specifiedprobabilitythreshold. Itisworthnotingthatthestringencyoftheτ valuemaydependonthe
dimensionalityoftheclusteringdata.
ThechoiceofclusteringalgorithmdeterminesthefunctionalformofL . Forexample,thedistortionfunctionorWard’s
1
linkagearenaturalchoicesforL whenusingk-meansandhierarchicalclusteringmethods,respectively. Alternatively,
1
thenegativeofthegapstatistic[20]canalsobeanexcellentchoiceintheseapplicationscenarios. Formodel-based
clusteringalgorithms,theL functioncanbederivedfromvariousmodelselectioncriteria,e.g.,thenegativeofBayesian
1
informationcriterion(BIC).
2.3 ConnectionstoRelatedApproaches
ThemisclassificationprobabilitydefinedbyP fallsintothecategoryofinternalclusteringvalidityindices[21,22,23],
mc
whichassessthequalityofaclusteringsolutionwithoutadditionalexternalinformationbeyondtheobserveddata. This
categoryincludesmanycommonlyappliedstatisticalmeasures,suchastheSilhouetteindex[24],Calinski-Harabaze
index[25],Dunnindex[26],amongothers. Acommonbehaviorofinternalclusteringvalidityindicesisthatthey
evaluateboththecohesion(orcompactness)withinaclusteraswellastheseparationbetweenclusters. ForP ,the
mc
within-clustercohesionisquantifiedthroughtheestimatedparametersinthelikelihoodfunction,p(x | θ(x ) = k).
c
Theseparation,relativetothecohesion,isquantifiedbytheoverallmisclassificationprobability.
Henning[19]andMelnykov[27]alsocomputemisclassificationprobabilitiestoassesstheseparationbetweenclusters
inthecontextofmixturemodelsforclustering.Theyintroducethemetrics—named“directlyestimatedmisclassification
probability"(DEMP)andDEMP+—specificallydesignedtocomputemisclassificationprobabilitiesbetweenpairs
of clusters to inform decisions about the local merging of mixture components. In comparison, P is a global
mc
measureofthemisclassificationprobabilityacrossallclusters. It,too,canbeusedtocombinemixturecomponents
toforminterpretableclusters, asillustratedinSection2.4. Additionally, theentropycriterionproposedbyCeleux
and Soromenho [28, 29, 30] provides another alternative approach to quantify the separability of clusters using a
classificationproblemset-up.
3InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
TheDistinguishabilitycriterionalsoagreeswiththeprincipleofstabilitymeasurescommonlyemployedinclustering
analysis. Specifically,iftheunderlyingclusterdistributionsareallwell-separated,asindicatedbylowP values,
mc
datasampledrepeatedlyfromthesedistributionsareexpectedtoproduceconsistentclusteringoutcomes[31,32,33].
EmpiricalevidencehasshownthatP andvariousmeasuresofclusteringinstabilityarehighlycorrelated,whichis
mc
demonstratedinthesubsequentsections.
2.4 FiniteMixtureModelsIncorporatingP
mc
Finitemixturemodels(MM)areprobabilisticmodelsthatcanseamlesslyincorporatetheDistinguishabilitycriterion.
MM-basedclusteringalgorithmsprimarilyinferthedistributionalcharacteristicsunderlyingeachlatentcluster. Asa
result,noadditionalassumptionsareneededtocomputeP intheMMsetting—therequiredquantities,{α , p(x|
mc k
θ =k), π },arealldirectoutputsorby-productsfromstandardMMinferenceprocedures[14],e.g.,theEMalgorithm.
k
We make an important distinction between a mixture component and an interpretable cluster, a point previously
discussedby[27,29,30,34]. Weviewmixturemodelsasflexibledensityestimationdevices,wherethenumberof
mixturecomponentsischosentoadequatelyfittheobserveddata. Ontheotherhand,thedistributionofanunderlying
cluster—characterizedbytheDistinguishabilitycriterion—mayitselfbeamixturedistributioncomprisingmultiple
components. Thisdistinctionnaturallyleadstocombiningthelossfunctionsrepresentedby−BIC,whichevaluatesthe
goodness-of-fitofamixturedensity,andP ,whichcharacterizestheseparationbetweenpotentialclusters.
mc
Tooptimizethecombinedlossfunction,wefirstfindP(x),theoptimalmixturedistributionwithκcomponents,by
maximizingtheBIC.Subsequently,wemergethemixturecomponentsintoclustersuntilP fallsbelowapre-specified
mc
thresholdτ. Sincemergingmixturecomponentsintoclustersdoesnotalterthemixturecomponentdistributionsin
anyway,theBICisunchangedbythemergingprocess. Itcanbeshownthatmergingexistingclustersinthismanner
alwaysdecreasesP (AppendixB).Specifically,underthedefaultrandomizedclassifierδ ,thereductionofP by
mc r mc
combiningclustersiandj isgivenby
(cid:90)
∆P(i,j) =2 π (x)π (x)P(dx). (4)
mc i j
Forclusterswithlittletonooverlap,i.e.,π (x)π (x)→0forallxvalues,merging(i,j)resultsinminimalchangesin
i j
P . Ontheotherhand,forclusterswithsignificantoverlap,∆P(i,j)canbesubstantial.
mc mc
ByfurtherutilizingtheclustermergingpropertyofP (Proposition1,MethodsSection),i.e.,
mc
(cid:88)
P = ∆P(i,j), (5)
mc mc
i<j
weproposeanefficientP HierarchicalMerging(PHM)algorithmtosequentiallyamalgamatemixturecomponents
mc
intoclusters(Algorithm1). Briefly,startingbyassigningeachoftheκmixturecomponentstoindividualclusters,the
PHMalgorithmpre-computes∆P(i,j) forall(i,j)clusterpairs. Itthensequentiallycombinesthepairsofclusters
mc
withthelargest∆P(i,j)intoasingleclusterandupdatesthe∆P valuesfortheremainingclusters. Theprocessis
mc mc
repeateduntiltheupdatedP fallsatorbelowapre-definedτ value. Intuitively,thisprocedureprioritizesmergingthe
mc
mostsimilarorcloselyrelatedclustersateachstep,quantifiedbytheir∆P(i,j)value.
mc
By setting τ = 0, the algorithm runs until all mixture components have been merged into a single cluster. The
completemergingprocesscanbevisualizedusingadendrogram(AppendixC),characterizingthehierarchicalmerging
ordersbetweenindividualmixturecomponentsandmergedclusters. Inmanyscientificapplications,e.g.,geneticsand
single-celldataanalysis,suchadendrogramcanprovideasnapshotoftheunderlyingcontinuousdifferentiationprocess
thatformstheidentifiedclusters. Weprovidetwoexamplesinourrealdataapplications(Section2.7).
ToillustratethePHMalgorithmwithGaussianmixturemodels(GMMs),weusethesyntheticdatafromSection4.1in
Baudryetal. [30]. Specifically,600observationsaredrawnfromsixGaussiandistributionsarrangedalongthecorners
ofasquareinthefollowingmanner. TwooverlappingGaussiandistributionsareplacedinthetopleftcornerofthe
square,eachwith1/5ofthesamples. ThebottomleftandtoprightcornerseachhaveasingleGaussiandistribution,
eachcontributing1/5ofthesamples. Finally,twooverlappingGaussiandistributionsareplacedinthebottomright
corner,eachwith1/10ofthesamples.
AGMMwithsixcomponentsisselectedbyBICusingtheRpackagemclust[35]. Theobservedclusteringdatais
shownintheleftpanelofFigure1,withcolorscorrespondingtoanobservation’sassignmenttooneoftheκGMM
components. TheinitialclusterconfigurationlabelingeachmixturecomponentasasingleclusterhasP =0.139.
mc
4InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Figure1:(Left)600simulatedobservationsdrawnfromamixtureofsixtwo-dimensionalGaussiandistributions. Colors
indicatetheclusterassignmentlabelstoeachofthesixestimatedmixturecomponents. (Right)Heatmapvisualizing
∆P valuesfortheestimatedmixturecomponents. TheintensityofthecolorindicatestherelativeproportionofP
mc mc
contributedbytheoverlapbetweenthesecomponents(i.e.,∆P(i,j)).
mc
TheheatmapintherightpanelofFigure1visualizesthe∆P(i,j)contributionsfromeachpairofmixturecomponents,
mc
showingthatthemainsourcesofP comefromtheoverlappingcomponentsinthetopleftandbottomrightcorners.
mc
Withathresholdofτ =0.01,thePHMalgorithmsequentiallycombinesthemixturecomponentsinthetopleftand
bottomrightcorners,reducingthevaluesofP to0.049and0.004,respectively. Intheend,thealgorithmreturnsfour
mc
clusters: onecorrespondingtothecomponentsineachofthefourcorners.
2.5 Applicationsink-meansClustering
ApplyingtheDistinguishabilitycriteriontoheuristics-basedclusteringalgorithmsrequiresadditionaldistributional
assumptionstocomputeP . Althoughpopularalgorithmsofthiskind—suchask-meansandhierarchicalclustering—
mc
rely on intuitive heuristics, their implicit connections to probability models have been well studied. These results
provideinsightsintothedatatypesforwhichcertainnon-model-basedclusteringalgorithmsareexpectedtobeoptimal.
Thek-meansalgorithm,inparticular,hasbeenshownasequivalenttoapproximatelyoptimizingamultivariateGaussian
classificationlikelihoodfunction[13,14]. Hence,whenapplyingtheDistinguishabilitycriterionwiththeusualk-means
distortionfunction,itseemsnaturaltoassumethatdatawithineachinferredpartitionarenormallydistributed. Itthen
Algorithm1:P HierarchicalMerging(PHM)algorithm
mc
Input: InputdataX,P thresholdτ
mc
Result: Groupingsofmixturecomponentsintoclusters
1 ProcedurePHM
2 FitamixturemodeltoX,determiningthenumberofcomponentsbymaximizingBIC
3 Initializeclusterstoindividualmixturecomponents
4 Compute∆P m(i c,j)forallpairsofclustersi,j
5 whileP mc >τ do
6 Groupclustersi,j withmaximal∆P m(i c,j)intoasingleclusterk′
7 Updatethedistributionquantitiesforthisnewcluster:
8 α k′ ←α j +α i
9 p(x|θ =k′)←α k− ′1·[α i·p(x|θ =i)+α j ·p(x|θ =j)]
10 π k′(x)←π i(x)+π j(x)
11 UpdateP mc ←P mc−∆P m(i c,j)
12 Compute∆P m(k c′,k)foralluninvolvedclustersk: ∆P m(k c′,k) =∆P m(i c,k)+∆P m(j c,k)
13 return
5InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Figure2: (Left)450simulatedobservationsdrawnfromamixtureofthreeGaussiandistributions. Colorindicatestrue
generatingdistributionwhileshapeindicatestheassignedk-meanscluster. (CenterandRight)Valueofthegapstatistic,
P ,andtheSilhouetteindexfordifferentnumbersofclustersbasedonthekmeansclusteringpartitionwithGaussian
mc
clusterdistributions.
becomesstraightforwardtoestimatethenecessaryparametersandcomputeP giventhepartitioneddatafromthe
mc
k-meansoutput.
Weillustrateaproceduretodeterminethenumberofclusters(K)ink-meansclusteringbyoptimizingthecombined
loss of P and the gap statistic [20]. For a given data partitioning from the k-means algorithm, the gap statistic
mc
compares the observed within-cluster dispersion to the expected dispersion under a null reference distribution. It
subsequentlyestimatestheoptimalnumberofclusterscorrespondingtothelargestgapstatisticvaluefromarangeof
potentialK values. WeconsiderobservationsdrawnfromthreeGaussianclustersinR2centeredat(0,0),(1.75,1.75),
and(-4,4)withidentitycovariancematrices(n =150). TheobserveddataareshowninFigure2,withsubstantial
k
overlapbetweenthedatageneratedfromtwoofthethreedistributions. Thek-meansalgorithmisperformedforK =1
to7usingtheRpackageClusterR[36]withkmeans++initialization[37]. InadditiontotheP values,wealso
mc
presenttheaveragedSilhouetteindex[24]foreachvalueofK =2,...,7.
Thevaluesofthegapstatistic,P ,andtheSilhouetteindexfordifferentK areshowninFigure2. Whenusedalone,
mc
thegapstatisticselectsK =3,coincidingwiththenumberofgeneratingdistributions. Thereisanoticeabledifference
intheinferredclusters’separabilitybetweenK =2(P =0.002)andK =3(P =0.062). ForaP threshold
mc mc mc
τ =0.01,optimizingthecombinedlossfunction(3)leadstoselectingK =2,whichseemsmostreasonablejudging
bythedatavisualization. ThisdecisionisalsoconsistentwiththeSilhouetteindex,whichtakesamaximalvalueof
0.632atK =2comparedto0.522atK =3. AdditionallywefindthatP ishighlynegativelycorrelatedwithcluster
mc
stabilitymeasures. WepresentthevaluesofthestabilitymeasureproposedbyLangeetal. [32]usingtheadjustedRand
indextocomputeclustersimilaritiesaswellasthepredictionstrength[33]inSupplementaryTable1.
Asnoticedintheoriginalpaper[20],thegapstatisticcanstruggletodetermineK whentheunderlyinggenerative
distributionshavesubstantialoverlappingsupport. TheabovenumericalexampleillustratesthattheDistinguishability
criterioncanalleviatethischallengeink-meansclustering.
2.6 HypothesisTestinginHierarchicalClustering
Ithasbecomeincreasinglycommontoperformformalstatisticaltestinginpost-clusteringanalysistoreducecluster
over-identification. RecentworksbyGaoetal. [16],Chenetal. [17],andGrabskietal. [18]highlightthenecessity
andimportanceofsuchanalysisinscientificapplications,wherefalsepositivefindingsofclustersareconsideredmore
costlythanfalsenegativefindings. Inmanyapplicationcontexts,itisoftenofinteresttoassesswhetherthepartitions
oftheobserveddataoutputbyheuristics-basedclusteringalgorithms(e.g.,hierarchicalclustering)couldarisefrom
a single homogeneous distribution (most commonly, a single Gaussian distribution) by chance. P emerges as a
mc
6InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
naturalteststatisticinthisparametrichypothesistestingframeworkbecauseitsvaluesareexpectedtobequantitatively
differentunderthenullandalternativescenarios.
Formally,weconsidertestingthenullhypothesis,
H :thedataaregeneratedfromasingleGaussiandistribution
0
inthehierarchicalclusteringsetting[15,16,18]. Followinghierarchicalclusteringofobserveddata,wecomputeP
mc
forthepairofinferredclustersresultingfromthefirstsplitofthedendrogram(i.e.,K =2). Thenulldistributionof
P canbeestimatedbyasimpleMonteCarloprocedurethatrepeatedlysamplesdatafromH ,performshierarchical
mc 0
clustering,andcomputesP forthepartitionsdefinedbythefirstsplit. Alternatively,thep-valueoftheP statistic
mc mc
canbederivedfromthestandardparametricbootstrapprocedure[38].
WepresenttheresultsofsimulationstudiesexaminingtheperformanceofP inthishypothesistestingsetting.
mc
First,weillustrateP ’sabilitytocontrolforspuriousclusterdetection. For5,000simulationreplicates,wedraw150
mc
observationsfromaN(0,1)distributionandperformhierarchicalclusteringbasedonthesquaredEuclideandistance
withtheWardlinkage. WecomputeP andderivethecorrespondingp-valuesbasedontheestimatednulldistribution
mc
from5,000additionalMonteCarlosimulations,eachwithasamplesizeof150. WealsocomputetheP p-values
mc
usingabootstrapprocedure,using500bootstrapreplicatestoestimatethep-valueforeachsimulationreplicate. For
comparison,wealsocomputethep-valuesfromtheselectiveinferenceprocedureproposedbyGaoetal. [16]aswellas
thestandardtwo-samplet-test. TheresultsareshowninthetoppanelofFigure3. Asexpected,thep-valuesfromthe
two-samplet-testareallintherangeof10−20to10−40,providingnocontrolforfalsepositivefindings. Thisisbecause
thenaivet-testfailstotakeintoaccountthatthehierarchicalclusteringprocedurealwayspartitionsdataaccordingto
theirobservedvaluesevenunderH ,asnotedby[16]. Incontrast,p-valuesderivedfromP andGaoetal.’smethods
0 mc
arebothroughlyuniformlydistributedunderH ,suggestingwell-controlledtypeIerrorrates. Specifically,wefindthat
0
P =0.094correspondstothecutoffatthe5%αlevel,andtherealizedtypeIerrorrateis4.8%. ThetypeIerrorrate
mc
fromthebootstrapprocedureusingP atthesamecontrollevelis1%,whichismoreconservative.
mc
Second, we examine the power of the P -based hypothesis test to identify truly separated clusters. Specifically,
mc
wedrawsamplesfromtwodistinctGaussiandistributions,N(µ ,σ2)andN(µ ,σ2),wherearangeof|µ −µ |/σ
1 2 1 2
Figure3:(Top)Thedistributionofp-valuesbasedonP ,Gaoetal.’sselectiveinferenceprocedure,andthetwo-sample
mc
t-testfor5,000simulationreplicates. (Bottom)PowercomparisonbetweenP andGaoetal.’smethodtodetectthe
mc
presenceoftwoGaussianclusterscontrollingthetypeIerrorrateatlevelα=0.05astheclusterseparabilityincreases.
Thepowerateachvalue|µ −µ |/σiscalculatedbasedon500simulationreplicates.
1 2
7InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Figure 4: (Left) Bill and flipper lengths for the subset of female penguins with complete data from the un-scaled
palmerpenguinsdata. Colorindicatesspecies,whileshapeindicatestheassignedk-meansclusteratK =3. (Right)
ValueofthegapstatisticandP fordifferentnumbersofclustersbasedonthek-meansclusteringpartitionwith
mc
Gaussianclusterdistributions.
values are selected from the set {1,1.5,...,7}. In each alternative scenario represented by a unique |µ −µ |/σ
1 2
value,wedraw75observationsfromeachclusterdistributionandcomputethep-valuesusingP aswellasGaoet
mc
al.’sprocedure. Wecalculatethepowerbasedon500replicatesforeachalternativescenario. Ascanbeseeninthe
bottompanelofFigure3,P exhibitshigherpowerthanGaoetal.’sprocedureformoderate-to-largedegreesofcluster
mc
separation(definedby|µ −µ |/σ ∈[2,6]).
1 2
2.7 RealDataApplications
2.7.1 ClusterAnalysisofPalmerPenguinData
We analyze the penguins data from the palmerpenguins package [39], which consists of bill, flipper, and mass
measurementsfromthreespeciesofpenguinsinthePalmerArchipelagobetween2007and2009.Followingtheanalysis
by[16],weconsiderthesubsetoffemalepenguinswithcompletedataforbillandflipperlength(bothmeasuredin
millimeters),leavinguswith165observations. Priortoperformingtheclustering,wecenterandscaletheobservations
sothatthemeasurementshavezeromeanandunitvariance. FollowingtheprocedurelaidoutinSection2.5,weperform
k-meansclusteringofthescaledobservationsintoK =1,...,8clustersandcomputethegapstatisticaswellasP
mc
foreachgroupingoftheobservations. Figure4showsthedatausedforclusteringandthevaluesofthegapstatisticand
P fordifferentvaluesofK.
mc
FortheP thresholdof0.05,thecombinedlossdefinedbythegapstatisticandP isoptimizedatK =3. Boththe
mc mc
visualizationoftheclusteringdataandtheexternalspeciesinformationsuggestthattheresultisreasonable. Weobserve
thatthevaluesofP inthisexamplearestronglynegativelycorrelatedwithotherclustervalidityindices[24,32,33]to
mc
evaluatethek-meansclusteringpartitions(SupplementaryTable2). Werepeatthisanalysisusinghierarchicalclustering
basedonthesquaredEuclideandistancewiththeWardlinkageandcometothesameconclusions(Supplementary
Figure3andSupplementaryTable3).
2.7.2 InferringPopulationStructurefromHGDPdata
Inthisillustration,weapplythePHMalgorithmtoperformclusteranalysisonthegeneticdatafromtheHumanGenome
Diversity Project (HGDP) [40, 41], aiming to identify population structures. The dataset comprises 927 unrelated
individualssampledworldwideandgenotypedat2,543autosomalSNPs.Thegeographicsamplinglocationsarebroadly
dividedinto7continentalgroups: Europe,Central/SouthAsia(C/SAsia),Africa,MiddleEast,theAmericas,EastAsia,
andOceania.
Following the standard procedures for genetic data analysis, we pre-process the genotype matrix using principal
component analysis (PCA) and select the first 5 PCs for our analysis based on the elbow point of the scree plot
(SupplementaryFigure4). ToapplythePHMalgorithm,wefitaGMMtothedimension-reducedPCscorematrixand
selectthemodelwith9componentsbasedontheBIC.Eachindividual’sposteriorcomponentassignmentprobability
8InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Figure5:(Top)DendrogramvisualizingtheorderofcomponentmergesinthePHMprocedure. Numericvaluesindicate
thereductionofP bythemerge. Colorscorrespondtothemixturecomponentsfromthedistructplot. (Bottom)
mc
Distructplotforposteriorcomponentassignmentprobability,withcolorindicatingmixturecomponents. Observations
aregroupedbythegeographicregionoftheirsamplinglocation.
π isshowninthedistructplot[42]inFigure5. ExceptfortheEurope,Central/SouthAsia,andMiddleEastgroups,
k
theremainingcontinentalgroupstendtocorrespondtouniquemixturecomponents.
Starting with assigning each mixture component to its own cluster and an initial P = 0.1437, we proceed with
mc
thestepsofthePHMalgorithmwithτ = 0,i.e., merginguntilallobservationsbelongtoasingleclusterandP
mc
isdecreasedto0. Figure5visualizesthemergingprocessasadendrogram. Thenumbersineachnodeindicatethe
reduction of P , i.e., ∆P , by combining the corresponding branches into a single cluster. A smaller value of
mc mc
∆P indicatesthattheclustersbeingcombinedaremoredistinct. Becausethealgorithmalwaysprioritizesmerging
mc
the most overlapping clusters, the merging sequence reflects the relative genetic dissimilarities between different
clusters. Thisrelationshipcanbestraightforwardlyinterpretedfromthedendrogram. Forexample,thefirsttwomerges
—reducingP to0.063and0.021, respectively—formaclusterrepresentingsamplesfromEurope, theMiddle
mc
East,andCentral/SouthAsia,reflectingaclosegeneticrelationshipandnoticeablegeneticadmixtureamongthese
populationgroups. Thegeneralstructureofthedendrogramcanberoughlyexplainedbythelikelypathofhistorical
humanmigrations: fromAfricaintotheMiddleEast,fromtheMiddleEasttoEuropeandCentral/SouthAsia,from
Central/SouthAsiatoEastAsia,andfromEastAsiatoOceaniaandtheAmericas. Theoverallpatternofhumangenetic
diversityamongthecontinentalgroupsidentifiedfromouranalysisisalsocorroboratedbymoresophisticatedgenetic
analysisusingadditionalinformation(e.g.,haplotypeanalysis)[41]andhigh-coveragegenomesequencingdata[43].
2.7.3 ClusterAnalysisofSingle-cellRNASequenceData
Inthisillustration,weapplythePHMalgorithmtosingle-cellRNAsequencing(scRNA-seq)datafromasampleof
peripheralbloodmononuclearcellstoidentifythedifferentcelltypes. Thedata(sequencedontheIlluminaNextSeq
500andfreelyavailablefrom10xGenomics)consistofgeneexpressioncountsfor2,700singlecellsat13,714genes.
Therawsequencedataarequality-controlledandpre-processedusingstandardproceduresimplementedintheSeurat
package[44],leavinguswith2,638cells. Principalcomponentanalysisissubsequentlyperformedonthenormalized
andscaledexpressioncountsfordimensionreduction. Thefirst10componentsareselectedbasedontheelbowpoint
9InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Figure6:(Top)DendrogramvisualizingtheorderofcomponentmergesinthePHMprocedure. Numericvaluesindicate
thereductionofP bythemerge. Colorscorrespondtothemixturecomponentsfromthedistructplot. (Bottom)
mc
Distructplotforposteriorcomponentassignmentprobability,withcolorindicatingmixturecomponents. Observations
aregroupedbyannotatedcelltype.
of the scree plot (Supplementary Figure 5). The resulting 2,638 × 10 data matrix is used for our cluster analysis.
Additionally,eachcellisannotatedasoneofthefollowingcelltypes: NaïveCD4+Tcells,MemoryCD4+Tcells,
CD8+Tcells,NaturalKiller(NK)cells,Bcells,CD14+monocytes,FCGR3A+monocytes,Dendriticcells(DC),and
Platelets,usingknownbiomarkers. Theannotatedcelltypeinformationisnotusedinourclusteranalysisprocedure.
WefitaGMMonthedimension-reducedPCmatrixandselectthemodelwith9componentsbasedontheoptimalBIC.
Theposteriorcomponentassignmentprobabilityπ foreachcellisvisualizedinFigure6. Withtheexceptionofthe
k
NaïveandMemoryCD4+Tcells,eachcelltypecorrespondstoauniquemixturecomponent.
Starting with each mixture component as its own cluster and an initial P = 0.0703, we perform the merging
mc
procedureuntilallcomponentshavebeencombinedintoasinglecluster. Themergingprocessisrepresentedbythe
dendrogramin Figure6. Wenote thatthepattern shownin thedendrogramhas astrikingsimilarityto theknown
immunecelldifferentiationtrajectories. Forexample,thefirstmergecombinesthetwotypesofCD4+Tcells,reducing
P from0.070to0.042. Furthermore,CD8+Tcells,CD4+Tcells,NKcells,andBcells—allderivedfromLymphoid
mc
progenitorcells—aregroupedtogetherinthedendrogramandseparatedfromthebranchconsistingofFCGR4A+,
CD14+monocytes,andDendriticcells—allofwhicharederivedfromMyeloidprogenitorcells.
3 ConclusionandDiscussion
Inthiswork,weintroducetheDistinguishabilitycriterion,P ,toquantifytheseparabilityofclustersinferredfrom
mc
clusteranalysisprocedures. Wediscusstheintuitionbehindthecriterion,aswellasthederivationandpropertiesofP .
mc
Weproposeacombinedlossfunction-basedcomputationalframeworkthatintegratestheDistinguishabilitycriterion
withavailablemodelandheuristics-basedclusteringalgorithmsanddemonstrateitsusewithsyntheticandrealdata
applications.
TheproposedP isaninternalclusteringvalidityindextoassesstheseparabilityoftheclusteringresults,withunique
mc
advantages over alternative validity indices. Since P is measured on the probability scale, the threshold for our
mc
10InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
proposedconstrainedoptimizationproblemisinterpretable. Additionally,P measurementsaredirectlycomparable
mc
acrossdifferentdatasetsandvarioustypesofclusteringapplications,enablingfutureworkonassessingthereplicability
ofclusteringanalysis.
WhileournumericalillustrationsprimarilyuseGaussianormixturesofGaussiandistributionstoevaluateP ,itis
mc
importanttohighlighttheproposedcomputationalframework’sflexibilityandcompatibilitywithanyvalidparametric
likelihoodfunction. Asaresult, theapplicationsofP canbeextendedtoamorediverseclassoflatentvariable
mc
models,e.g.,latentDirichletallocation(LDA)andgeneralizedfactoranalysismodels,inordertohelpaddressthe
similarmodelselectionproblemsthatarisewiththeiruse. Wewillexploretheseextensionsinourfuturework.
ThepropertiesofP arebestutilizedinourproposedPHMalgorithm,whichismotivatedbyBaudryetal.’sentropy
mc
criterion-based merging procedure [30]. By combining mixture components into clusters, both algorithms enjoy
excellentmodelfitandinterpretabilityfortheinferredclusters. However,theconstrainedoptimizationformulation
describedaboveleadstoamoreinterpretablestoppingruleforthePHMalgorithmcomparedtoBaudryetal.’sprocedure.
Furthermore,thePHMalgorithmshowsimprovedcomputationalefficiency—duetotheclustermergingpropertyof
P ,thecomplexityofthePHMalgorithmisinvarianttothesamplesizeoftheobservedclusteringdata,makingit
mc
moresuitableforanalyzinglarge-scaledata. Finally,wenotethatthedendrogramvisualizingthecompletemerging
procedurecanbeusedtorepresentacoalescentprocessthathasmanyapplicationsacrossscientificdisciplines,suchas
developmentalbiology,humangenetics,andevolutionarybiology. Onepossiblefutureresearchdirectionisshifting
thefocusfromclusteranalysistouncoveringtheunderlyingcoalescenttreesbyincorporatingmorecontext-specific
information,withthePHMalgorithmasanaturalstartingpoint.
ItisalsopossibletoextendthePHMalgorithmtoworkwithageneralclassofhardclusteringalgorithmsthatoutput
optimalpartitionsofobserveddata. Onestrategyistotreateachoutputpartitionasadistinctpopulationsampleand
estimateitsdistributionusingafinitemixturemodel. Wecanthenre-normalizethemixtureproportionsgloballyand
initilizetheclustersattheleveloftheoutputpartitions,atwhichpointapplyingthePHMalgorithmisstraightforward.
ThissimplestrategyextendstheapplicationsoftheDistinguishabilitycriteriontoamorediverseclassofclustering
algorithms,includingdensity-basedandgraphclusteringalgorithms.
4 Methods
4.1 OverviewofClusteringMethods
Webrieflyreviewexistingclusteringmethods,focusingonthealgorithmsusedinthispaper. Theavailableclustering
methodscanberoughlyclassifiedintotwocategories: heuristics-basedclusteringmethods,representedbythek-means
andhierarchicalclusteringalgorithms, andmodel-basedclusteringmethods, representedbyfinite-mixturemodels.
Theheuristics-basedclusteringmethodstypicallydonotmakeexplicitdistributionalassumptionsandinsteadperform
hardclusteringbyoutputtingoptimalpartitionsoftheobserveddatabasedontheircorrespondingobjectivefunctions.
Themodel-basedclusteringmethodsperformformalstatisticalinferenceofthelatentclusterstructuresunderlyingthe
observeddata.
4.1.1 k-meansclustering
k-meansclusteringisawidelyusedheuristics-basedclusteringalgorithmforpartitioningasampleintoclusters. Asthe
namesuggests,thisprocedureidentifiesclustersbytheircentroids(definedasthemeanofallpointsinthecluster)and
assignsobservationstotheclusterwiththenearestcentroid. Moreformally,foraspecifiednumberofclustersK,the
algorithmgroupstheobservationsintodisjointsetsC ,...,C tominimizethedistortionfunction,i.e.,
1 K
K (cid:12)(cid:12) (cid:12)(cid:12)2
(cid:88)(cid:88)(cid:12)(cid:12) (cid:88) (cid:12)(cid:12)
min (cid:12)(cid:12)x i− x j/|C k|(cid:12)(cid:12) ,
C1,...,CKk=1i∈Ck(cid:12)(cid:12)
j∈Ck
(cid:12)(cid:12)
2
which corresponds to minimizing the within-cluster variances. The clustering is usually performed using Lloyd’s
algorithm[45],whichalternatesbetweenassigningobservationstotheclusterwiththenearestcentroidandupdating
theclustercentroidtoeventuallyarriveatalocallyoptimalsolution.
Ithasbeenshownthatthek-meansalgorithmisequivalenttoanapproximatemaximumlikelihoodprocedure,wherethe
underlyingprobabilitymodelassumesamultivariateGaussiandistributionforeachlatentcluster[13,14]. Hence,the
observationfrequencyofeachclusterandthecorrespondingparametersfortheGaussiandistributioncanbeestimated
straightforwardlyusingthepartitionedoutputfromthek-meansalgorithm.
11InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
4.1.2 Hierarchicalclustering
Hierarchical clustering produces a sequence of partitions for observations. In the commonly used agglomerative
hierarchical clustering procedure, each observation comprises its own cluster in the initial partition. Subsequent
clusteringsinthesequenceareformedbyrepeatedlycombiningthemostsimilarpairofgroupsofobservationsinto
a single group until all observations have been combined into a single cluster. The similarity between groups of
observationsinhierarchicalclusteringistypicallydefinedbyadistancemeasurebetweenpairsofobservations,such
asthesquaredEuclideandistance,inadditiontoafunctionthatgeneralizesthissimilaritytogroupsofobservations
(referredtoasalinkagefunction). Commonlyusedlinkagefunctionsbetweengroupsofobservationsaresinglelinkage,
completelinkage,averagelinkage,andtheWardlinkage.
ThehierarchicalclusteringalgorithmisalsooftenconnectedtoprobabilitymodelsassumingaGaussiandatadistribution
underlyingeachcluster[46,14],andexplicitGaussianassumptionsarecommonlyusedformodel-selectionorpost-
selectioninferenceinhierarchicalclustering[15,16,18].
4.1.3 Model-basedClustering
Finitemixturemodelsarethemostrepresentativeapproachformodel-basedclustering,whereamixturedistributionwith
finitecomponentsmodelstheobservedclusteringdata. Traditionally,eachmixturecomponentistakentocorrespond
to a homogeneous subpopulation or cluster. The goal of inference in the mixture model setting is to estimate the
characteristicsofeachmixturecomponent,i.e.,p(x|θ =k),andthecorrespondingmixtureproportion,i.e.,α . The
k
expectation-maximization(EM)algorithmiscommonlyusedtofindmaximumlikelihoodestimatesoftheparameters
ofinterest. TheGaussianmixturemodel(GMM),whichmodelseachmixturecomponentusingauniqueGaussian
distribution,isprobablythemostcommonlyusedmixturemodelinpractice. Thisisbecause,amongotherreasons,the
GMMisconsideredtobeauniversalapproximator[47]andisflexibleenoughtofitdiversedatatypes.
Unlikemostheuristics-basedclusteringapproaches,model-basedclusteringalgorithmsperformsoft(orfuzzy)cluster-
ing,astheydonotdirectlypartitiontheobserveddata. Instead,everydatapointhasanassociated(posterior)probability
distributionoverallpossibleclusterassignments. Apost-hocclassificationprocedure,withapre-specifieddecisionrule
usingtheclusterassignmentprobabilities,canbeappliedtopartitiontheobserveddata.
Foramorethoroughreviewofmodel-basedclustering,wereferthereaderto[14,48,49]
4.2 DerivationandEstimationofP
mc
TheBayesriskforageneralclassifierδ(x) : Rp (cid:55)→ {1,...,K}assigningapointxtooneoftheK clusterscanbe
derivedasfollows,
P
mc
=E x(cid:104) E θ(cid:2) L(δ(x),θ)|x(cid:3)(cid:105)
 
(cid:90) K
(cid:88)(cid:88)
=  π i(x) Pr(δ(x)=j |x)P(dx)
(6)
j=1i̸=j
 
(cid:90) K
(cid:88)
=  (1−π j(x)) Pr(δ(x)=j |x)P(dx)
j=1
Foramoredetailedderivation,seeAppendixA.ThemarginaldatadistributionP(x),withloosenotation,isgivenby
K K
(cid:88) (cid:88)
P(x)= Pr(θ =k)p(x|θ =k)= α p(x|θ =k).
k
k=1 k=1
Forthedefaultrandomizeddecisionrule,δ (x)∼Categorical(π(x)),
r
Pr(δ(x)=j |x)=π (x). (7)
j
Thus,
12InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
 
(cid:90) K
(cid:88)
P mc,δr =  π j(x)(1−π j(x))P(dx)
j=1 (8)
(cid:90)
(cid:88)
=2 π (x)π (x)P(dx)
i j
i<j
TocomputeP usingtheoptimaldecisionrule,δ (x)=argmax π (x),wedefineapartitionofthesamplespace,
mc o k k
∪K R ,suchthatR :={x:δ (x)=k}. Itfollowsthat,
k=1 k k o
Pr(δ(x)=j |x)=1{x∈R }, (9)
j
and
 
(cid:90) K
(cid:88)(cid:88)
P mc,δo =  π i(x)1{x∈R j}P(dx)
j=1i̸=j (10)
(cid:90) (cid:18) (cid:19)
= 1−maxπ (x) P(dx)
k
k
Notethat,
K k
(cid:88) (cid:88)
π (x)(1−π (x))≥ π (x)(1−maxπ (x))=1−maxπ (x), ∀x (11)
j j j k k
k k
j=1 j=1
Hence,
P ≥P
mc,δr mc,δo
Unlessotherwisespecificied,weusethenotationP torefertoP bydefault. Inthispaper,weestimateP by
mc mc,δr mc
plugginginthepointestimatesoftheα ’saswellasthekeydistributionalparametersinthecorrespondinglikelihood
k
functionsobtainedfromtheobserveddata.
4.3 LowerandUpperBoundsofP
mc
Whenallclustersarewell-separated,P approachesitslowerboundat0. Morespecifically,assumingwell-separated
mc
clusters,bothofthefollowingconditionsshouldhold:
π (x)π (x)→0, ∀xand(i,j)pairs,
i j
and
maxπ (x)→1, ∀x.
k
k
Hence,bothdecisionrules(δ andδ )approachperfectclassificationaccuracy.
r o
P ismaximizedwhenallclustersarecompletelyoverlapping,i.e.,
mc
p(x|θ =i)=p(x|θ =j)∀xand(i,j)pairs. (12)
Thus,π (x)=α , ∀x. Itfollowsthat
k k
K
(cid:88)
maxP = α (1−α ),
mc,δr k k
k=1
13InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
and
maxP =1−maxα .
mc,δo
k
k
Inthespecialcasethatα =1/K forallkvalues,
k
K−1
maxP =maxP = . (13)
mc,δr mc,δo K
4.4 TheClusterMergingPropertyofP
mc
The merging property is specific to the default P , evaluated using the randomized decision rule δ . For a given
mc r
clusterconfigurationwithK ≥2,considermergingtwoarbitraryclusterstoformanewcombinedcluster. LetP and
mc
P† denotethemisclassificationprobabilitiesbeforeandafterthemerging,respectively. Thefollowingproposition
mc
summarizesthemergingproperty.
Proposition1. Mergingtwoexistingclustersindexedbyiandj leadsto
(cid:90)
∆P(i,j) :=P −P† =2 π (x)π (x)P(dx)≥0.
mc mc mc i j
Furthermore,
(cid:88)
P = ∆P(i,j)
mc mc
i<j
Proof. AppendixB.
TheclustermergingpropertyformsthebasisofthePHMalgorithm. As∆P(i,j)canbepre-computedforallpairsof
mc
clustersfromtheinitialconfiguration,thesubsequentupdatesforP —mergingonepairofclustersatatime—become
mc
straightforwardtocompute.
4.5 NumericalEvaluationofP
mc
Evaluating P numerically can be challenging, especially when clustering data are high-dimensional. For low-
mc
dimensionaldata,itispossibletoevaluateEqn(6)bynumericalintegrationusingvariousquadraturemethods. However,
theygenerallydonotscalewellwhentheclusteringdatadimensionalitybecomeslargerthan5. Whenthemarginaldata
distribution,P(x),canbedirectlysampledfrom(asinthecaseinallexamplespresentedinthispaper),MonteCarlo
(MC)integrationbecomesanefficientsolution. Specifically,wesampleM datapointsfromP(x)andapproximate
P by
mc
 
M K (cid:18) (cid:19)
Pˆ
mc
= M1 (cid:88) (cid:88) 1−π j(x i) Pr(δ(x i)=j |x) (14)
i=1 j=1
√
TheuniqueadvantageoftheMonteCarlointegrationmethodisthatitserrorboundisalwaysO(1/ M)regardlessof
thedimensionalityofx. WeprovidecomparisonsbetweentheMonteCarlomethodandthenumericalintegrationfor
evaluatingP insomelow-dimensionalsettings(SupplementaryMethod1andSupplementaryTable4),indicating
mc
thattheMCintegrationmethodisaccurateandefficient.
14InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
References
[1] ElkeBraun,BartGeurten,andMartinEgelhaaf. Identifyingprototypicalcomponentsinbehaviourusingclustering
algorithms. PloSone,5(2):e9361,2010.
[2] SWibisono,MTAnwar,AjiSupriyanto,andIHAAmin. Multivariateweatheranomalydetectionusingdbscan
clusteringalgorithm. InJournalofPhysics: ConferenceSeries,volume1869,page012077.IOPPublishing,2021.
[3] ParvezAhmad,SaqibQamar,andSyedQasimAfserRizvi. Techniquesofdatamininginhealthcare: areview.
InternationalJournalofComputerApplications,120(15),2015.
[4] Jui-HungKao,Ta-ChienChan,FeipeiLai,Bo-ChengLin,Wei-ZenSun,Kuan-WuChang,Fang-YieLeu,and
Jeng-WeiLin. Spatialanalysisanddataminingtechniquesforidentifyingriskfactorsofout-of-hospitalcardiac
arrest. InternationalJournalofInformationManagement,37(1):1528–1538,2017.
[5] SarahShafqat,SairaKishwer,RaihanUrRasool,JunaidQadir,TehminaAmjad,andHafizFarooqAhmad. Big
dataanalyticsenhancedhealthcaresystems: areview. TheJournalofSupercomputing,76:1754–1799,2020.
[6] JuanXie,AnjunMa,YuZhang,BingqiangLiu,ChanglinWan,ShaCao,ChiZhang,andQinMa. Qubic2: a
novelbiclusteringalgorithmforlarge-scalebulkrna-sequencingandsingle-cellrna-sequencingdataanalysis.
bioRxiv,page409961,2018.
[7] Vladimir Yu Kiselev, Tallulah S Andrews, and Martin Hemberg. Challenges in unsupervised clustering of
single-cellrna-seqdata. NatureReviewsGenetics,20(5):273–282,2019.
[8] ItamarKanter,PieroDalerba,andTomerKalisky. Aclusterrobustnessscoreforidentifyingcellsubpopulationsin
singlecellgeneexpressiondatasetsfromheterogeneoustissuesandtumors. Bioinformatics,35(6):962–971,2019.
[9] RenQi,AnjunMa,QinMa,andQuanZou. Clusteringandclassificationmethodsforsingle-cellrna-sequencing
data. Briefingsinbioinformatics,21(4):1196–1208,2020.
[10] JohnHarmonWolfe. Objectclusteranalysisofsocialareas. PhDthesis,UniversityofCalifornia,1963.
[11] RichardMCormack. Areviewofclassification. JournaloftheRoyalStatisticalSociety: SeriesA(General),
134(3):321–353,1971.
[12] GbeminiyiJohnOyewoleandGeorgeAlexThopil. Dataclustering: applicationandtrends. ArtificialIntelligence
Review,56(7):6439–6475,2023.
[13] HansHBock. Probabilisticmodelsinclusteranalysis. ComputationalStatistics&DataAnalysis,23(1):5–28,
1996.
[14] ChrisFraleyandAdrianERaftery. Model-basedclustering,discriminantanalysis,anddensityestimation. Journal
oftheAmericanStatisticalAssociation,97(458):611–631,June2002.
[15] Patrick K. Kimes, Yufeng Liu, David Neil Hayes, and James Stephen Marron. Statistical significance for
hierarchicalclustering. Biometrics,73(3):811–821,January2017.
[16] Lucy L Gao, Jacob Bien, and Daniela Witten. Selective inference for hierarchical clustering. Journal of the
AmericanStatisticalAssociation,pages1–11,2022.
[17] YiqunTChenandDanielaMWitten.Selectiveinferencefork-meansclustering.arXivpreprintarXiv:2203.15267,
2022.
[18] IsabellaN.Grabski, KellyStreet, andRafaelA.Irizarry. Significanceanalysisforclusteringwithsingle-cell
rna-sequencingdata. NatureMethods,20(8):1196–1202,July2023.
[19] ChristianHennig. Whatarethetrueclusters? PatternRecognitionLetters,64:53–62,October2015.
[20] RobertTibshirani,GuentherWalther,andTrevorHastie. Estimatingthenumberofclustersinadatasetviathe
gapstatistic. JournaloftheRoyalStatisticalSociety: SeriesB(StatisticalMethodology),63(2):411–423,2001.
[21] MariaHalkidi,YannisBatistakis,andMichalisVazirgiannis. Onclusteringvalidationtechniques. Journalof
intelligentinformationsystems,17:107–145,2001.
[22] Minho Kim and RS Ramakrishna. New indices for cluster validity assessment. Pattern Recognition Letters,
26(15):2353–2363,2005.
[23] Yanchi Liu, Zhongmou Li, Hui Xiong, Xuedong Gao, and Junjie Wu. Understanding of internal clustering
validationmeasures. In2010IEEEinternationalconferenceondatamining,pages911–916.IEEE,2010.
[24] PeterJRousseeuw. Silhouettes: agraphicalaidtotheinterpretationandvalidationofclusteranalysis. Journalof
computationalandappliedmathematics,20:53–65,1987.
15InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
[25] TadeuszCalin´skiandJerzyHarabasz. Adendritemethodforclusteranalysis. CommunicationsinStatistics-theory
andMethods,3(1):1–27,1974.
[26] JosephCDunn. Well-separatedclustersandoptimalfuzzypartitions. Journalofcybernetics,4(1):95–104,1974.
[27] Volodymyr Melnykov. Merging mixture components for clustering through pairwise overlap. Journal of
ComputationalandGraphicalStatistics,25(1):66–90,January2016.
[28] GillesCeleuxandGildaSoromenho. Anentropycriterionforassessingthenumberofclustersinamixturemodel.
JournalofClassification,13(2):195–212,September1996.
[29] C.Biernacki,G.Celeux,andG.Govaert. Assessingamixturemodelforclusteringwiththeintegratedcompleted
likelihood. IEEETransactionsonPatternAnalysisandMachineIntelligence,22(7):719–725,July2000.
[30] Jean-PatrickBaudry,AdrianE.Raftery,GillesCeleux,KennethLo,andRaphaëlGottardo. Combiningmixture
componentsforclustering. JournalofComputationalandGraphicalStatistics,19(2):332–353,January2010.
[31] UlrikeVonLuxburgetal. Clusteringstability: anoverview. FoundationsandTrends®inMachineLearning,
2(3):235–274,2010.
[32] TilmanLange,VolkerRoth,MikioLBraun,andJoachimMBuhmann. Stability-basedvalidationofclustering
solutions. Neuralcomputation,16(6):1299–1323,2004.
[33] RobertTibshiraniandGuentherWalther. Clustervalidationbypredictionstrength. JournalofComputationaland
GraphicalStatistics,14(3):511–528,2005.
[34] ChristianHennig. Methodsformerginggaussianmixturecomponents. AdvancesinDataAnalysisandClassifica-
tion,4(1):3–34,January2010.
[35] LucaScrucca,MichaelFop,T.BrendanMurphy,andAdrianE.Raftery. mclust5: clustering,classificationand
densityestimationusingGaussianfinitemixturemodels. TheRJournal,8(1):289–317,2016.
[36] Lampros Mouselimis. ClusterR: Gaussian Mixture Models, K-Means, Mini-Batch-Kmeans, K-Medoids and
AffinityPropagationClustering,2023. Rpackageversion1.3.1.
[37] DavidArthur,SergeiVassilvitskii,etal. k-means++: Theadvantagesofcarefulseeding. InSoda,volume7,pages
1027–1035,2007.
[38] DavidVHinkley. Bootstrapmethods. JournaloftheRoyalStatisticalSocietySeriesB:StatisticalMethodology,
50(3):321–337,1988.
[39] Allison Marie Horst, Alison Presmanes Hill, and Kristen B Gorman. palmerpenguins: Palmer Archipelago
(Antarctica)penguindata,2020. Rpackageversion0.1.0.
[40] LLucaCavalli-Sforza. Thehumangenomediversityproject: past,presentandfuture. NatureReviewsGenetics,
6(4):333–340,2005.
[41] DonaldFConrad,MattiasJakobsson,GrahamCoop,XiaoquanWen,JeffreyDWall,NoahARosenberg,and
JonathanKPritchard.Aworldwidesurveyofhaplotypevariationandlinkagedisequilibriuminthehumangenome.
Naturegenetics,38(11):1251–1260,2006.
[42] NoahARosenberg. Distruct: aprogramforthegraphicaldisplayofpopulationstructure. Molecularecology
notes,4(1):137–138,2004.
[43] AndersBergström,ShaneAMcCarthy,RuoyunHui,MohamedAAlmarri,QasimAyub,PetrDanecek,Yuan
Chen,SabineFelkel,PilleHallast,JackKamm,etal. Insightsintohumangeneticvariationandpopulationhistory
from929diversegenomes. Science,367(6484):eaay5012,2020.
[44] YuhanHao,TimStuart,MadelineHKowalski,SaketChoudhary,PaulHoffman,AustinHartman,AviSrivastava,
GesmiraMolla,ShaistaMadad,CarlosFernandez-Granda,andRahulSatija. Dictionarylearningforintegrative,
multimodalandscalablesingle-cellanalysis. NatureBiotechnology,2023.
[45] StuartLloyd. Leastsquaresquantizationinpcm. IEEEtransactionsoninformationtheory,28(2):129–137,1982.
[46] GeoffreyJMcLachlan,SharonXLee,andSurenIRathnayake. Finitemixturemodels. Annualreviewofstatistics
anditsapplication,6:355–378,2019.
[47] IanGoodfellow,YoshuaBengio,andAaronCourville. Deeplearning. MITpress,2016.
[48] PaulDMcNicholas. Model-basedclustering. JournalofClassification,33:331–373,2016.
[49] IsobelClaireGormley,ThomasBrendanMurphy,andAdrianERaftery. Model-basedclustering. AnnualReview
ofStatisticsandItsApplication,10:573–595,2023.
16InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
AppendixA DerivationofP
mc
TocomputetheBayesriskforageneralclassifierδ(x)underthe0-1loss,wefirstevaluateitsposteriorexpectedloss
E (cid:2) L(δ(x),θ)|x(cid:3) asfollows:
θ
E (cid:2) L(δ(x),θ)|x(cid:3) =Pr(θ ̸=δ(x)|x)
θ
K
(cid:88)
= Pr(δ(x)=j, θ ̸=j |x)
j=1
K
(cid:88)
= Pr(θ ̸=j | x) Pr(δ(x)=j |x)
j=1
K
(cid:88)(cid:88)
= Pr(θ =i| x) Pr(δ(x)=j |x)
j=1i̸=j
k
(cid:88)(cid:88)
= π (x) Pr(δ(x)=j |x)
i
j=1i̸=j
k
(cid:88)
= (1−π (x)) Pr(δ(x)=j |x)
j
j=1
NotethatPr(θ ̸=j |x,δ(x))=Pr(θ ̸=j |x). Subsequently,
P
mc
=E x(cid:104) E θ(cid:2) L(δ(x),θ)|x(cid:3)(cid:105)
 
(cid:90) K
(cid:88)(cid:88)
=  π i(x) Pr(δ(x)=j |x)P(dx)
j=1i̸=j
 
(cid:90) K
(cid:88)
=  (1−π j(x)) Pr(δ(x)=j |x)P(dx)
j=1
AppendixB ProofofProposition1
Proof. Considermergingtwoexistingclustersi,j toanewcombinedclusterk′. Itfollowsthat
α =α +α
k′ i j
and
α p(x|θ =i)+α p(x|θ =j)
p(x|θ =k)= i j
α
k′
Consequently,byapplyingBayesrule,
π (x)=π (x)+π (x) (15)
k′ i j
LetSdenotethesetofindicesoftheexistingclustersnotimpactedbythemerge,where|S|=K−2. ByEqn(8),P
mc
canbewrittenas
17InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
(cid:90)
(cid:88)
P =2 π (x)π (x)P(dx)
mc m n
m,n∈S,m<n
(cid:90) (cid:90)
(cid:88) (cid:88)
+2 π (x)π (x)P(dx)+2 π (x)π (x)P(dx)
l i l j
l∈S l∈S
(cid:90)
+2 π (x)π (x)P(dx)
i j
ByEqn(15),
(cid:90) (cid:90) (cid:90)
(cid:88) (cid:88) (cid:88)
2 π (x)π (x)P(dx)+2 π (x)π (x)P(dx)=2 π (x)π (x)P(dx)
l i l j l k′
l∈S l∈S l∈S
andnotethat,
(cid:90) (cid:90)
(cid:88) (cid:88)
P† =2 π (x)π (x)P(dx)+2 π (x)π (x)P(dx)
mc m n l k′
m,n∈S,m<n l∈S
Itbecomesevidentthat
(cid:90)
∆P(i,j) =P −P† =2 π (x)π (x)P(dx)≥0 (16)
mc mc mc i j
Pluggingintheexpressionof∆P(i,j)intoEqn(8)yields
mc
(cid:88)
P = ∆P(i,j) (17)
mc mc
i<j
Remark Themergingpropertyisspecifictotherandomizeddecisionruleδ underthe0-1loss. Fortheoptimal
r
decisionrule,δ ,itcanbeshownthatP† ≤P aftermergingapairofexistingclusters. However,thequantitive
o mc mc
expressionfor∆P(i,j)isanalyticallyintractable.
mc
AppendixC P DendrogramConstruction
mc
WeuseadendrogramtovisualizethePHMproceduredescribedinAlgorithm1. Theleafnodesinthetreecorrespond
totheindividualcomponentsofthemixturemodel(MM)usedtofitthedata. Theedgesbetweenparentandchild
nodescorrespondtoaclustermergestep;wepresentthe∆P reductionfromthemergeontheparentnodeforthe
mc
mergedclusters. Inthisway,itispossibletodeterminethevalueofP aftereachsuccessivemergebysubtractingthe
mc
cumulative∆P valuesfromtheleafnodesofthetreeuptotheheightofaspecificmergefromtheP oftheinitial
mc mc
clusterconfiguration.
The height of a merge in the tree is determined as follows. Let P0 denote the P value at the initial cluster
mc mc
configurationand,foragivenmerge,letP‡ bethevalueofthecriterionpriortothatmergetakingplace. Weplacethe
mc
mergeinthedendrogramataheightcorrespondingtothelog scaledratioofthesevalues,i.e.,log P0 /P‡ . The
10 10 mc mc
log transformationpreventsthemergesfromearlyonintheprocedurefrombeing“squashed"tothebottomofthe
10
tree. WeopttousetheP valuebeforethemergeoccursratherthanaftertoavoiddividingbyzerowhencalculating
mc
theheightofthefinalmerge(whichwouldresultinP =0).
mc
18InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Supplementary Figures
1InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
SupplementaryFigure1: ValuesofP basedontherandomizedandoptimaldecisionrulesδ andδ . Thevalue
mc r o
P isshowninthey-axisandiscalculatedfortwounivariateGaussiandistributionsN(µ ,σ)andN(µ ,σ)where
mc 1 2
π =π =0.5. Thex-axisindicatesthedegreeofclusterseparationintermsofthedistributionparameters.
1 2
2InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
SupplementaryFigure2: DistributionplotsfortwounivariateGaussiandistributionsN(0,1)(solidline)andN(µ,1)
(dashed line) at decreasing values of P , where π = π = 0.5. The distance between the two centroids, |µ|,
mc 1 2
determinesthespecificP value.
mc
3InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
SupplementaryFigure3:(Left)Billandflipperlengthsforthesubsetofpalmerpenguinsdata. Colorindicatesspecies
whileshapeindicatestheassignedhierarchicalclusteringpartition. (Right)ValueofthegapstatisticandP basedon
mc
hierarchicalclusteringfordifferentnumbersofclusterswithGaussianclusterdistributions.
4InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
SupplementaryFigure4: Screeplotvisualizingstandarddeviationcapturedbyeachofoftheprincipalcomponent
vectorsfromtheHGDPdata.
5InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
SupplementaryFigure5: Screeplotvisualizingstandarddeviationcapturedbyeachofoftheprincipalcomponent
vectorsfromthescRNA-seqdata.
6InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Supplementary Tables
7InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
K Gap P Silhouette Stability(ARI) PredictionStrength
mc
1 0.403 0.000 — — —
2 0.824 0.002 0.632 0.995 1.000
3 1.058 0.062 0.522 0.961 0.842
4 0.901 0.101 0.417 0.758 0.618
5 0.808 0.126 0.353 0.723 0.519
6 0.722 0.137 0.353 0.630 0.679
7 0.740 0.158 0.348 0.619 0.399
SupplementaryTable1: ValuesofP andotherclustervalidityindicesforthesimulatedk-meansexampledata.
mc
8InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
K Gap P Silhouette Stability(ARI) PredictionStrength
mc
1 0.562 0.000 — — —
2 1.115 0.014 0.583 0.957 0.902
3 1.208 0.025 0.595 0.947 0.897
4 0.964 0.076 0.468 0.757 0.458
5 0.890 0.124 0.372 0.679 0.470
6 0.886 0.147 0.385 0.646 0.362
7 0.884 0.138 0.390 0.641 0.333
8 0.804 0.148 0.369 0.627 0.383
SupplementaryTable2: ValuesofP andotherclustervalidityindicesforthePalmerpenguinsdatabasedonthe
mc
k-meansclusteringsoftheobservations.
9InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
K Gap P Silhouette Stability(ARI) PredictionStrength
mc
1 0.545 0.000 — — —
2 1.133 0.012 0.581 0.909 0.942
3 1.325 0.024 0.595 0.943 0.899
4 1.124 0.063 0.483 0.757 0.544
5 1.051 0.099 0.470 0.702 0.464
6 1.024 0.141 0.382 0.652 0.413
7 1.013 0.132 0.387 0.656 0.366
8 0.994 0.128 0.381 0.645 0.345
SupplementaryTable3: ValuesofP andotherclustervalidityindicesforthePalmerpenguinsdatabasedonthe
mc
hierarchicalclusteringsoftheobservations.
10InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
p P Elapsed(s) Pˆ σ(Pˆ ) Elapsed(s)
mc mc mc
1 0.13144 0.011 0.13143 0.00056 1.010
2 0.13144 0.164 0.13141 0.00041 0.885
3 0.13144 3.163 0.13133 0.00042 0.994
4 0.13144 23.061 0.13132 0.00058 0.854
5 0.13145 23.689 0.13140 0.00051 0.954
Supplementary Table 4: P values computed using cubature methods and Monte Carlo integration based on 50
mc
replicates. TheP andPˆ valuesareaveragesacrossallreplicates. Elapsedtime(inseconds)istheaveragetimefor
mc mc
asinglereplicate.
11InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
Supplementary Methods
12InterpretableClusteringwiththeDistinguishabilityCriterion APREPRINT
1 MonteCarloP Comparison
mc
HerewecompareP computedusingstandardcubaturemethodstoPˆ estimatedusingaMonteCarlo(MC)integral
mc mc
(asdescribedinSection4.5). Weuse50replicatestoobtainthetimingmeasurementsandquantifytheuncertaintyinthe
MCintegral. Allvaluespresentedaremeansoverthesereplicatesunlessotherwisespecified. FortheMCintegration
procedureweuseM =105samplepoints,andparallelizethecomputationover8cores.
ThedistributioninquestionconsistsofthreeGaussianclusterswithπ =1/3inRpwithI variance. Theclusterare
k p
centeredat0pand±dp,wheredpisthep-dimensionalvectorwhoseelementsarealld. dissetsothattheEuclidean
(cid:112)
distancebetweendp andtheoriginisfixedtobe3;i.e. d = 32/p. ThisissothatthevalueofP remainsfixed
mc
acrossdimensionsandwedonotneedtoworryaboutthedimensionalityaffectingthetruevalueofP . Theresultsfor
mc
dimensionp=1,...,5arepresentedinSupplementaryTable4.
BothapproachesproducehighlysimilarvaluesofP acrossdimensions. ThestandarddeviationoftheMCestimates
mc
isquitelow, indicatingstabilityintheestimationprocedure. However, whilethecubaturemethodevaluationtime
rapidlyincreasesinp,thetimefortheMCprocedureisroughlyconstant. ThesetogetherhighlighttheMCestimation
procedureasaviableandaccurateapproachtoestimateP ,especiallyinmoderatedimensionaldata(i.e.,p ≥ 3)
mc
wherecubaturemethodsmaystruggletoreachasolutioninareasonabletime.
13