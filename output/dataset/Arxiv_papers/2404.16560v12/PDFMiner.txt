4
2
0
2

r
p
A
5
2

]
L
M

.
t
a
t
s
[

1
v
8
2
3
6
1
.
4
0
4
2
:
v
i
X
r
a

Distributionally Robust Safe Screening

Hiroyuki Hanadaâˆ—â€ 

Satoshi Akahaneâ€¡

Tatsuya Aoyamaâ€¡

Tomonari Tanakaâ€¡

Yoshito Okuraâ€¡

Yu InatsuÂ§

Noriaki Hashimotoâˆ—

Taro MurayamaÂ¶

Lee HanjuÂ¶

Shinya KojimaÂ¶

Ichiro Takeuchiâ€¡âˆ—â€–

April 26, 2024

Abstract

1

Introduction

In this study, we propose a method Distributionally
Robust Safe Screening (DRSS), for identifying unnec-
essary samples and features within a DR covariate
shift setting. This method effectively combines DR
learning, a paradigm aimed at enhancing model ro-
bustness against variations in data distribution, with
safe screening (SS), a sparse optimization technique
designed to identify irrelevant samples and features
prior to model training. The core concept of the DRSS
method involves reformulating the DR covariate-shift
problem as a weighted empirical risk minimization
problem, where the weights are subject to uncertainty
within a predetermined range. By extending the SS
technique to accommodate this weight uncertainty,
the DRSS method is capable of reliably identifying
unnecessary samples and features under any future
distribution within a specified range. We provide a
theoretical guarantee of the DRSS method and vali-
date its performance through numerical experiments
on both synthetic and real-world datasets.

âˆ—RIKEN, Wako, Saitama, Japan
â€ hiroyuki.hanada@riken.jp
â€¡Nagoya University, Nagoya, Aichi, Japan
Â§Nagoya Institute of Technology, Nagoya, Aichi, Japan
Â¶DENSO CORPORATION, Kariya, Aichi, Japan
â€–ichiro.takeuchi@mae.nagoya-u.ac.jp

1

In this study, we consider the problem of identi-
fying unnecessary samples and features in a class
of supervised learning problems within dynamically
changing environments. Identifying unnecessary sam-
ples/features offers several benefits. It helps in de-
creasing the storage space required for keeping the
training data for updating the machine learning (ML)
models in the future. Moreover, in situations demand-
ing real-time adaptation of ML models to quick envi-
ronmental changes, the use of fewer samples/features
enables more efficient learning.

Our basic idea to tackle this problem is to effectively
combine distributionally robust (DR) learning and safe
screening (SS). DR learning is a ML paradigm that
focuses on developing models robust to variations in
the data distribution, providing performance guaran-
tees across different distributions (see, e.g., [1]). On
the other hand, SS refers to sparse optimization tech-
niques that can identify irrelevant samples/features
before model training, ensuring computational effi-
ciency by avoiding unnecessary computations on cer-
tain samples/features which do not contribute to the
final solution [2, 3]. The key technical idea of SS is
to identify a bound of the optimal solution before
solving the optimization problem. This allows for the
identification of unnecessary samples/features, even
without knowing the optimal solution.

As a specific scenario of dynamically changing en-
vironment, we consider covariate shift setting [4, 5]
with unknown test distribution. In this setting, the

 
 
 
 
 
 
distribution of input features in the training data may
undergo changes in the test phase, yet the actual
nature of these changes remains unknown. A ML
problem (e.g., regression/classification problem) in
covariate shift setting can be formulated as a weighted
empirical risk minimization (weighted ERM) problem,
where weights are assigned based on the density ratio
of each sample in the training and test distributions.
Namely, by assigning higher weights to training sam-
ples that are important in the test distribution, the
model can focus on learning from relevant samples
and mitigate the impact of distribution differences
between the training and the test phases. If the distri-
bution during the test phase is known, the weights can
be uniquely fixed. However, if the test distribution
is unknown, it is necessary to solve a weighted ERM
problem with unknown weights.

Our main contribution is to propose a DRSS
method for covariate shift setting with unknown test
distribution. The proposed method can identify un-
necessary samples/features regardless of how the dis-
tribution changes within a certain range in the test
phase. To address this problem, we extend the exist-
ing SS methods in two stages. The first is to extend
the SS for ERM so that it can be applied to weighted
ERM. The second is to further extend the SS so that
it can be applied to weighted ERM when the weights
are unknown. While the first extension is relatively
straightforward, the second extension presents a non-
trivial technical challenge (Figure 1). To overcome
this challenge, we derive a novel bound of the optimal
solutions of the weighted ERM problem, which prop-
erly accounts for the uncertainty in weights stemming
from the uncertainty of the test distribution.

In this study, we consider DRSS for samples in
sample-sparse models such as SVM [6], and that for
features for feature-sparse models such as Lasso [7].
We denote the DRSS for samples as distributionally
robust safe sample screening (DRSsS) and that for fea-
tures as distributionally robust safe feature screening
(DRSfS), respectively.

Our contributions in this study are summarized
as follows. First, by effectively combining DR and
SS, we introduce a framework for identifying unnec-
essary samples/features under dynamically changing
uncertain environment. Second, We consider a DR

Figure 1: Schematic illustration of the proposed Dis-
tributionally Robust Safe Screening (DRSS) method.
Panel A displays the training samples, each assigned
equal weight, as indicated by the uniform size of the
points. Panel B depicts various unknown test distri-
butions, highlighting how the significance of training
samples varies with different realizations of the test
distribution. Panel C shows the outcomes of safe
sample screening (SsS) across multiple realizations
of test distributions. Finally, Panel D presents the
results of the proposed DRSS method, demonstrating
its capability to identify redundant samples regardless
of the observed test distribution.

covariate-shift setting where the input distribution
of an ERM problem changes within a certain range.
In this setting, we propose a novel method called
DRSS method that can identify samples/features that
are guaranteed not to affect the optimal solution, re-
gardless of how the distribution changes within the
specified range. Finally, through numerical exper-
iments, we verify the effectiveness of the proposed
DRSS method. Although the DRSS method is devel-
oped for convex ERM problems, in order to demon-
strate the applicability to deep learning models, we
also present results where the DRSS method is ap-
plied in a problem setting where the final layer of the
model is fine-tuned according to changes in the test
distribution.

2

Safe Sample ScreeningUnknownTest DistributionDRSS Method (Proposed)DistributionallyRobustSafe ScreeningABCD1.1 Related Works

The DR setting has been explored in various ML prob-
lems, aiming to enhance model robustness against
data distribution variations. A DR learning problem
is typically formulated as a worst-case optimization
problem since the goal of DR learning is to ensure
model performance under the worst-case data dis-
tribution within a specified range. Hence, a variety
of optimization techniques tailored to DR learning
have been investigated within both the ML and opti-
mization communities [8, 9, 1]. The proposed DRSS
method is one of such DR learning methods, focusing
specifically on the problem of sample/feature deletion.
The ability to identify irrelevant samples/features is
of practical significance. For example, in the context
of continual learning (see, e.g., [10]), it is crucial to
effectively manage data by selectively retaining and
discarding samples/features, especially in anticipa-
tion of changes in future data distributions. Incorrect
deletion of essential data can lead to catastrophic for-
getting [11], a phenomenon where a ML model, after
being trained on new data, quickly loses information
previously learned from older datasets. The proposed
DRSS method tackles this challenge by identifying
samples/features that, regardless of future data dis-
tribution shifts, will not have any influence on all
possible newly trained model in the future.

SS refers to optimization techniques in sparse
learning that identify and exclude irrelevant sam-
ples or features from the learning process. SS can
reduce computational cost without changing the fi-
nal trained model. Initially, SfS was introduced by
[2] for the Lasso. Subsequently, SsS was proposed
by [3] for the SVM. Among various SS methods de-
veloped so far, the most commonly used is based
on the duality gap [12, 13]. Our proposed DRSS
method also adopts this approach. Over the past
decade, SS has seen diverse developments, including
methodological improvements and expanded applica-
tion scopes [14, 15, 16, 17, 18, 19, 20, 21]. Unlike
other SS studies that primarily focused on reducing
computational costs, this study adopts SS for a dif-
ferent purpose. We employ SS across scenarios where
data distribution varies within a defined range, aim-
ing to discard unnecessary samples/features. To our

Table 1: Notations used in the paper. R: all real num-
bers, N: all positive integers, n, m, p âˆˆ N: integers,
f : Rn â†’ R âˆª {+âˆž}: convex function, M âˆˆ RnÃ—m:
matrix, v âˆˆ Rn: vector.
mij âˆˆ R (small case of matrix variable)

the element at the ith row and
the jth column of M

vi âˆˆ R (nonbold font of vector variable)

Mi: âˆˆ R1Ã—n
M:j âˆˆ RmÃ—1
[n]
Râ‰¥0
âŠ—
diag(v) âˆˆ RnÃ—n

vÃ—â–¡M âˆˆ RnÃ—m
0n âˆˆ Rn
1n âˆˆ Rn
âˆ¥vâˆ¥p âˆˆ Râ‰¥0
âˆ‚f (v) âŠ† Rn

Z[f ] âŠ† Rn
f âˆ—(v) âˆˆ R âˆª {+âˆž}

â€œf is Îº-strongly

convexâ€ (Îº > 0)

â€œf is Âµ-smoothâ€

(Âµ > 0)

the ith element of v
the ith row of M
the jth column of M
{1, 2, . . . , n}
all nonnegative real numbers
elementwise product
diagonal matrix; (diag(v))ii = vi
and (diag(v))ij = 0 (i Ì¸= j)
diag(v)M
[0, 0, . . . , 0]âŠ¤ (vector of size n)
[1, 1, . . . , 1]âŠ¤ (vector of size n)
((cid:80)n
i )1/p (p-norm)
all g âˆˆ Rn s.t. â€œfor any vâ€² âˆˆ Rn,
f (vâ€²) âˆ’ f (v) â‰¥ gâŠ¤(vâ€² âˆ’ v)â€
(subgradient)
{vâ€² âˆˆ Rn | âˆ‚f (vâ€²) = {0n}}
supvâ€²âˆˆRn (vâŠ¤vâ€² âˆ’ f (vâ€²))
(convex conjugate)
f (v) âˆ’ Îºâˆ¥vâˆ¥2
respect to v

2 is convex with

i=1 vp

âˆ¥f (v) âˆ’ f (vâ€²)âˆ¥2 â‰¤ Âµâˆ¥v âˆ’ vâ€²âˆ¥2

for any v, vâ€² âˆˆ Rn

knowledge, no existing studies have utilized SS within
the DR learning framework.

2 Preliminaries

Notations used in this paper are described in Table 1.

3

2.1 Weighted Regularized Empiri-
cal Risk Minimization (Weighted
RERM) for Linear Prediction

we have the following dual problem of (1):

Î±âˆ—(w) := argmax

Î±âˆˆRn

Dw(Î±), where

We mainly assume the weighted regularized empirical
risk minimization (weighted RERM) for linear pre-
diction. This may include kernelized versions, which
are discussed in Appendix C. Suppose that we learn
the model parameters as linear prediction coefficients,
that is, learn Î²âˆ—(w) âˆˆ Rd such that the outcome for a
sample x âˆˆ Rd is predicted as xâŠ¤Î²âˆ—(w).

Definition 2.1. Given n training samples of d-
dimensional input variables, scalar output variables
and scalar sample weights, denoted by X âˆˆ RnÃ—d,
y âˆˆ Rn and w âˆˆ Rn
â‰¥0, respectively, the training com-
putation of weighted RERM for linear prediction is
formulated as follows:

Pw(Î²), where

Î²âˆ—(w) := argmin
Î²âˆˆRd
n
(cid:88)

Pw(Î²) :=

wiâ„“yi( Ë‡Xi:Î²) + Ï(Î²).

(1)

i=1

Here, â„“y : R â†’ R is a convex loss function1, Ï : Rd â†’
R is a convex regularization function, and Ë‡X âˆˆ RnÃ—d
is a matrix calculated from X and y and determined
depending on â„“. In this paper, unless otherwise noted,
we consider binary classifications (y âˆˆ {âˆ’1, +1}n)
with Ë‡X := yÃ—â–¡X. For regressions (y âˆˆ Rn) we usually
set Ë‡X := X.

Remark 2.2. We add that, we adopt the formulation
X:d = 1n so that Î²âˆ—(w)
(the last element) represents
the common coefficient for any sample (called the
intercept).

d

Since â„“ and Ï are convex, we can easily confirm that

Pw(Î²) is convex with respect to Î².

Applying Fenchelâ€™s duality theorem (Appendix A.2),

Dw(Î±) :=
n
(cid:88)

âˆ’

wiâ„“âˆ—
yi

(âˆ’Î³iÎ±i) âˆ’ Ïâˆ—(((Î³ âŠ— w)Ã—â–¡ Ë‡X)âŠ¤Î±),

(2)

i=1

where Î³ is a positive-valued vector. The relationship
between the original problem (1) (called the primal
problem) and the dual problem (2) are described as
follows:

Pw(Î²âˆ—(w)) = Dw(Î±âˆ—(w)),
Î²âˆ—(w) âˆˆ âˆ‚Ïâˆ—(((Î³ âŠ— w)Ã—â–¡ Ë‡X)âŠ¤Î±âˆ—(w)),
âˆ€i âˆˆ [n] : âˆ’Î³iÎ±âˆ—(w)

âˆˆ âˆ‚â„“yi( Ë‡Xi:Î²âˆ—(w)).

i

(3)

(4)

(5)

2.2 Sparsity-inducing Loss Functions
and Regularization Functions

In weighted RERM, we call that a loss function â„“
induces sample-sparsity if elements in Î±âˆ—(w) are easy
to become zero. Due to (5), this can be achieved by â„“
such that {t âˆˆ R | 0 âˆˆ âˆ‚â„“y(t)} is not a point but an
interval.

Similarly, we call that a regularization function Ï
induces feature-sparsity if elements in Î²âˆ—(w) are easy
to become zero. Due to (4), this can be achieved by
Ï such that {v âˆˆ Rd | âˆƒj âˆˆ [d âˆ’ 1] : 0 âˆˆ [âˆ‚Ïâˆ—(v)]j} is
not a point but a region.

For example, the hinge loss â„“y(t) = max{0, 1 âˆ’ t}
(y âˆˆ {âˆ’1, +1}) is a sample-sparse loss function since
{t âˆˆ R | 0 âˆˆ âˆ‚â„“y(t)} = [1, +âˆž). Similarly, the L1-
regularization Ï(v) = Î» (cid:80)dâˆ’1
j=1 |vj| (Î» > 0: hyperpa-
rameter) is a feature-sparse regularization function
since {v âˆˆ Rd | âˆƒj âˆˆ [d âˆ’ 1] : 0 âˆˆ [âˆ‚Ïâˆ—(v)]j} = {v âˆˆ
Rd | âˆƒj âˆˆ [d âˆ’ 1] :
|vj| â‰¤ Î», vd = 0}. See Section 4
for examples of using them.

3 Distributionally Robust Safe

Screening

1For â„“y(t), we assume that only t is a variable of the function
(y is assumed to be a constant) when we take its subgradient
or convex conjugate.

In this section we show DRSS rules for weighted
RERM with two steps. First, in Sections 3.1 and

4

3.2, we show SS rules for weighted RERM but not
DR setup. To do this, we extended existing SS rules
in [13, 15]. Then we derive DRSS rules in Section 3.3.

3.1 (Non-DR) Safe Sample Screening

3.2 (Non-DR) Safe Feature Screening

We consider identifying j âˆˆ [d] such that Î²âˆ—(w)
= 0,
that is, identifying that the jth feature is not used in
the prediction, even when the sample weights w are
changed.

j

i

We consider identifying training samples that do not
affect the training result Î²âˆ—(w). Due to the relation-
ship (4), if there exists i âˆˆ [n] such that Î±âˆ—(w)
= 0,
then the ith row (sample) in Ë‡X does not affect Î²âˆ—(w).
However, since computing Î±âˆ—(w) is as costly as Î²âˆ—(w),
it is difficult to use the relationship as it is. To solve
the problem, the SsS first considers identifying the
possible region Bâˆ—(w) âŠ‚ Rd such that Î²âˆ—(w) âˆˆ Bâˆ—(w)
is assured. Then, with Bâˆ—(w) and (5), we can conclude
that the ith training sample do not affect the training
result Î²âˆ—(w) if (cid:83)

Î²âˆˆBâˆ—(w) âˆ‚â„“yi( Ë‡Xi:Î²) = {0}.

For simplicity, suppose that the regularization func-
tion Ï is decomposable, that is, Ï is represented as
Ï(Î²) := (cid:80)d
j=1 Ïƒj(Î²j) (Ïƒ1, Ïƒ2, . . . , Ïƒd: R â†’ R). Then,
since Ïâˆ—(v) = (cid:80)d
j (vj) and therefore [âˆ‚Ïâˆ—(v)]j =
âˆ‚Ïƒâˆ—

j (vj), from (4) we have

j=1 Ïƒâˆ—

Î²âˆ—(w)
j

where

âˆˆ âˆ‚Ïƒâˆ—

j ((Î³ âŠ— w âŠ— Ë‡X:j)âŠ¤Î±âˆ—(w))
j ( Ë‡Ë‡X (Î³,w)âŠ¤

Î±âˆ—(w)),

:j
:= Î³ âŠ— w âŠ— Ë‡X:j.

= âˆ‚Ïƒâˆ—
Ë‡Ë‡X (Î³,w)

:j

First we show how to compute Bâˆ—(w). In this paper
we adopt the computation methods that is available
when the regularization function Ï in Pw (and also
Pw itself) of (1) are strongly convex.

If we know Î±âˆ—(w), we can identify whether Î²âˆ—(w)
= 0
holds. However, like SsS (Section 3.1), we would like
to check the condition without computing Î±âˆ—(w) or
Î²âˆ—(w).

j

Lemma 3.1. Suppose that Ï in Pw (and also Pw
itself ) of (1) are Îº-strongly convex. Then, for any
Ë†Î² âˆˆ Rd and Ë†Î± âˆˆ Rn, we can assure Î²âˆ—(w) âˆˆ Bâˆ—(w) by
taking

Bâˆ—(w) :=

(cid:110)

Î²

(cid:12)
(cid:12) âˆ¥Î² âˆ’ Ë†Î²âˆ¥2 â‰¤ r(w, Î³, Îº, Ë†Î², Ë†Î±)
(cid:12)
(cid:114) 2
Îº

r(w, Î³, Îº, Ë†Î², Ë†Î±) :=

(cid:111)

,

[Pw( Ë†Î²) âˆ’ Dw( Ë†Î±)].

where

The proof is presented in Appendix A.3. The
amount Pw( Ë†Î²) âˆ’ Dw( Ë†Î±) is known as the duality gap,
which must be nonnegative due to (3). So we ob-
tain the following gap safe sample screening rule from
Lemma 3.1:

Lemma 3.2. Under the same assumptions as Lemma
3.1, Î±âˆ—(w)
= 0 is assured (i.e., the ith training sample
i
does not affect the training result Î²âˆ—(w)) if there exists
Ë†Î² âˆˆ Rd and Ë†Î± âˆˆ Rn such that

[ Ë‡Xi:

Ë†Î² âˆ’ âˆ¥ Ë‡Xi:âˆ¥2r(w, Î³, Îº, Ë†Î², Ë†Î±),
Ë‡Xi:

Ë†Î² + âˆ¥ Ë‡Xi:âˆ¥2r(w, Î³, Îº, Ë†Î², Ë†Î±)] âŠ† Z[â„“yi].

The proof is presented in Appendix A.4.

So, like SsS, SfS first considers identifying the pos-
sible region Aâˆ—(w) âŠ‚ Rn such that Î±âˆ—(w) âˆˆ Aâˆ—(w) is
assured. Then we can conclude that Î²âˆ—(w)
= 0 is
assured if (cid:83)

j ( Ë‡Ë‡X (Î³,w)âŠ¤
:j
Then we show how to compute Aâˆ—(w). With Lemma
A.3, we can calculate Aâˆ—(w) as follows, if the loss
function â„“y in Pw of (1) is smooth:

Î±âˆˆAâˆ—(w) âˆ‚Ïƒâˆ—

Î±) = {0}.

j

Lemma 3.3. Suppose that â„“y in Pw of (1) is Âµ-
smooth. Then, for any Ë†Î² âˆˆ Rd and Ë†Î± âˆˆ Rn, we can
assure Î±âˆ—(w) âˆˆ Aâˆ—(w) by taking

Aâˆ—(w) :=

(cid:110)

Î±

(cid:12)
(cid:111)
(cid:12) âˆ¥Î± âˆ’ Ë†Î±âˆ¥2 â‰¤ Â¯r(w, Î³, Âµ, Ë†Î², Ë†Î±)
(cid:12)

,

where

Â¯r(w, Î³, Âµ, Ë†Î², Ë†Î±) :=
(cid:115)

2Âµ
miniâˆˆ[n] wiÎ³2
i

[Pw( Ë†Î²) âˆ’ Dw( Ë†Î±)].

The proof is presented in Appendix A.5. Similar to
Lemma 3.2, we obtain the gap safe feature screening
rule from Lemma 3.3:

Lemma 3.4. Under the same assumptions as Lemma
3.3, Î²âˆ—(w)
= 0 is assured (i.e., the jth feature does
j

5

not affect prediction results) if there exists Ë†Î² âˆˆ Rd
and Ë†Î± âˆˆ Rn such that

[ Ë‡Ë‡X (Î³,w)âŠ¤

:j

Ë†Î± âˆ’ âˆ¥ Ë‡Ë‡X (Î³,w)

:j

âˆ¥2Â¯r(w, Î³, Âµ, Ë†Î², Ë†Î±),

Ë‡Ë‡X (Î³,w)âŠ¤

:j

Ë†Î± + âˆ¥ Ë‡Ë‡X (Î³,w)

:j

âˆ¥2Â¯r(w, Î³, Âµ, Ë†Î², Ë†Î±)] âŠ† Z[Ïƒâˆ—
j ].

The proof is almost same as Lemma 3.2.

3.3 Application to Distributionally Ro-

bust Setup

In Sections 3.1 and 3.2 we showed the conditions when
samples or features are screened out. In this section
we show how to use the conditions for the change of
sample weights w.

Definition 3.5 (weight-changing safe screening
(WCSS)). Given X âˆˆ RnÃ—d, y âˆˆ Rn, Ëœw âˆˆ Rn
â‰¥0 and
w âˆˆ Rn
â‰¥0, suppose that Î²âˆ—( Ëœw) in Definition 2.1 (and
also Î±âˆ—( Ëœw)) are already computed, but Î²âˆ—(w) not.
Then WCSsS (resp. WCSfS) from Ëœw to w is de-
fined as finding i âˆˆ [n] satisfying Lemma 3.2 (resp.
j âˆˆ [d âˆ’ 1] satisfying Lemma 3.4).

Definition 3.6 (Distributionally robust safe screening
(DRSS)). Given X âˆˆ RnÃ—d, y âˆˆ Rn, Ëœw âˆˆ Rn
â‰¥0 and
W âŠ‚ Rn
â‰¥0, suppose that Î²âˆ—( Ëœw) in Definition 2.1 (and
also Î±âˆ—( Ëœw)) are already computed. Then the DRSsS
(resp. DRSfS) for W is defined as finding i âˆˆ [n]
satisfying Lemma 3.2 (resp. j âˆˆ [d âˆ’ 1] satisfying
Lemma 3.4) for any w âˆˆ W.

For Definition 3.5, we have only to apply SS rules
in Lemma 3.2 or 3.4 by setting Ë†Î² â† Î²âˆ—( Ëœw) and Ë†Î± â†
Î±âˆ—( Ëœw). On the other hand, for Definition 3.6, we need
to maximize or minimize the interval in Lemma 3.2
or 3.4 in w âˆˆ W.

Theorem 3.7. The DRSsS rule for W is calculated
as:

[ Ë‡Xi:Î²âˆ—( Ëœw) âˆ’ âˆ¥ Ë‡Xi:âˆ¥2R, Ë‡Xi:Î²âˆ—( Ëœw) + âˆ¥ Ë‡Xi:âˆ¥2R] âŠ† Z[â„“yi],

where R := maxwâˆˆW r(w, Î³, Îº, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw)).

6

Similarly, the DRSfS rule for W is calculated as:

[L âˆ’ N R, L + N R] âŠ† Z[Ïƒâˆ—
Ë‡Ë‡X (Î³,w)âŠ¤

L := min
wâˆˆW

:j

j ], where

L := max
wâˆˆW

Ë‡Ë‡X (Î³,w)âŠ¤

:j

Î±âˆ—( Ëœw) = min
wâˆˆW
Î±âˆ—( Ëœw) = max
wâˆˆW

(Î³ âŠ— Ë‡X:j âŠ— Î±âˆ—( Ëœw))âŠ¤w,

(Î³ âŠ— Ë‡X:j âŠ— Î±âˆ—( Ëœw))âŠ¤w,

N := max
wâˆˆW

R := max
wâˆˆW

âˆ¥ Ë‡Ë‡X (Î³,w)

:j

âˆ¥2 =

(cid:114)

max
wâˆˆW

âˆ¥w âŠ— Î³ âŠ— Ë‡X:jâˆ¥2
2,

Â¯r(w, Î³, Âµ, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw)).

Thus, solving the maximizations and/or minimiza-
tions in Theorem 3.7 provides DRSsS and DRSfS
rules. However, how to solve it largely depends on the
choice of â„“, Ï and W. In Section 4 we show specific
calculations of Theorem 3.7 for some typical setups.

4 DRSS for Typical ML Setups

In this section we show DRSS rules derived in Section
3.3 for two typical ML setups: DRSsS for L1-loss L2-
regularized SVM (Section 4.1) and DRSfS for L2-loss
L1-regularized SVM (Section 4.2) under W := {w |
âˆ¥w âˆ’ Ëœwâˆ¥2 â‰¤ S}.

In the processes, we need to solve constrained max-
imizations of convex functions. Although maximiza-
tions of convex functions are not easy in general (min-
imizations are easy), we show that the maximizations
need in the processes can be algorithmically solved in
Section 4.3.

4.1 DRSsS for L1-loss L2-regularized

SVM

L1-loss L2-regularized SVM is a sample-sparse model
for binary classification (y âˆˆ {âˆ’1, +1}n) that satisfies
the preconditions to apply SsS (Lemma 3.1). Detailed
calculations are presented in Appendix B.1.

For L1-loss L2-regularized SVM, we set Ï and â„“ as:

Ï(Î²) :=

Î»
2

âˆ¥Î²âˆ¥2
2

(Î» > 0 : hyperparameter),

â„“y(t) := max{0, 1 âˆ’ t}

(where y âˆˆ {âˆ’1, +1}).

Then Ï is Î»-strongly convex. Setting Î³ = 1n, the dual
objective function is described as

4.2 DRSfS for L2-loss L1-regularized

SVM

Dw(Î±) =
ï£±
(cid:80)n
ï£²

ï£³

âˆ’âˆž.

i=1 wiÎ±i âˆ’ 1

2Î» Î±âŠ¤(wÃ—â–¡ Ë‡X)(wÃ—â–¡ Ë‡X)âŠ¤Î±,
(âˆ€i âˆˆ [n] : 0 â‰¤ Î±i â‰¤ 1)
(otherwise)

L2-loss L1-regularized SVM is a feature-sparse model
for binary classification (y âˆˆ {âˆ’1, +1}n) that satisfies
the preconditions to apply SfS (Lemma 3.3). Detailed
calculations are presented in Appendix B.2.

(6)

Here, in the viewpoint of minimization, we may con-
sider this problem as a maximization with the con-
straint â€œâˆ€i âˆˆ [n] : 0 â‰¤ Î±i â‰¤ 1â€.

Optimality conditions (4) and (5) are described as:

Î²âˆ—(w) =

1
Î»

(wÃ—â–¡ Ë‡X)âŠ¤Î±âˆ—(w),

âˆ€i âˆˆ [n] : Î±âˆ—(w)

i

âˆˆ

(7)

(8)

( Ë‡Xi:Î²âˆ—(w) â‰¤ 1)
( Ë‡Xi:Î²âˆ—(w) = 1)
( Ë‡Xi:Î²âˆ—(w) â‰¥ 1)

ï£±
ï£´ï£²

ï£´ï£³

{1},
[0, 1],
{0}.

Noticing that Z[â„“yi] = (1, +âˆž), by Theorem 3.7,

the DRSsS rule for W is calculated as:

Ë‡Xi:Î²âˆ—( Ëœw) âˆ’ âˆ¥ Ë‡Xi:âˆ¥2 max
wâˆˆW

r(w, Î³, Îº, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw)) > 1,

where

r(w, Î³, Îº, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw))

(cid:114) 2
Îº

:=

[Pw(Î²âˆ—( Ëœw)) âˆ’ Dw(Î±âˆ—( Ëœw))],

Pw(Î²âˆ—( Ëœw)) âˆ’ Dw(Î±âˆ—( Ëœw))

n
(cid:88)

:=

wi[â„“yi( Ë‡Xi:Î²âˆ—( Ëœw)) âˆ’ Î±âˆ—( Ëœw)

i

] + Î»âˆ¥Î²âˆ—( Ëœw)âˆ¥2
2

i=1
1
2Î»

+

wâŠ¤(Î±âˆ—( Ëœw)Ã—â–¡ Ë‡X)(Î±âˆ—( Ëœw)Ã—â–¡ Ë‡X)âŠ¤w.

Here, we can find that Pw(Î²âˆ—( Ëœw))âˆ’Dw(Î±âˆ—( Ëœw)), which
we need to maximize in reality, is the sum of linear
function and convex quadratic function with respect
to w âˆˆ W. (Since (Î±âˆ—( Ëœw)Ã—â–¡ Ë‡X)(Î±âˆ—( Ëœw)Ã—â–¡ Ë‡X)âŠ¤ is positive
semidefinite, we know that it is convex quadratic). Al-
though constrained maximization of a convex function
is difficult in general, for this case we can algorithmi-
cally maximize it (Section 4.3).

For L2-loss L1-regularized SVM, we set Ïƒj (and

consequently Ï) and â„“ as:

âˆ€j âˆˆ [d âˆ’ 1] : Ïƒj(Î²j) := Î»|Î²j|
Ïƒd(Î²d) := 0,
â„“y(t) := (max{0, 1 âˆ’ t})2

(where y âˆˆ {âˆ’1, +1}).

(Î» > 0 : hyperparameter),

Notice that Ïƒd(Î²d) is not defined as Î»|Î²d| but 0: we
rarely regularize the intercept with L1-regularization.

Setting Î³ = Î»1n, the dual objective function is

described as

Dw(Î±) =

(cid:40)

âˆ’Î» (cid:80)n
âˆ’âˆž,

i=1 wi

Î»Î±2

i âˆ’4Î±i
4

,

((11)â€“(13) are met)
(otherwise)

(9)

where Î±i â‰¥ 0,

|(w âŠ— Ë‡X:j)âŠ¤Î±| â‰¤ 1,
âˆ€j âˆˆ [d âˆ’ 1] :
(w âŠ— Ë‡X:d)âŠ¤Î± = (w âŠ— y)âŠ¤Î± = 0.

(10)

(11)

(12)

(13)

Optimality conditions (4) and (5) are described as

âˆ€j âˆˆ [d âˆ’ 1] :

|(w âŠ— Ë‡X:j)âŠ¤Î±âˆ—(w)| < 1 â‡’ Î²âˆ—(w)

j

= 0,

(14)

âˆ€i âˆˆ [n] : Î±âˆ—(w)

i

=

2
Î»

max{0, 1 âˆ’ Ë‡Xi:Î²âˆ—(w)}.

(15)

Noticing that Z[Ïƒâˆ—

j ] = (âˆ’Î», Î»), by Theorem 3.7, the

DRSfS rule for W is calculated as:

L âˆ’ N R > âˆ’Î», L + N R < Î»,

7

where

problems:

L := Î» min
wâˆˆW

( Ë‡X:j âŠ— Î±âˆ—( Ëœw))âŠ¤w,

( Ë‡X:j âŠ— Î±âˆ—( Ëœw))âŠ¤w,

L := Î» max
wâˆˆW
(cid:114)

N := Î»

max
wâˆˆW

âˆ¥w âŠ— Ë‡X:jâˆ¥2
2

(cid:114)

= Î»

max
wâˆˆW

{wâŠ¤diag( Ë‡X:j âŠ— Ë‡X:j)w},

R := max
wâˆˆW

Â¯r(w, Î³, Âµ, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw)),

Â¯r(w, Î³, Âµ, Î²âˆ—( Ëœw), Î±âˆ—( Ëœw))

(cid:115)

:=

2Âµ
miniâˆˆ[n] wiÎ³2
i

[Pw(Î²âˆ—( Ëœw)) âˆ’ Dw(Î±âˆ—( Ëœw))],

Pw(Î²âˆ—( Ëœw)) âˆ’ Dw(Î±âˆ—( Ëœw))

n
(cid:88)

(cid:34)
â„“yi( Ë‡Xi:Î²âˆ—( Ëœw)) + Î»

wi

=

i=1
+ Ï(Î²âˆ—( Ëœw)).

i âˆ’ 4Î±âˆ—( Ëœw)
Î»(Î±âˆ—( Ëœw))2
4

i

Here, the expressions in L and L are linear with
respect to w, and the expression in N inside the square
root is convex and quadratic with respect to w. Also,
2Âµ
R is decomposed to two maximizations
miniâˆˆ[n] wiÎ³2
i
and Pw(Î²âˆ—(w)) âˆ’ Dw(Î±âˆ—(w)), where the former is
easily computed while the latter is linear with respect
to w. So, similar to L1-loss L2-regularized SVM, we
can obtain the maximization result by maximizing
or minimizing the linear terms by Lemma A.4 in
Appendix A, and maximizing the convex quadratic
function by the method of Section 4.3.

4.3 Maximizing Linear and Convex
Quadratic Functions in Hyperball
Constraint

To derive DRSS rules of Sections 4.1 and 4.2, we
need to compute the following forms of optimization

wâŠ¤Aw + 2bâŠ¤w,

max
wâˆˆW
where W := {w âˆˆ Rn | âˆ¥w âˆ’ Ëœwâˆ¥2 â‰¤ S},

(16)

b âˆˆ Rn,

Ëœw âˆˆ Rn,
A âˆˆ RnÃ—n : symmetric, positive semidefinite,

nonzero.

Lemma 4.1. The maximization (16) is achieved by
the following procedure. First, we define Q âˆˆ RnÃ—n
and Î¦ := diag(Ï•1, Ï•2, . . . , Ï•n) as the eigendecompo-
sition of A such that A = QâŠ¤Î¦Q, Q is orthogonal
(QQâŠ¤ = QâŠ¤Q = I). Also, let Î¾ := âˆ’Î¦Q Ëœwâˆ’Qb âˆˆ Rn,
and

T (Î½) =

n
(cid:88)

(cid:18) Î¾i

Î½ âˆ’ Ï•i

i=1

(cid:19)2

.

(17)

(cid:35)

Then, the maximization (16) is equal to the largest
value among them:

â€¢ For each Î½ such that T (Î½) = S2 (see Lemma 4.2),
the value Î½S2 + (Î½ Ëœw + b)âŠ¤QâŠ¤(Î¦ âˆ’ Î½I)âˆ’1Î¾ + bâŠ¤ Ëœw,
and

â€¢ For each Î½ âˆˆ {Ï•1, Ï•2, . . . , Ï•n} (duplication re-
moved) such that â€œâˆ€i âˆˆ [n] : Ï•i = Î½ â‡’ Î¾i = 0â€,
the value

[Î½S2 + (Î½ Ëœw + b)âŠ¤QâŠ¤Ï„ + bâŠ¤ Ëœw],

max
Ï„ âˆˆRn

subject to âˆ€i âˆˆ FÎ½ :

Ï„i =

,

Î¾i
Ï•i âˆ’ Î½
(cid:88)
Ï„ 2
i ,

iâˆˆFÎ½

(cid:88)

iâˆˆUÎ½

i = S2 âˆ’
Ï„ 2

where UÎ½ := {i | i âˆˆ [n], Ï•i = Î½}, FÎ½ := [n] \ UÎ½.

(Note that the maximization is easily computed
by Lemma A.4.)

The proof is presented in Appendix A.6.

Lemma 4.2. Under the same definitions as Lemma
4.1, The equation T (Î½) = S2 can be solved by the
following procedure: Let e := [e1, e2, . . . , eN ] (N â‰¤ n,
k Ì¸= kâ€² â‡’ ek Ì¸= ekâ€²) be a sequence of indices such that

8

Figure 2: An example of the expression T (Î½) (black
solid line) in Lemmas 4.1 and 4.2. Colored dash
lines denote terms in the summation (Î¾ek /(Î½ âˆ’ Ï•ek ))2.
We can see that, given an interval (Ï•ek , Ï•ek+1) (k âˆˆ
[N âˆ’ 1]), the function is convex.

1. ek âˆˆ [n] for any k âˆˆ [N ],

2. i âˆˆ [n] is included in e if and only if Î¾i Ì¸= 0, and

3. Ï•e1 â‰¤ Ï•e2 â‰¤ Â· Â· Â· â‰¤ Ï•eN .

Note that, if Ï•ek < Ï•ek+1 (k âˆˆ [N âˆ’ 1]), then T (Î½)
is a convex function in the interval (Ï•ek , Ï•ek+1) with
limÎ½â†’Ï•ek +0 = limÎ½â†’Ï•ek+1 âˆ’0 = +âˆž. Then, unless
N = 0, each of the following intervals contains just
one solution of T (Î½) = S2:

â€¢ Intervals (âˆ’âˆž, Ï•e1) and (Ï•eN , +âˆž).

â€¢ Let Î½#(k) := argminÏ•ek <Î½<Ï•ek+1

T (Î½). For each

k âˆˆ [N âˆ’ 1] such that Ï•ek < Ï•ek+1,

â€“ intervals (Ï•ek , Î½#(k)) and (Î½#(k), Ï•ek+1) if

T (Î½#(k)) < S2,

â€“ interval

[Î½#(k), Î½#(k)]

(i.e.,

point)

if

T (Î½#(k)) = S2.

Figure 3: Concept of how to apply SS for deep learning.
SS is applied to the last layer for the final prediction.

5 Application to Deep Learning

So far, our discussion of SS rules has primarily focused
on ML models with linear predictions and convex loss
and regularization functions. However, there may be
scenarios where we would like to employ more complex
ML models, such as deep learning (DL).

For DL models, deriving SS rules for the entire
model can be challenging due to the complexity of
bounding the change in model parameters against
changes in sample weights. However, we can simplify
the process by focusing on the fact that each layer of
DL is often represented as a convex function. There-
fore, we propose applying SS rules specifically to the
last layer of DL models.

In this formulation, the layers preceding the last one
are considered as a fixed feature extraction process,
even when the sample weights change (see Figure
3). We believe that this approach is valid when the
change in sample weights is not significant. We plan
to experimentally evaluate the effectiveness of this
formulation in Section 6.3.

6 Numerical Experiment

It follows that T (Î½) = S2 has at most 2n solutions.

6.1 Experimental Settings

By Lemma 4.2, in order to compute the solution of
T (Î½) = S2, we have only to compute Î½#(k) by Newton
method or the like, and to compute the solution for
each interval by Newton method or the like. We show
an example of T (Î½) in Figure 2, and the proof in
Appendix A.7.

We evaluate the performances of DRSsS and DRSfS
across different values of acceptable weight changes
S and hyperparameters for regularization strength Î».
Performance is measured using safe screening rates,
representing the ratio of screened samples or features
to all samples or features. We consider three setups:

9

ðœ™ð‘’1ðœ™ð‘’2ðœ™ð‘’3ðœ™ð‘’4ðœ™ð‘’ð‘ðœˆð‘†2Final prediction (Convex function)Feature extraction(Assumed to be fixed afterinitial learning)ð‘¥1ð‘¥2ð‘¥ð‘‘ð‘¦DRSsS with L1-loss L2-regularized SVM (Section 4.1),
DRSfS with L2-loss L1-regularized SVM (Section 4.2),
and DRSsS with deep learning (Section 5) where
the last layer incorporates DRSsS with L1-loss L2-
regularized SVM.

In these experiments, we set initialize the sample
weights before change ( Ëœw) as Ëœw = 1n. Then, we set
S in DRSS for W := {w | âˆ¥w âˆ’ Ëœwâˆ¥2 â‰¤ S} (Section
4) as follows:

â€¢ First we assume the weight change that the
weights for positive samples ({i | yi = +1}) from
1 to a, while retaining the weights for negative
samples ({i | yi = âˆ’1}) as 1.

â€¢ Then, we defined S as the size of weight change
n+|a âˆ’ 1|
above; specifically, we set S =
(n+: number of positive samples in the train-
ing dataset).

âˆš

We vary a within the range 0.9 â‰¤ a â‰¤ 1.1, assuming
a maximum change of up to 10% per sample weight.

6.2 Relationship between the Weight
Changes and Safe Screening Rate

First, we present safe screening rates for two SVM
setups. The datasets used in these experiments are
detailed in Table 2.
In this experiment, we adapt
the regularization hyperparameter Î» based on the
characteristics of the data. These details are described
in Appendix D.1.

As an example, for the â€œsonarâ€ dataset, we show
the DRSsS result in Figure 4 and the DRSfS result in
Figure 5. Results for other datasets are presented in
Appendix D.2.

These plots allow us to assess the tolerance for
changes in sample weights. For instance, with a = 0.98
(weight of each positive sample is reduced by two per-
cent, or equivalent weight change in L2-norm), the
sample screening rate is 0.31 for L1-loss L2-regularized
SVM with Î» = 6.58e + 1, and the feature screen-
ing rate is 0.29 for L2-loss L1-regularized SVM with
Î» = 3.47e + 1. This implies that, even if the weights
are changed in such ranges, a number of samples or
features are still identified as redundant in the sense
of prediction.

Table 2: Datasets for DRSsS/DRSfS experiments.
All are binary classification datasets from LIBSVM
dataset [22]. The mark â€  denotes datasets with one
feature removed due to computational constraints.
See Appendix D.1 for details.

Task
DRSsS

Name

australian
breast-cancer
heart
ionosphere
sonar
splice (train)
svmguide1 (train)

DRSsS madelon (train)
sonar
splice (train)

n
690
683
270
351
208
1000
3089
2000
208
1000

n+
307
239
120
225
97
517
2000
1000
97
517

d
15
11
14
35
61
61
5
â€  500
â€  60
61

6.3 Safe Sample Screening for Deep

Learning Model

We applied DRSsS to DL models (Section 5), assuming
that all layers are fixed except for the last layer.

We utilized a neural network architecture compris-
ing the following components: firstly, ResNet50 [23]
with an output of 2,048 features, followed by a fully
connected layer to reduce the features to 10, and
finally, L1-loss L2-regularized SVM (Section 4.1) ac-
companied by the intercept feature (Remark 2.2).

For the experiment, we employed the CIFAR-10
dataset [24], a well-known benchmark dataset for
image classification tasks. We configured the net-
work to classify images into two classes: â€œairplaneâ€
and â€œautomobileâ€. Given that there are 5,000 im-
ages for each class, we split the dataset into train-
ing:validation:testing=6:2:2, resulting in a total of
6,000 images in the training dataset.

The resulting safe sample screening rates are illus-
trated in Figure 6. We observed similar outcomes to
those obtained with ordinary SVMs in Section 6.2.

This experiment validates the feasibility of apply-
ing DRSsS to DL models, demonstrating consistent
results with traditional SVM setups.

10

7 Conclusion

In this paper, we discussed DR-SS, considering the
possible changes in sample weights to represent DR
setup. We developed a method for calculating SS
that can handle changes in sample weights by intro-
ducing nontrivial computational techniques, such as
constrained maximization of certain convex functions
(Section 4.3). Additionally, to address the constraint
of SS, which typically applies to ML by minimizing
convex functions, we provided an application to DL
by applying SS to the last layer of DL model. While
this approach is an approximation, it holds certain
validity.

For the future work, we aim to explore different
environmental changes. In this paper, we focused on
weight constraint by L2-norm âˆ¥w âˆ’ Ëœwâˆ¥2 â‰¤ S (Section
4) due to computational considerations. However,
when interpreting changes in weights, the constraint
of L1-norm âˆ¥w âˆ’ Ëœwâˆ¥1 â‰¤ S may be more appropri-
ate, as it reflects changes in weights by altering the
number of samples. Furthermore, in the context of
DR-SS for DL, we are interested in loosening the
constraint of fixing the network except for the last
layer. Investigating this aspect could provide valuable
insights into the flexibility of DR-SS methodologies
in DL applications.

Software and Data

The code and the data to reproduce the experiments
are available as the attached file.

Potential Broader Impact

This paper contributes to machine learning in dynam-
ically changing environments, a scenario increasingly
prevalent in real-world data analyses. We believe
that, in such situations, ensuring prediction perfor-
mance against environmental changes and minimizing
storage requirements for expanding datasets will be
beneficial. The method does not present significant
ethical concerns or foreseeable societal consequences
because this work is theoretical and, as of now, has

no direct applications that might impact society or
ethical considerations.

Acknowledgements

This work was partially supported by MEXT KAK-
ENHI (20H00601), JST CREST (JPMJCR21D3 in-
cluding AIP challenge program, JPMJCR22N2), JST
Moonshot R&D (JPMJMS2033-05), JST AIP Acceler-
ation Research (JPMJCR21U2), NEDO (JPNP18002,
JPNP20006) and RIKEN Center for Advanced Intel-
ligence Project.

References

[1] Ruidi Chen and Ioannis Ch. Paschalidis. Dis-
tributionally robust learning. arXiv Preprint,
2021.

[2] Laurent El Ghaoui, Vivian Viallon, and Tarek
Rabbani. Safe feature elimination for the lasso
and sparse supervised learning problems. Pacific
Journal of Optimization, 8(4):667â€“698, 2012.

[3] Kohei Ogawa, Yoshiki Suzuki, and Ichiro
Takeuchi. Safe screening of non-support vectors
in pathwise svm computation. In Proceedings of
the 30th International Conference on Machine
Learning, pages 1382â€“1390, 2013.

[4] Hidetoshi Shimodaira. Improving predictive in-
ference under covariate shift by weighting the
log-likelihood function. Journal of statistical plan-
ning and inference, 90(2):227â€“244, 2000.

[5] Masashi Sugiyama, Matthias Krauledat, and
Klaus-Robert MÂ¨uller. Covariate shift adaptation
by importance weighted cross validation. Journal
of Machine Learning Research, 8(35):985â€“1005,
2007.

[6] C. Cortes and V. Vapnik. Support-vector net-
works. Machine Learning, 20:273â€“297, 1995.

[7] Robert Tibshirani. Regression shrinkage and
selection via the lasso. Journal of the Royal Sta-

11

tistical Society Series B: Statistical Methodology,
58(1):267â€“288, 1996.

[8] Joel Goh and Melvyn Sim. Distributionally
robust optimization and its tractable approxi-
mations. Operations Research, 58(4-1):902â€“917,
2010.

[9] Erick Delage and Yinyu Ye. Distributionally
robust optimization under moment uncertainty
with application to data-driven problems. Oper-
ations Research, 58(3):595â€“612, 2010.

[10] Liyuan Wang, Xingxing Zhang, Kuo Yang,
Longhui Yu, Chongxuan Li, Lanqing HONG,
Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun
Zhu. Memory replay with data compression for
continual learning. In International Conference
on Learning Representations, 2022.

[11] James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago
Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran,
and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the Na-
tional Academy of Sciences, 114(13):3521â€“3526,
2017.

[12] Olivier Fercoq, Alexandre Gramfort, and Joseph
Salmon. Mind the duality gap: safer rules for the
lasso. In Proceedings of the 32nd International
Conference on Machine Learning, pages 333â€“342,
2015.

[13] Eugene Ndiaye, Olivier Fercoq, Alexandre Gram-
fort, and Joseph Salmon. Gap safe screening rules
for sparse multi-task and multi-class models. In
Advances in Neural Information Processing Sys-
tems, pages 811â€“819, 2015.

[14] Shota Okumura, Yoshiki Suzuki, and Ichiro
Takeuchi. Quick sensitivity analysis for incre-
mental data modification and its application to
leave-one-out cv in linear classification problems.
In Proceedings of the 21th ACM SIGKDD In-
ternational Conference on Knowledge Discovery
and Data Mining, pages 885â€“894, 2015.

[15] Atsushi Shibagaki, Masayuki Karasuyama, Ko-
hei Hatano, and Ichiro Takeuchi. Simultaneous
safe screening of features and samples in doubly
sparse modeling. In International Conference on
Machine Learning, pages 1577â€“1586, 2016.

[16] Kazuya Nakagawa, Shinya Suzumura, Masayuki
Karasuyama, Koji Tsuda, and Ichiro Takeuchi.
Safe pattern pruning: An efficient approach for
predictive pattern mining. In Proceedings of the
22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1785â€“1794. ACM, 2016.

[17] Shaogang Ren, Shuai Huang, Jieping Ye, and
Xiaoning Qian. Safe feature screening for gen-
eralized lasso. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 40(12):2992â€“
3006, 2018.

[18] Jiang Zhao, Yitian Xu, and Hamido Fujita. An
improved non-parallel universum support vec-
tor machine and its safe sample screening rule.
Knowledge-Based Systems, 170:79â€“88, 2019.

[19] Zhou Zhai, Bin Gu, Xiang Li, and Heng Huang.
Safe sample screening for robust support vector
machine. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 6981â€“
6988, 2020.

[20] Hongmei Wang and Yitian Xu. A safe double
screening strategy for elastic net support vec-
tor machine. Information Sciences, 582:382â€“397,
2022.

[21] Takumi Yoshida, Hiroyuki Hanada, Kazuya Nak-
agawa, Kouichi Taji, Koji Tsuda, and Ichiro
Takeuchi. Efficient model selection for predictive
pattern mining model by safe pattern pruning.
Patterns, 4(12):100890, 2023.

[22] Chih-Chung Chang and Chih-Jen Lin. Libsvm: A
library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology
(TIST), 2(3):27, 2011. Datasets are provided in
authorsâ€™ website: https://www.csie.ntu.edu.
tw/~cjlin/libsvmtools/datasets/.

12

[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
(CVPR), June 2016.

[24] Alex Krizhevsky. The cifar-10 dataset, 2009.

[25] Ralph Tyrell Rockafellar. Convex analysis.

Princeton university press, 1970.

[26] Jean-Baptiste Hiriart-Urruty

and Claude
LemarÂ´echal. Convex Analysis and Minimization
Algorithms II: Advanced Theory and Bundle
Methods. Springer, 1993.

Figure 4: Ratio of screened samples by DRSsS for
dataset â€œsonarâ€.

Figure 5: Ratio of screened features by DRSfS for
dataset â€œsonarâ€.

Figure 6: Ratio of screened samples by DRSsS
for dataset with CIFAR-10 dataset and DL model
ResNet50.

13

0.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.208=0.657=2.08=6.577=20.8=65.77=2080.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features=7.48e1=1.61e+0=3.47e+0=7.48e+0=1.61e+1=3.47e+1=7.48e+10.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=6.00e+0=1.90e+1=6.00e+1=1.90e+2=6.00e+2=1.90e+3=6.00e+3A Proofs

A.1 General Lemmas

Lemma A.1. For a convex function f : Rd â†’ R âˆª {+âˆž}, f âˆ—âˆ— is equivalent to f if f is convex, proper (i.e.,
âˆƒv âˆˆ Rd : f (v) < +âˆž) and lower-semicontinuous.

Proof. See Section 12 of [25] for example.

Lemma A.1 is known as Fenchel-Moreau theorem. Especially, Lemma A.1 holds if f is convex and

âˆ€v âˆˆ Rd : f (v) < +âˆž.

Lemma A.2. For a convex function f : Rd â†’ R âˆª {+âˆž},

â€¢ f âˆ— is (1/Î½)-strongly convex if f is proper and Î½-smooth.

â€¢ f âˆ— is (1/Îº)-smooth if f is proper, lower-semicontinuous and Îº-strongly convex.

Proof. See Section X.4.2 of [26] for example.

Lemma A.3. Suppose that f : Rd â†’ Râˆª{+âˆž} is a Îº-strongly convex function, and let vâˆ— = argminvâˆˆRd f (v)
be the minimizer of f . Then, for any v âˆˆ Rd, we have
(cid:114) 2
Îº

âˆ¥v âˆ’ vâˆ—âˆ¥2 â‰¤

[f (v) âˆ’ f (vâˆ—)].

Proof. See [13] for example.

Lemma A.4. For any vector a, c âˆˆ Rn and S > 0,

min
vâˆˆRn: âˆ¥vâˆ’câˆ¥2â‰¤S

aâŠ¤v = aâŠ¤c âˆ’ Sâˆ¥aâˆ¥2,

max
vâˆˆRn: âˆ¥vâˆ’câˆ¥2â‰¤S

aâŠ¤v = aâŠ¤c + Sâˆ¥aâˆ¥2.

Proof. By Cauchy-Schwarz inequality,

âˆ’ âˆ¥aâˆ¥2âˆ¥v âˆ’ câˆ¥2 â‰¤ aâŠ¤(v âˆ’ c) â‰¤ âˆ¥aâˆ¥2âˆ¥v âˆ’ câˆ¥2.

Noticing that the first inequality becomes equality if âˆƒÏ‰ > 0 : a = âˆ’Ï‰(v âˆ’ c), while the second inequality
becomes equality if âˆƒÏ‰â€² > 0 : a = Ï‰â€²(v âˆ’ c). Moreover, since âˆ¥v âˆ’ câˆ¥2 â‰¤ S,

âˆ’ Sâˆ¥aâˆ¥2 â‰¤ aâŠ¤(v âˆ’ c) â‰¤ Sâˆ¥aâˆ¥2

also holds, with the equality holds if âˆ¥v âˆ’ câˆ¥2 = S.

On the other hand, if we take v that satisfies both of the equality conditions of Cauchy-Schwarz inequality

above, that is,

â€¢ (for the first inequality being equality) v = c âˆ’ (S/âˆ¥aâˆ¥2)a,

â€¢ (for the second inequality being equality) v = c + (S/âˆ¥aâˆ¥2)a,

then the inequalities become equalities. This proves that âˆ’Sâˆ¥aâˆ¥2 and Sâˆ¥aâˆ¥2 are surely the minimum and
maximum of aâŠ¤(v âˆ’ c), respectively.

14

A.2 Derivation of Dual Problem by Fenchelâ€™s Duality Theorem

As the formulation of Fenchelâ€™s duality theorem, we follow the one in Section 31 of [25].

Lemma A.5 (A special case of Fenchelâ€™s duality theorem: f, g < +âˆž). Let f : Rn â†’ R and g : Rd â†’ R be
convex functions, and A âˆˆ RnÃ—d be a matrix. Moreover, we define

vâˆ— := min
vâˆˆRd
uâˆ— := max
uâˆˆRn

[f (Av) + g(v)],

[âˆ’f âˆ—(âˆ’u) âˆ’ gâˆ—(AâŠ¤u)].

Then Fenchelâ€™s duality theorem assures that

f (Avâˆ—) + g(vâˆ—) = âˆ’f âˆ—(âˆ’uâˆ—) âˆ’ gâˆ—(AâŠ¤uâˆ—),
âˆ’ uâˆ— âˆˆ âˆ‚f (Avâˆ—),
vâˆ— âˆˆ âˆ‚gâˆ—(AâŠ¤uâˆ—).

Sketch of the proof. Introducing a dummy variable Ïˆ âˆˆ Rn and a Lagrange multiplier u âˆˆ Rn, we have

(18)

(19)

(20)

min
vâˆˆRd

[f (Av) + g(v)] = max
uâˆˆRn

min
vâˆˆRd, ÏˆâˆˆRn

[f (Ïˆ) + g(v) âˆ’ uâŠ¤(Av âˆ’ Ïˆ)]

= âˆ’ min
uâˆˆRn

= âˆ’ min
uâˆˆRn

max
vâˆˆRd, ÏˆâˆˆRn
[f âˆ—(âˆ’u) + gâˆ—(AâŠ¤u)] = max
uâˆˆRn

[âˆ’f (Ïˆ) âˆ’ g(v) + uâŠ¤(Av âˆ’ Ïˆ)] = âˆ’ min
uâˆˆRn
[âˆ’f âˆ—(âˆ’u) âˆ’ gâˆ—(AâŠ¤u)].

max
vâˆˆRd, ÏˆâˆˆRn

[{(âˆ’u)âŠ¤Ïˆ âˆ’ f (Ïˆ)} + {(AâŠ¤u)âŠ¤v âˆ’ g(v)}]

(21)

Moreover, by the optimality condition of a problem with a Lagrange multiplier (20), the optima of it, denoted
by vâˆ—, Ïˆâˆ— and uâˆ—, must satisfy

Avâˆ— = Ïˆâˆ—, AâŠ¤uâˆ— âˆˆ âˆ‚g(vâˆ—), âˆ’uâˆ— âˆˆ âˆ‚f (Ïˆâˆ—) = âˆ‚f (Avâˆ—).

On the other hand, introducing a dummy variable Ï• âˆˆ Rd and a Lagrange multiplier v âˆˆ Rd for (21), we have

max
uâˆˆRn

[âˆ’f âˆ—(âˆ’u) âˆ’ gâˆ—(AâŠ¤u)] = min
vâˆˆRd

max
uâˆˆRn,Ï•âˆˆRd
[{(Av)âŠ¤(âˆ’u) âˆ’ f âˆ—(âˆ’u)} + {vâŠ¤Ï• âˆ’ gâˆ—(Ï•)}]

= min
vâˆˆRd

= min
vâˆˆRd

max
uâˆˆRn,Ï•âˆˆRd
[f âˆ—âˆ—(Av) + gâˆ—âˆ—(v)] = min
vâˆˆRd

[f (Av) + g(v)].

(âˆµ Lemma A.1)

[âˆ’f âˆ—(âˆ’u) âˆ’ gâˆ—(Ï•) âˆ’ vâŠ¤(AâŠ¤u âˆ’ Ï•)]

(22)

Likely above, by the optimality condition of a problem with a Lagrange multiplier (22), the optima of it,
denoted by uâˆ—, Ï•âˆ— and vâˆ—, must satisfy

AâŠ¤uâˆ— = Ï•âˆ—,

vâˆ— âˆˆ âˆ‚gâˆ—(Ï•âˆ—) = âˆ‚gâˆ—(AâŠ¤uâˆ—), Avâˆ— âˆˆ âˆ‚f (âˆ’uâˆ—).

Lemma A.6 (Dual problem of weighted regularized empirical risk minimization (weighted RERM)). For
the minimization problem

Î²âˆ—(w) := argmin
Î²âˆˆRd

Pw(Î²), where Pw(Î²) :=

n
(cid:88)

i=1

wiâ„“yi( Ë‡Xi:Î²) + Ï(Î²),

((1) restated)

15

we define the dual problem as the one obtained by applying Fenchelâ€™s duality theorem (Lemma A.5), which is
defined as

Î±âˆ—(w) := argmax

Î±âˆˆRn

Dw(Î±), where Dw(Î±) := âˆ’

n
(cid:88)

i=1

Moreover, Î²âˆ—(w) and Î±âˆ—(w) must satisfy

wiâ„“âˆ—
yi

(âˆ’Î³iÎ±i) + Ïâˆ—(((Î³ âŠ— w)Ã—â–¡ Ë‡X)âŠ¤Î±).

((2) restated)

Pw(Î²âˆ—(w)) = Dw(Î±âˆ—(w)),
Î²âˆ—(w) âˆˆ âˆ‚Ïâˆ—(((Î³ âŠ— w)Ã—â–¡ Ë‡X)âŠ¤Î±âˆ—(w)),
âˆ€i âˆˆ [n] : âˆ’Î³iÎ±âˆ—(w)

âˆˆ âˆ‚â„“yi( Ë‡Xi:Î²âˆ—(w)).

i

((3) restated)

((4) restated)

((5) restated)

Proof. To apply Fenchelâ€™s duality theorem, we have only to set f , g and A in Lemma A.5 as

f (u) :=

n
(cid:88)

i=1

wiâ„“yi(ui),

g(Î²) := Ï(Î²), A := Ë‡X.

Here, noticing that

f âˆ—(u) = sup
uâ€²âˆˆRn

[uâŠ¤uâ€² âˆ’

n
(cid:88)

i=1

wiâ„“yi(uâ€²

i)] = sup
uâ€²âˆˆRn

n
(cid:88)

i=1

[uiuâ€²

i âˆ’ wiâ„“yi(uâ€²

i)]

= sup
uâ€²âˆˆRn

n
(cid:88)

i=1

wi

(cid:20) ui
wi

uâ€²
i âˆ’ â„“yi(uâ€²
i)

(cid:21)

=

n
(cid:88)

i=1

wiâ„“âˆ—
yi

(cid:19)

,

(cid:18) ui
wi

from (19) we have

âˆ’f âˆ—(âˆ’u) âˆ’ gâˆ—(AâŠ¤u) = âˆ’

n
(cid:88)

i=1

wiâ„“âˆ—
yi

(cid:19)

(cid:18)

âˆ’

ui
wi

âˆ’ Ïâˆ—( Ë‡X âŠ¤u).

Replacing ui â† Î³iwiÎ±i, that is, u â† (Î³ âŠ— w âŠ— Î±), we have the dual problem (2).

The relationships between the primal and the dual problem are described as follows:
âˆ’ uâˆ— âˆˆ âˆ‚f (Avâˆ—) â‡’ âˆ’ Î³ âŠ— w âŠ— Î±âˆ—(w) âˆˆ âˆ‚f ( Ë‡XÎ²âˆ—(w)) â‡’ âˆ’ Î³iwiÎ±âˆ—(w)
â‡’ âˆ’Î³iÎ±âˆ—(w)
vâˆ— âˆˆ âˆ‚gâˆ—(AâŠ¤uâˆ—) â‡’ Î²âˆ—(w) âˆˆ âˆ‚gâˆ—( Ë‡X âŠ¤Î³ âŠ— w âŠ— Î±âˆ—(w)) = âˆ‚gâˆ—(((Î³ âŠ— w)Ã—â–¡ Ë‡X)âŠ¤Î±âˆ—(w)).

âˆˆ âˆ‚â„“yi( Ë‡Xi:Î²âˆ—(w)),

i

i

âˆˆ wiâˆ‚â„“yi( Ë‡Xi:Î²âˆ—(w))

A.3 Proof of Lemma 3.1

Proof. [13]

âˆ¥ Ë†Î² âˆ’ Î²âˆ—(w)âˆ¥2 â‰¤

=

â‰¤

(cid:114) 2
Î»
(cid:114) 2
Î»
(cid:114) 2
Î»

[Pw( Ë†Î²) âˆ’ Pw(Î²âˆ—(w))]

(âˆµ setting f â† Pw in Lemma A.3)

[Pw( Ë†Î²) âˆ’ Dw(Î±âˆ—(w))]

(âˆµ (3))

[Pw( Ë†Î²) âˆ’ Dw( Ë†Î±)].

(âˆµ Î±âˆ—(w) is a maximizer of Dw)

16

A.4 Proof of Lemma 3.2
Proof. Due to (5), if âˆ‚â„“yi( Ë‡Xi:Î²âˆ—(w)) = {0} is assured, then Î±âˆ—(w)
but know Bâˆ—(w) (Lemma 3.1), we can assure Î±âˆ—(w)
that âˆ‚â„“yi is monotonically increasing2, we have

i
= 0 if (cid:83)

i

= 0 is assured. Since we do not know Î²âˆ—(w)
Î²âˆˆBâˆ—(w) âˆ‚â„“yi( Ë‡Xi:Î²) = {0} is assured. Noticing

(cid:91)

âˆ‚â„“yi( Ë‡Xi:Î²) = {0} â‡”

(cid:91)

Î²âˆˆBâˆ—(w)

â‡”

(cid:104) Ë‡Xi:

Î²âˆˆBâˆ—(w)
Ë†Î² âˆ’ âˆ¥ Ë‡Xi:âˆ¥2r(w, Î³, Îº, Ë†Î², Ë†Î±), Ë‡Xi:

Ë‡Xi:Î² âŠ† Z[â„“yi] â‡” [ min
Î²âˆˆBâˆ—(w)

Ë‡Xi:Î², max
Î²âˆˆBâˆ—(w)

Ë‡Xi:Î²] âŠ† Z[â„“yi]

(cid:105)
Ë†Î² + âˆ¥ Ë‡Xi:âˆ¥2r(w, Î³, Îº, Ë†Î², Ë†Î±)

âŠ† Z[â„“yi].

(âˆµ Lemma A.4)

A.5 Proof of Lemma 3.3

Proof. The proof is almost the same as that for Lemma 3.1 (see Appendix A.3), but we additionally need to
show that âˆ’Dw is ((miniâˆˆ[n] wiÎ³2
i )/Âµ)-strongly convex (in this case Dw is called strongly concave).
As discussed in Lemma A.2, âˆ’â„“âˆ—
yi
(âˆ’Î³iÎ±i) âˆ’ (1/2Âµ)(Î³iÎ±i)2 is convex with respect to Î±i,

(t) is (1/Âµ)-strongly convex, that is, âˆ’â„“âˆ—
yi

(t) âˆ’ (1/2Âµ)t2 is convex. Thus,

â€¢ âˆ’â„“âˆ—
yi

i /2Âµ)Î±2

(âˆ’Î³iÎ±i) âˆ’ (wiÎ³2

â€¢ âˆ’wiâ„“âˆ—
yi
â€¢ âˆ’ (cid:80)n
(âˆ’Î³iÎ±i) âˆ’ (cid:80)n
i=1 wiâ„“âˆ—
yi
So, âˆ’ (cid:80)n
(âˆ’Î³iÎ±i) is convex with respect to Î± even subtracted by (cid:80)n
i=1 wiâ„“âˆ—
yi
i /Âµ)]âˆ¥Î±âˆ¥2
(1/2)[miniâˆˆ[n](wiÎ³2
2.

i is convex with respect to Î±i,

i is convex with respect to Î±.

i=1(wiÎ³2

i /2Âµ)Î±2

k=1[miniâˆˆ[n](wiÎ³2

i /2Âµ)]Î±2

k =

A.6 Proof of Lemma 4.1

Lemma A.7. For the optimization problem

wâŠ¤Aw + 2bâŠ¤w,

max
wâˆˆW
subject to W := {w âˆˆ Rn | âˆ¥w âˆ’ Ëœwâˆ¥2 â‰¤ S},
b âˆˆ Rn,
where

Ëœw âˆˆ Rn,
A âˆˆ RnÃ—n : symmetric, positive semidefinite, nonzero,

((16) restated)

its stationary points are obtained as the solution of the following equations with respect to w and Î½ âˆˆ R:

Aw + b âˆ’ Î½(w âˆ’ Ëœw) = 0,
âˆ¥w âˆ’ Ëœwâˆ¥2 = S.

(23)

(24)

Also, when both (23) and (24) are satisfied, the function to be maximized is calculated as

wâŠ¤Aw + 2bâŠ¤w = Î½S2 + (Î½ Ëœw + b)âŠ¤(w âˆ’ Ëœw) + ËœwâŠ¤b.
(25)
2Since âˆ‚â„“yi is a multi-valued function, the monotonicity must be defined accordingly: we call a multi-valued function

F : R â†’ 2R is monotonically increasing if, for any t < tâ€², F must satisfy â€œâˆ€s âˆˆ F (t), âˆ€sâ€² âˆˆ F (tâ€²): s â‰¤ sâ€²â€.

17

Proof. First, wâŠ¤Aw + 2bâŠ¤w is convex and not constant. Then we can show that (16) is optimized in
{w âˆˆ Rn | âˆ¥w âˆ’ Ëœwâˆ¥2 = S}, that is, at the surface of the hyperball W (Theorem 32.1 of [25]). This proves
(24). Moreover, with the fact, we write the Lagrangian function with Lagrange multiplier Î½ âˆˆ R as:

L(w, Î½) := wâŠ¤Aw + 2bâŠ¤w âˆ’ Î½(âˆ¥w âˆ’ Ëœwâˆ¥2

2 âˆ’ S2).

Then, due to the property of Lagrange multiplier, the stationary points of (16) are obtained as

âˆ‚L
âˆ‚w
âˆ‚L
âˆ‚Î½

= 2Aw + 2b âˆ’ 2Î½(w âˆ’ Ëœw) = 0,

= âˆ¥w âˆ’ Ëœwâˆ¥2

2 âˆ’ S2 = 0,

where the former derives (23).

Finally we show (25). If both (23) and (24) are satisfied,

wâŠ¤Aw + 2bâŠ¤w = wâŠ¤(Î½(w âˆ’ Ëœw) âˆ’ b) + 2bâŠ¤w

(âˆµ (23))

= Î½wâŠ¤(w âˆ’ Ëœw) + bâŠ¤w
= Î½(w âˆ’ Ëœw)âŠ¤(w âˆ’ Ëœw) + Î½ ËœwâŠ¤(w âˆ’ Ëœw) + bâŠ¤(w âˆ’ Ëœw) + bâŠ¤ Ëœw
= Î½S2 + Î½ ËœwâŠ¤(w âˆ’ Ëœw) + bâŠ¤(w âˆ’ Ëœw) + bâŠ¤ Ëœw
= Î½S2 + (Î½ Ëœw + b)âŠ¤(w âˆ’ Ëœw) + bâŠ¤ Ëœw

(âˆµ (24))

((25) restated)

Proof of Lemma 4.1. The condition (23) is calculated as

Aw + b = Î½(w âˆ’ Ëœw),
(A âˆ’ Î½I)(w âˆ’ Ëœw) = âˆ’A Ëœw âˆ’ b.

Here, let us apply eigendecomposition of A, denoted by A = QâŠ¤Î¦Q, where Q âˆˆ RnÃ—n is orthogonal
(QQâŠ¤ = QâŠ¤Q = I) and Î¦ := diag(Ï•1, Ï•2, . . . , Ï•n) is a diagonal matrix consisting of eigenvalues of A. Such a
decomposition is assured to exist since A is assumed to be symmetric and positive semidefinite. Then,

(QâŠ¤Î¦Q âˆ’ Î½I)(w âˆ’ Ëœw) = âˆ’QâŠ¤Î¦Q Ëœw âˆ’ b,
QâŠ¤(Î¦ âˆ’ Î½I)Q(w âˆ’ Ëœw) = âˆ’QâŠ¤Î¦Q Ëœw âˆ’ b,
(Î¦ âˆ’ Î½I)Ï„ = Î¾,

(where Ï„ := Q(w âˆ’ Ëœw),

âˆ€i âˆˆ [n] :

(Ï•i âˆ’ Î½)Ï„i = Î¾i.

Î¾ := âˆ’Î¦Q Ëœw âˆ’ Qb âˆˆ Rn, )

Note that we have to be also aware of the constraint

S = âˆ¥Ï„ âˆ¥2 =

âˆš

Ï„ âŠ¤Ï„ =

(cid:113)

(w âˆ’ Ëœw)âŠ¤QâŠ¤Q(w âˆ’ Ëœw) = âˆ¥w âˆ’ Ëœwâˆ¥2.

(26)

(27)

(28)

Here, we consider these two cases.

18

1. First, consider the case when (Î¦âˆ’Î½I) is nonsingular, that is, when Î½ is different from any of Ï•1, Ï•2, . . . , Ï•n.

Then, from (28) we have

S2 = âˆ¥Ï„ âˆ¥2 =

n
(cid:88)

i=1

Ï„ 2
i =

n
(cid:88)

(cid:18) Î¾i

(cid:19)2

Î½ âˆ’ Ï•i

i=1

(cid:0)=: T (Î½)(cid:1).

(29)

So, values of (16) for all stationary points with respect to w and Î½ (on condition that (Î¦ âˆ’ Î½I) is
nonsingular) can be obtained by computing (25) for each Î½ satisfying (29), that is,

â€¢ for such Î½ computing Ï„ by (27), and
â€¢ computing (25) as Î½S2 + (Î½ Ëœw + b)âŠ¤(w âˆ’ Ëœw) + bâŠ¤ Ëœw = Î½S2 + (Î½ Ëœw + b)âŠ¤QâŠ¤Ï„ + bâŠ¤ Ëœw.

2. Secondly, consider the case when (Î¦ âˆ’ Î½I) is nonsingular, that is, when Î½ is equal to one of Ï•1, Ï•2, . . . , Ï•n.
First, given Î½, let UÎ½ := {i | i âˆˆ [n], Ï•i = Î½} be the indices of {Ï•i}i equal to Î½ (this may include more
than one indices), and FÎ½ := [n] \ UÎ½. Note that, by assumption, UÎ½ is not empty. Then, all stationary
points of (16) with respect to w and Î½ (on condition that (Î¦ âˆ’ Î½I) is singular) can be found by computing
the followings for each Î½ âˆˆ {Ï•1, Ï•2, . . . , Ï•n} (duplication excluded):

â€¢ If Î¾i Ì¸= 0 for at least one i âˆˆ UÎ½, the equation (27) cannot hold.
â€¢ If Î¾i = 0 for all i âˆˆ NÎ½, the equation (27) may hold. So we calculate Ï„ that maximizes (16) as

follows:

â€“ Fix Ï„i = Î¾i/(Ï•i âˆ’ Î½) for i âˆˆ FÎ½.
â€“ Set the constraint (cid:80)
Ï„ 2
i (due to (28)).
â€“ Maximize (16) with respect to {Ï„i}iâˆˆUÎ½ under the constraints above. Here, by (25) we have

i = S2 âˆ’ (cid:80)
Ï„ 2

iâˆˆFÎ½

iâˆˆUÎ½

only to calculate

[Î½S2 + (Î½ Ëœw + b)âŠ¤(w âˆ’ Ëœw) + bâŠ¤ Ëœw],

max
Ï„ âˆˆRn

(30)

subject to âˆ€i âˆˆ FÎ½ :

Ï„i =

,

Î¾i
Ï•i âˆ’ Î½
(cid:88)
Ï„ 2
i ,

iâˆˆFÎ½

(cid:88)

iâˆˆUÎ½

i = S2 âˆ’
Ï„ 2

which is easily computed by Lemma A.4. The value of the maximization result is equal to that
of (16) on condition that Î½ is specified above.

So, collecting these result and taking the largest one, the maximization (on condition that (Î¦ âˆ’ Î½I)
is singular) is completed.

Taking the maximum of the two cases, we have the maximization result of (16).

A.7 Proof of Lemma 4.2

Proof. We show the statements in the lemma that, if Ï•ek < Ï•ek+1 (k âˆˆ [N âˆ’1]), then T (Î½) is a convex function
in the interval (Ï•ek , Ï•ek+1) with limÎ½â†’Ï•ek +0 = limÎ½â†’Ï•ek+1 âˆ’0 = +âˆž. Then the conclusion immediately follows.

19

The latter statement clearly holds. The former statement is proved by directly computing the derivative.

d
dÎ½

T (Î½) =

d
dÎ½

n
(cid:88)

(cid:18) Î¾i

(cid:19)2

n
(cid:88)

= âˆ’2

Î½ âˆ’ Ï•i

Î¾2
i
(Î½ âˆ’ Ï•i)3 .

i=1
It is an increasing function with respect to Î½, as long as Î½ does not match any of {Ï•i}n
So it is convex in the interval Ï•ek < Î½ < Ï•ek+1.

i=1

i=1 such that Î¾i Ì¸= 0.

B Detailed Calculations

In this appendix we describe detailed calculations omitted in the main paper.

B.1 Calculations for L1-loss L2-regularized SVM (Section 4.1)

For this setup, we can calculate as

Ïâˆ—(Î²) :=

1
2Î»

âˆ¥Î²âˆ¥2
2,

â„“âˆ—
y(t) :=

(cid:40)

t,
+âˆž,

(âˆ’1 â‰¤ t â‰¤ 0)
(otherwise)

âˆ‚Ïâˆ—(Î²) :=

(cid:27)

Î²

,

(cid:26) 1
Î»

âˆ‚â„“y(t) :=

ï£±
ï£´ï£²

ï£´ï£³

{âˆ’1},
[âˆ’1, 0],
{0}.

(t < 1)
(t = 1)
(t > 1)

Then we have the dual problem in the main paper (6).

B.2 Calculations for L2-loss L1-regularized SVM (Section 4.2)

For this setup, we can calculate as

Ïâˆ—(Î²) :=

(cid:40)

0,
+âˆž,

(Î²d = 0, âˆ€j âˆˆ [d âˆ’ 1] :
(otherwise)
ï£±

|Î²j| â‰¤ Î»)

â„“âˆ—
y(t) :=

(cid:40) t2+4t
4
+âˆž,

,

(t â‰¤ 0)
(otherwise)

âˆ€j âˆˆ [d âˆ’ 1] :

[âˆ‚Ïâˆ—(Î²)]j :=

ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³

âˆ’âˆž,
[âˆ’âˆž, 0],
0,
[0, +âˆž],
+âˆž,

(Î²j < âˆ’Î»)
(Î²j = âˆ’Î»)
(|Î²j| < Î»)
(Î²j = Î»)
(Î²j > Î»)

[âˆ‚Ïâˆ—(Î²)]d :=

ï£±
ï£´ï£²

ï£´ï£³

âˆ’âˆž,
[âˆ’âˆž, +âˆž],
+âˆž,

(Î²d < 0)
(Î²d = 0)
(Î²d > 0)

âˆ‚â„“y(t) := âˆ’2 max{0, 1 âˆ’ t}.

Then, setting Î³i = Î» for all i âˆˆ [n], the dual objective function is described as

Dw(Î±) =

where

(cid:40)

i=1 wi

âˆ’ (cid:80)n
+âˆž,

Î»2Î±2

i âˆ’4Î»Î±i

4

,

(if (32) are satisfied)
(otherwise)

Î»Î±i â‰¥ 0 â‡” Î±i â‰¥ 0,

âˆ€j âˆˆ [d âˆ’ 1] :
((Î»1n âŠ— w) âŠ— Ë‡X:d)âŠ¤Î± = 0 â‡” (w âŠ— Ë‡X:d)âŠ¤Î± = 0.

|((Î»1n âŠ— w) âŠ— Ë‡X:j)âŠ¤Î±| â‰¤ Î» â‡” |(w âŠ— Ë‡X:j)âŠ¤Î±| â‰¤ 1,

20

(31)

(32a)

(32b)

(32c)

Optimality conditions (4) and (5) are described as

âˆ€j âˆˆ [d âˆ’ 1] :

|(Î»1n âŠ— w âŠ— Ë‡X:j)âŠ¤Î±âˆ—(w)| < Î» â‡” |(w âŠ— Ë‡X:j)âŠ¤Î±âˆ—(w)| < 1 â‡’ Î²âˆ—(w)

j

= 0,

âˆ€i âˆˆ [n] :

Î»Î±âˆ—(w)
i

= 2 max{0, 1 âˆ’ Ë‡Xi:Î²âˆ—(w)}.

(33)

(34)

C Application of Safe Sample Screening to Kernelized Features

The kernel method in ML means computation methods when the input variable vector of a sample x âˆˆ Rd
cannot be specifically obtained (this includes the case when d is infinite), but for the input variable vectors
for any two samples x, xâ€² âˆˆ Rd its inner product xâŠ¤xâ€² can be obtained. In such a case, we cannot discuss
SfS since we cannot obtain each feature specifically, however, we can discuss SsS.

We show that the SsS rules for L1-loss L2-regularized SVM (Section 4.1) can be applied even if the features

are kernelized.

First, if features are kernelized, we cannot obtain either X or Î²âˆ—( Ëœw) specifically. However, since we can

obtain Î±âˆ—( Ëœw), with (7) we have

âˆ€x âˆˆ Rd : xâŠ¤Î²âˆ—( Ëœw) =

xâŠ¤(wÃ—â–¡ Ë‡X)âŠ¤Î±âˆ—( Ëœw) =

1
Î»

1
Î»

n
(cid:88)

i=1

wiÎ±âˆ—( Ëœw)
i

(xâŠ¤ Ë‡Xi:).

(35)

This means that we can calculate the inner product of Î²âˆ—( Ëœw) and any vector.

Then, in order to calculate the quantity (9) to conduct SsS, we have only to calculate

â€¢ Ë‡Xi:Î²âˆ—( Ëœw) can be calculated by (35),

â€¢ âˆ¥ Ë‡Xi:âˆ¥2 =

(cid:113)

Ë‡X âŠ¤
i:

Ë‡Xi: is obtained as the kernel value, and

â€¢ Pw(Î²âˆ—( Ëœw)) âˆ’ Dw(Î±âˆ—( Ëœw)) can be calculated by (35) and kernel values since two variables whose values

cannot be specifically obtained ( ËœX and Î²âˆ—( Ëœw)) appears only as inner products.

So, all values needed to derive SsS rules (9) can be computed even if features are kernelized.

D Details of Experiments

D.1 Detailed Experimental Setup

The criteria of selecting datasets (Table 2) and detailed setups are as follows:

â€¢ All of the datasets are downloaded from LIBSVM dataset [22]. We used scaled datasets for ones used
in DRSfS or only scaled datasets are provided (â€œionosphereâ€, â€œsonarâ€ and â€œspliceâ€). We used training
datasets only if test datasets are provided separately (â€œspliceâ€, â€œsvmguide1â€ and â€œmadelonâ€).

â€¢ For DRSsS, we selected datasets from LIBSVM dataset containing 100 to 10,000 samples, 100 or fewer
features, and the area under the curve (AUC) of the receiver operating characteristic (ROC) is 0.9 or
higher for the regularization strengths (Î») we examined so that they tend to facilitate more effective
sample screening.

21

â€¢ For DRSfS, we selected datasets from LIBSVM dataset containing 50 to 1,000 features, 10,000 or fewer
samples, and containing no categorical features. Also, due to computational constraints, we excluded
features that have at least one zero (marked â€œâ€ â€ in Table 2). As a result, one feature from â€œmadelonâ€
and one from â€œsonarâ€ have been excluded.

â€¢ In the table, the column â€œdâ€ denotes the number of features including the intercept feature (Remark

2.2).

The choice of regularization hyperparameter Î», based on the characteristics of the data, is as follows:

â€¢ For DRSsS, we set Î» as n, n Ã— 10âˆ’0.5, n Ã— 10âˆ’1.0, . . ., n Ã— 10âˆ’3.0. (For DRSsS with DL, we set 1000

instead of n.) This is because the effect of Î» gets weaker for larger n.

â€¢ For DRSfS, we determine Î» based on Î»max, defined as the smallest Î» for which Î²âˆ—(w)

j

= 0 for any

j âˆˆ [d âˆ’ 1] explained below. We then set Î» as Î»max, Î»max Ã— 10âˆ’1/3, Î»max Ã— 10âˆ’2/3, . . ., Î»max Ã— 10âˆ’2.

Finally, we show the calculation of Î»max for L2-loss L1-regularized SVM. By (14), we would like to find Î»
so that |(w âŠ— Ë‡X:j)âŠ¤Î±âˆ—(w)| < 1 for all j âˆˆ [d âˆ’ 1]. In order to judge this, we need Î±âˆ—(w), which is calculated
as follows:

â€¢ Solve the primal problem (1) for L2-loss L1-regularized SVM by fixing Î²âˆ—(w)

= 0 for any j âˆˆ [d âˆ’ 1],

j

that is,

Î²âˆ—(w)
d

n
(cid:88)

= argmin

Î²d

i=1

(cid:88)

= argmin

Î²d

wiâ„“yi(Ë‡xidÎ²d) = argmin

i=1
wi(max{0, 1 âˆ’ Î²d})2 +

Î²d

iâˆˆ[n], yi=+1

iâˆˆ[n], yi=âˆ’1

n
(cid:88)

wi(max{0, 1 âˆ’ yiÎ²d})2

(cid:88)

wi(max{0, 1 + Î²d})2

=

(cid:80)

iâˆˆ[n], yi=+1 wi âˆ’ (cid:80)
i=1 wi

(cid:80)n

iâˆˆ[n], yi=âˆ’1 wi

computed above and Î²âˆ—(w)

j

= 0 for any j âˆˆ [d âˆ’ 1], calculate Î±$ = Î»Î±âˆ—(w) = [2 max{0, 1 âˆ’

â€¢ If |(w âŠ— Ë‡X:j)âŠ¤Î±$)| < Î» for all j âˆˆ [d âˆ’ 1], then Î²âˆ—(w)

= 0 for any j âˆˆ [d âˆ’ 1]. So, we set Î»max =

â€¢ With Î²âˆ—(w)
Ë‡Xi:Î²âˆ—(w)}]n

d

i=1 by (15).

maxjâˆˆ[dâˆ’1] |(w âŠ— Ë‡X:j)âŠ¤Î±$)|.

.

j

D.2 All Experimental Results of Section 6.2

For the experiment of Section 6.2, ratios of screened samples by DRSsS setup is presented in Figure 7, while
ratios of screened features by DRSfS setup in Figure 8.

22

Dataset: australian

Dataset: breast-cancer

Dataset: heart

Dataset: ionosphere

Dataset: sonar

Dataset: splice

Dataset: svmguide1

Figure 7: Ratios of screened samples by DRSsS.

23

0.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.690=2.181=6.9=21.81=69.0=218=6900.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.683=2.159=6.83=21.59=68.3=215=6830.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.27=0.853=2.7=8.538=27.0=85.38=2700.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.351=1.109=3.510=11.09=35.1=110=3510.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=0.208=0.657=2.08=6.577=20.8=65.77=2080.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=1.0=3.162=10.0=31.62=100=316=10000.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples=3.089=9.768=30.89=97.68=308=976=3089Dataset: madelon

Dataset: sonar

Dataset: splice

Figure 8: Ratios of screened features by DRSfS.

24

0.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features=7.32e+2=1.58e+3=3.40e+3=7.32e+3=1.58e+4=3.40e+4=7.32e+40.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features=7.48e1=1.61e+0=3.47e+0=7.48e+0=1.61e+1=3.47e+1=7.48e+10.900.951.001.051.10a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features=7.34e+0=1.58e+1=3.41e+1=7.34e+1=1.58e+2=3.41e+2=7.34e+2