Distributionally Robust Safe Screening
Hiroyuki Hanada∗† Satoshi Akahane‡ Tatsuya Aoyama‡ Tomonari Tanaka‡
Yoshito Okura‡ Yu Inatsu§ Noriaki Hashimoto∗ Taro Murayama¶ Lee Hanju¶
Shinya Kojima¶ Ichiro Takeuchi‡∗‖
April 26, 2024
Abstract 1 Introduction
In this study, we consider the problem of identi-
fying unnecessary samples and features in a class
In this study, we propose a method Distributionally
of supervised learning problems within dynamically
Robust Safe Screening (DRSS), for identifying unnec-
changing environments. Identifying unnecessary sam-
essary samples and features within a DR covariate
ples/features offers several benefits. It helps in de-
shift setting. This method effectively combines DR
creasing the storage space required for keeping the
learning, a paradigm aimed at enhancing model ro-
training data for updating the machine learning (ML)
bustness against variations in data distribution, with
modelsinthefuture. Moreover,insituationsdemand-
safe screening (SS), a sparse optimization technique
ing real-time adaptation of ML models to quick envi-
designed to identify irrelevant samples and features
ronmental changes, the use of fewer samples/features
priortomodeltraining. ThecoreconceptoftheDRSS
enables more efficient learning.
method involves reformulating the DR covariate-shift
problem as a weighted empirical risk minimization Ourbasicideatotacklethisproblemistoeffectively
problem,wheretheweightsaresubjecttouncertainty combinedistributionallyrobust(DR)learningandsafe
within a predetermined range. By extending the SS screening (SS). DR learning is a ML paradigm that
technique to accommodate this weight uncertainty, focuses on developing models robust to variations in
the DRSS method is capable of reliably identifying the data distribution, providing performance guaran-
unnecessary samples and features under any future tees across different distributions (see, e.g., [1]). On
distribution within a specified range. We provide a the other hand, SS refers to sparse optimization tech-
theoretical guarantee of the DRSS method and vali- niques that can identify irrelevant samples/features
date its performance through numerical experiments before model training, ensuring computational effi-
on both synthetic and real-world datasets. ciency by avoiding unnecessary computations on cer-
tain samples/features which do not contribute to the
final solution [2, 3]. The key technical idea of SS is
to identify a bound of the optimal solution before
solving the optimization problem. This allows for the
∗RIKEN,Wako,Saitama,Japan identification of unnecessary samples/features, even
†hiroyuki.hanada@riken.jp without knowing the optimal solution.
‡NagoyaUniversity,Nagoya,Aichi,Japan
As a specific scenario of dynamically changing en-
§NagoyaInstituteofTechnology,Nagoya,Aichi,Japan
¶DENSOCORPORATION,Kariya,Aichi,Japan vironment, we consider covariate shift setting [4, 5]
‖ichiro.takeuchi@mae.nagoya-u.ac.jp with unknown test distribution. In this setting, the
1
4202
rpA
52
]LM.tats[
1v82361.4042:viXradistributionofinputfeaturesinthetrainingdatamay
undergo changes in the test phase, yet the actual
nature of these changes remains unknown. A ML
problem (e.g., regression/classification problem) in
covariateshiftsettingcanbeformulatedasaweighted
empirical risk minimization (weighted ERM) problem,
where weights are assigned based on the density ratio
of each sample in the training and test distributions.
Namely, by assigning higher weights to training sam-
ples that are important in the test distribution, the
model can focus on learning from relevant samples
and mitigate the impact of distribution differences
betweenthetrainingandthetestphases. Ifthedistri-
butionduringthetestphaseisknown,theweightscan
be uniquely fixed. However, if the test distribution
is unknown, it is necessary to solve a weighted ERM
problem with unknown weights.
Our main contribution is to propose a DRSS
method for covariate shift setting with unknown test
distribution. The proposed method can identify un-
necessary samples/features regardless of how the dis-
tribution changes within a certain range in the test
phase. To address this problem, we extend the exist-
ing SS methods in two stages. The first is to extend
the SS for ERM so that it can be applied to weighted
ERM. The second is to further extend the SS so that
it can be applied to weighted ERM when the weights
are unknown. While the first extension is relatively
straightforward, the second extension presents a non-
trivial technical challenge (Figure 1). To overcome
thischallenge, wederivea novelboundofthe optimal
solutions of the weighted ERM problem, which prop-
erly accounts for the uncertainty in weights stemming
from the uncertainty of the test distribution.
In this study, we consider DRSS for samples in
sample-sparse models such as SVM [6], and that for
features for feature-sparse models such as Lasso [7].
We denote the DRSS for samples as distributionally
robustsafesample screening(DRSsS)andthatforfea-
tures as distributionally robust safe feature screening
(DRSfS), respectively.
Our contributions in this study are summarized
as follows. First, by effectively combining DR and
SS, we introduce a framework for identifying unnec-
essary samples/features under dynamically changing
uncertain environment. Second, We consider a DR
gnineercS
elpmaS
efaS
B C
Unknown Distributionally
Test Distribution Robust
Safe Screening
A D
DRSS Method (Proposed)
Figure 1: Schematic illustration of the proposed Dis-
tributionally Robust Safe Screening (DRSS) method.
Panel A displays the training samples, each assigned
equal weight, as indicated by the uniform size of the
points. Panel B depicts various unknown test distri-
butions, highlighting how the significance of training
samples varies with different realizations of the test
distribution. Panel C shows the outcomes of safe
sample screening (SsS) across multiple realizations
of test distributions. Finally, Panel D presents the
results of the proposed DRSS method, demonstrating
itscapabilitytoidentifyredundantsamplesregardless
of the observed test distribution.
covariate-shift setting where the input distribution
of an ERM problem changes within a certain range.
In this setting, we propose a novel method called
DRSSmethodthatcanidentifysamples/featuresthat
are guaranteed not to affect the optimal solution, re-
gardless of how the distribution changes within the
specified range. Finally, through numerical exper-
iments, we verify the effectiveness of the proposed
DRSS method. Although the DRSS method is devel-
oped for convex ERM problems, in order to demon-
strate the applicability to deep learning models, we
also present results where the DRSS method is ap-
plied in a problem setting where the final layer of the
model is fine-tuned according to changes in the test
distribution.
21.1 Related Works
Table1: Notationsusedinthepaper. R: allrealnum-
bers, N: all positive integers, n,m,p ∈ N: integers,
TheDRsettinghasbeenexploredinvariousMLprob-
f : Rn → R∪{+∞}: convex function, M ∈ Rn×m:
lems, aiming to enhance model robustness against
matrix, v ∈Rn: vector.
data distribution variations. A DR learning problem
m ∈R (small case of matrix variable)
is typically formulated as a worst-case optimization ij
the element at the ith row and
problem since the goal of DR learning is to ensure
the jth column of M
model performance under the worst-case data dis-
v ∈R (nonbold font of vector variable)
tribution within a specified range. Hence, a variety i
the ith element of v
of optimization techniques tailored to DR learning
M ∈R1×n the ith row of M
have been investigated within both the ML and opti- i:
mization communities [8, 9, 1]. The proposed DRSS M :j ∈Rm×1 the jth column of M
method is one of such DR learning methods, focusing [n] {1,2,...,n}
specificallyontheproblemofsample/featuredeletion. R ≥0 all nonnegative real numbers
The ability to identify irrelevant samples/features is ⊗ elementwise product
of practical significance. For example, in the context diag(v)∈Rn×n diagonal matrix; (diag(v)) ii =v i
of continual learning (see, e.g., [10]), it is crucial to and (diag(v)) ij =0 (i̸=j)
effectively manage data by selectively retaining and v□×M ∈Rn×m diag(v)M
discarding samples/features, especially in anticipa- 0 n ∈Rn [0,0,...,0]⊤ (vector of size n)
tion of changes in future data distributions. Incorrect 1 n ∈Rn [1,1,...,1]⊤ (vector of size n)
deletion of essential data can lead to catastrophic for- ∥v∥ p ∈R ≥0 ((cid:80)n i=1v ip)1/p (p-norm)
getting [11], a phenomenon where a ML model, after ∂f(v)⊆Rn all g ∈Rn s.t. “for any v′ ∈Rn,
being trained on new data, quickly loses information
f(v′)−f(v)≥g⊤(v′−v)”
previously learned from older datasets. The proposed (subgradient)
DRSS method tackles this challenge by identifying Z[f]⊆Rn {v′ ∈Rn |∂f(v′)={0 n}}
samples/features that, regardless of future data dis- f∗(v)∈R∪{+∞} sup v′∈Rn(v⊤v′−f(v′))
tribution shifts, will not have any influence on all (convex conjugate)
possible newly trained model in the future. “f is κ-strongly f(v)−κ∥v∥2 2 is convex with
convex” (κ>0) respect to v
SS refers to optimization techniques in sparse
“f is µ-smooth” ∥f(v)−f(v′)∥ ≤µ∥v−v′∥
learning that identify and exclude irrelevant sam- 2 2
(µ>0) for any v,v′ ∈Rn
ples or features from the learning process. SS can
reduce computational cost without changing the fi-
nal trained model. Initially, SfS was introduced by
[2] for the Lasso. Subsequently, SsS was proposed
by [3] for the SVM. Among various SS methods de- knowledge, noexistingstudieshaveutilizedSSwithin
veloped so far, the most commonly used is based the DR learning framework.
on the duality gap [12, 13]. Our proposed DRSS
method also adopts this approach. Over the past
decade, SS has seen diverse developments, including
methodological improvements and expanded applica-
tion scopes [14, 15, 16, 17, 18, 19, 20, 21]. Unlike
other SS studies that primarily focused on reducing
2 Preliminaries
computational costs, this study adopts SS for a dif-
ferent purpose. We employ SS across scenarios where
data distribution varies within a defined range, aim-
ing to discard unnecessary samples/features. To our Notations used in this paper are described in Table 1.
32.1 Weighted Regularized Empiri- we have the following dual problem of (1):
cal Risk Minimization (Weighted
α∗(w) :=argmaxD (α), where
RERM) for Linear Prediction w
α∈Rn
D (α):= (2)
w
We mainly assume the weighted regularized empirical
n
risk minimization (weighted RERM) for linear pre- −(cid:88) w ℓ∗ (−γ α )−ρ∗(((γ⊗w)□×Xˇ)⊤α),
diction. This may include kernelized versions, which i yi i i
i=1
are discussed in Appendix C. Suppose that we learn
the model parameters as linear prediction coefficients, where γ is a positive-valued vector. The relationship
that is, learn β∗(w) ∈Rd such that the outcome for a between the original problem (1) (called the primal
sample x∈Rd is predicted as x⊤β∗(w). problem) and the dual problem (2) are described as
follows:
Definition 2.1. Given n training samples of d-
P (β∗(w))=D (α∗(w)), (3)
w w
dimensional input variables, scalar output variables
and scalar sample weights, denoted by X ∈ Rn×d, β∗(w) ∈∂ρ∗(((γ⊗w)□×Xˇ)⊤α∗(w)), (4)
y pu∈ taR tin onan od
f
ww ei∈ ghR tn ≥ ed0, Rre Es Rpe Mcti fv oe rly li, nt eh ae rt pr ra ein di in ctg ioc nom is- ∀i∈[n]: −γ iα i∗(w) ∈∂ℓ yi(Xˇ i:β∗(w)). (5)
formulated as follows:
2.2 Sparsity-inducing Loss Functions
and Regularization Functions
β∗(w) :=argminP (β), where
w
β∈Rd
In weighted RERM, we call that a loss function ℓ
P
(β):=(cid:88)n
w ℓ (Xˇ β)+ρ(β). (1)
induces sample-sparsity if elements in α∗(w) are easy
w i yi i: to become zero. Due to (5), this can be achieved by ℓ
i=1 such that {t ∈ R | 0 ∈ ∂ℓ (t)} is not a point but an
y
interval.
Here, ℓ :R→R is a convex loss function1, ρ:Rd →
y Similarly, we call that a regularization function ρ
R is a convex regularization function, and Xˇ ∈Rn×d
induces feature-sparsity if elements in β∗(w) are easy
is a matrix calculated from X and y and determined
to become zero. Due to (4), this can be achieved by
dependingonℓ. Inthispaper, unlessotherwisenoted, ρ such that {v ∈Rd |∃j ∈[d−1]: 0∈[∂ρ∗(v)] } is
we consider binary classifications (y ∈ {−1,+1}n) j
not a point but a region.
with Xˇ :=y□×X. For regressions (y ∈Rn) we usually
For example, the hinge loss ℓ (t)=max{0,1−t}
set Xˇ :=X. y
(y ∈{−1,+1}) is a sample-sparse loss function since
{t ∈ R | 0 ∈ ∂ℓ (t)} = [1,+∞). Similarly, the L1-
y
Remark 2.2. Weaddthat,weadopttheformulation
regularization ρ(v) =
λ(cid:80)d−1|v
| (λ > 0: hyperpa-
X :d =1 n so that β d∗(w) (the last element) represents rameter) is a feature-sparsj e=1 reguj larization function
the common coefficient for any sample (called the since {v ∈Rd |∃j ∈[d−1]: 0∈[∂ρ∗(v)] }={v ∈
j
intercept). Rd | ∃j ∈ [d−1] : |v | ≤ λ, v = 0}. See Section 4
j d
for examples of using them.
Sinceℓandρareconvex,wecaneasilyconfirmthat
P (β) is convex with respect to β.
w 3 Distributionally Robust Safe
ApplyingFenchel’sdualitytheorem(AppendixA.2),
Screening
1Forℓy(t),weassumethatonlytisavariableofthefunction
In this section we show DRSS rules for weighted
(y isassumedtobeaconstant)whenwetakeitssubgradient
orconvexconjugate. RERM with two steps. First, in Sections 3.1 and
43.2, we show SS rules for weighted RERM but not 3.2 (Non-DR) Safe Feature Screening
DR setup. To do this, we extended existing SS rules
We consider identifying j ∈ [d] such that β∗(w) = 0,
in [13, 15]. Then we derive DRSS rules in Section 3.3. j
that is, identifying that the jth feature is not used in
the prediction, even when the sample weights w are
3.1 (Non-DR) Safe Sample Screening
changed.
We consider identifying training samples that do not Forsimplicity, supposethattheregularizationfunc-
affect the training result β∗(w). Due to the relation- tion ρ is decomposable, that is, ρ is represented as
ship (4), if there exists i ∈ [n] such that α i∗(w) = 0, ρ(β):=(cid:80)d j=1σ j(β j) (σ 1,σ 2,...,σ d: R→R). Then,
then the ith row (sample) in Xˇ does not affect β∗(w). since ρ∗(v)=(cid:80)d σ∗(v ) and therefore [∂ρ∗(v)] =
j=1 j j j
However, since computing α∗(w) is as costly as β∗(w), ∂σ∗(v ), from (4) we have
j j
it is difficult to use the relationship as it is. To solve
the problem, the SsS first considers identifying the β∗(w) ∈∂σ∗((γ⊗w⊗Xˇ )⊤α∗(w))
possible region B∗(w) ⊂ Rd such that β∗(w) ∈ B∗(w) j j :j
isassured. Then,withB∗(w) and(5),wecanconclude
=∂σ∗(Xˇˇ(γ,w)⊤α∗(w)),
j :j
that the ith training sample do not affect the training where Xˇˇ(γ,w) :=γ⊗w⊗Xˇ .
result β∗(w) if (cid:83) ∂ℓ (Xˇ β)={0}. :j :j
β∈B∗(w) yi i:
FirstweshowhowtocomputeB∗(w). Inthispaper If we know α∗(w), we can identify whether β∗(w) =0
we adopt the computation methods that is available j
holds. However, like SsS (Section 3.1), we would like
when the regularization function ρ in P (and also
w to check the condition without computing α∗(w) or
P itself) of (1) are strongly convex.
w β∗(w).
So, like SsS, SfS first considers identifying the pos-
Lemma 3.1. Suppose that ρ in P (and also P
w w sible region A∗(w) ⊂ Rn such that α∗(w) ∈ A∗(w) is
itself) of (1) are κ-strongly convex. Then, for any
βˆ∈Rd and αˆ ∈Rn, we can assure β∗(w) ∈B∗(w) by assured. Then we can conclude that β j∗(w) = 0 is
taking assured if (cid:83) ∂σ∗(Xˇˇ(γ,w)⊤α)={0}.
α∈A∗(w) j :j
(cid:110) (cid:12) (cid:111) ThenweshowhowtocomputeA∗(w). WithLemma
B∗(w) := β (cid:12) (cid:12)∥β−βˆ∥ 2 ≤r(w,γ,κ,βˆ,αˆ) , A.3, we can calculate A∗(w) as follows, if the loss
(cid:114) function ℓ in P of (1) is smooth:
2 y w
where r(w,γ,κ,βˆ,αˆ):= [P (βˆ)−D (αˆ)].
κ w w Lemma 3.3. Suppose that ℓ in P of (1) is µ-
y w
smooth. Then, for any βˆ ∈Rd and αˆ ∈Rn, we can
The proof is presented in Appendix A.3. The
amount P (βˆ)−D (αˆ) is known as the duality gap, assure α∗(w) ∈A∗(w) by taking
w w
which must be nonnegative due to (3). So we ob- (cid:110) (cid:12) (cid:111)
A∗(w) := α(cid:12)∥α−αˆ∥ ≤r¯(w,γ,µ,βˆ,αˆ) ,
tainthefollowinggap safe sample screening rule from (cid:12) 2
Lemma 3.1: where r¯(w,γ,µ,βˆ,αˆ):=
Lemma 3.2. Under the same assumptionsas Lemma (cid:115)
2µ
d3 o.1 e, sα ni∗ o( tw a) ff= ec0
t
ti hs ea ts rs au ir ne id ng(i r.e e. s, ult the β∗it (h w)t )ra ii fn ti hn eg resa em xip stle
s
min i∈[n]w iγ
i2[P w(βˆ)−D w(αˆ)].
βˆ∈Rd and αˆ ∈Rn such that The proof is presented in Appendix A.5. Similar to
Lemma 3.2, we obtain the gap safe feature screening
[Xˇ βˆ−∥Xˇ ∥ r(w,γ,κ,βˆ,αˆ),
i: i: 2 rule from Lemma 3.3:
Xˇ βˆ+∥Xˇ ∥ r(w,γ,κ,βˆ,αˆ)]⊆Z[ℓ ].
i: i: 2 yi
Lemma 3.4. Under the same assumptionsasLemma
The proof is presented in Appendix A.4. 3.3, β∗(w) = 0 is assured (i.e., the jth feature does
j
5not affect prediction results) if there exists βˆ ∈ Rd Similarly, the DRSfS rule for W is calculated as:
and αˆ ∈Rn such that
[L−NR,L+NR]⊆Z[σ∗], where
j
[Xˇˇ :( jγ,w)⊤αˆ −∥Xˇˇ :( jγ,w)∥ 2r¯(w,γ,µ,βˆ,αˆ),
L:= min Xˇˇ(γ,w)⊤α∗(w˜) = min(γ⊗Xˇ ⊗α∗(w˜))⊤w,
:j :j
Xˇˇ(γ,w)⊤αˆ +∥Xˇˇ(γ,w)∥ r¯(w,γ,µ,βˆ,αˆ)]⊆Z[σ∗]. w∈W w∈W
:j :j 2 j L:= maxXˇˇ(γ,w)⊤α∗(w˜) = max(γ⊗Xˇ ⊗α∗(w˜))⊤w,
:j :j
w∈W w∈W
The proof is almost same as Lemma 3.2. (cid:114)
N := max∥Xˇˇ(γ,w)∥ = max∥w⊗γ⊗Xˇ ∥2,
:j 2 :j 2
w∈W w∈W
3.3 Application to Distributionally Ro- R:= maxr¯(w,γ,µ,β∗(w˜),α∗(w˜)).
w∈W
bust Setup
Thus, solving the maximizations and/or minimiza-
InSections3.1and3.2weshowedtheconditionswhen
tions in Theorem 3.7 provides DRSsS and DRSfS
samples or features are screened out. In this section
rules. However,howtosolveitlargelydependsonthe
we show how to use the conditions for the change of
choice of ℓ, ρ and W. In Section 4 we show specific
sample weights w.
calculations of Theorem 3.7 for some typical setups.
Definition 3.5 (weight-changing safe screening
(WCSS)). Given X ∈ Rn×d, y ∈ Rn, w˜ ∈ Rn and
≥0 4 DRSS for Typical ML Setups
w ∈Rn , suppose that β∗(w˜) in Definition 2.1 (and
≥0
also α∗(w˜)) are already computed, but β∗(w) not.
InthissectionweshowDRSSrulesderivedinSection
Then WCSsS (resp. WCSfS) from w˜ to w is de-
3.3 for two typical ML setups: DRSsS for L1-loss L2-
fined as finding i ∈ [n] satisfying Lemma 3.2 (resp.
regularized SVM (Section 4.1) and DRSfS for L2-loss
j ∈[d−1] satisfying Lemma 3.4).
L1-regularized SVM (Section 4.2) under W := {w |
∥w−w˜∥ ≤S}.
Definition3.6(Distributionallyrobustsafescreening 2
In the processes, we need to solve constrained max-
(DRSS)). Given X ∈ Rn×d, y ∈ Rn, w˜ ∈ Rn and
≥0 imizations of convex functions. Although maximiza-
W ⊂Rn , suppose that β∗(w˜) in Definition 2.1 (and
≥0 tionsofconvexfunctionsarenoteasyingeneral(min-
also α∗(w˜)) are already computed. Then the DRSsS imizations are easy), we show that the maximizations
(resp. DRSfS) for W is defined as finding i ∈ [n] need in the processes can be algorithmically solved in
satisfying Lemma 3.2 (resp. j ∈ [d−1] satisfying Section 4.3.
Lemma 3.4) for any w ∈W.
For Definition 3.5, we have only to apply SS rules 4.1 DRSsS for L1-loss L2-regularized
in Lemma 3.2 or 3.4 by setting βˆ←β∗(w˜) and αˆ ← SVM
α∗(w˜). Ontheotherhand, forDefinition3.6, weneed
to maximize or minimize the interval in Lemma 3.2 L1-loss L2-regularized SVM is a sample-sparse model
or 3.4 in w ∈W. forbinaryclassification(y ∈{−1,+1}n)thatsatisfies
thepreconditionstoapplySsS(Lemma3.1). Detailed
Theorem 3.7. The DRSsS rule for W is calculated calculations are presented in Appendix B.1.
as: For L1-loss L2-regularized SVM, we set ρ and ℓ as:
[Xˇ i:β∗(w˜)−∥Xˇ i:∥ 2R,Xˇ i:β∗(w˜)+∥Xˇ i:∥ 2R]⊆Z[ℓ yi], ρ(β):= λ 2∥β∥2
2
(λ>0: hyperparameter),
where R:=max r(w,γ,κ,β∗(w˜),α∗(w˜)). ℓ y(t):=max{0,1−t} (where y ∈{−1,+1}).
w∈W
6Thenρisλ-stronglyconvex. Settingγ =1 ,thedual 4.2 DRSfS for L2-loss L1-regularized
n
objective function is described as SVM
D (α)=
w L2-loss L1-regularized SVM is a feature-sparse model
 (cid:80)n w α − 1 α⊤(w□×Xˇ)(w□×Xˇ)⊤α, forbinaryclassification(y ∈{−1,+1}n)thatsatisfies
 i=1 i i 2λ
(∀i∈[n]:0≤α ≤1) (6) thepreconditionstoapplySfS(Lemma3.3). Detailed
i
 −∞. (otherwise) calculations are presented in Appendix B.2.
For L2-loss L1-regularized SVM, we set σ (and
j
Here, in the viewpoint of minimization, we may con- consequently ρ) and ℓ as:
sider this problem as a maximization with the con-
straint “∀i∈[n]:0≤α ≤1”.
i
∀j ∈[d−1]: σ (β ):=λ|β | (λ>0: hyperparameter),
Optimality conditions (4) and (5) are described as: j j j
σ (β ):=0,
d d
β∗(w) = 1 (w□×Xˇ)⊤α∗(w), (7) ℓ y(t):=(max{0,1−t})2 (where y ∈{−1,+1}).
λ
 {1}, (Xˇ β∗(w) ≤1)
 i:
Notice that σ (β ) is not defined as λ|β | but 0: we
∀i∈[n]: α∗(w) ∈ [0,1], (Xˇ β∗(w) =1) (8) d d d
i i: rarely regularize the intercept with L1-regularization.
{0}. (Xˇ β∗(w) ≥1)
i:
Setting γ = λ1 , the dual objective function is
n
described as
Noticing that Z[ℓ ] = (1,+∞), by Theorem 3.7,
yi
the DRSsS rule for W is calculated as:
D
(α)=(cid:40) −λ(cid:80)n i=1w iλα2 i− 44αi, ((11)–(13) are met)
Xˇ β∗(w˜)−∥Xˇ ∥ maxr(w,γ,κ,β∗(w˜),α∗(w˜))>1, w −∞, (otherwise)
i: i: 2
w∈W
(10)
where (9)
where α ≥0, (11)
r(w,γ,κ,β∗(w˜),α∗(w˜)) i
∀j ∈[d−1]: |(w⊗Xˇ )⊤α|≤1, (12)
(cid:114) :j
2
:= [P (β∗(w˜))−D (α∗(w˜))], (w⊗Xˇ )⊤α=(w⊗y)⊤α=0. (13)
κ w w :d
P (β∗(w˜))−D (α∗(w˜))
w w
n Optimality conditions (4) and (5) are described as
:=(cid:88) w [ℓ (Xˇ β∗(w˜))−α∗(w˜)]+λ∥β∗(w˜)∥2
i yi i: i 2
i=1
+
1
w⊤(α∗(w˜)□×Xˇ)(α∗(w˜)□×Xˇ)⊤w.
∀j ∈[d−1]: |(w⊗Xˇ :j)⊤α∗(w)|<1⇒β j∗(w) =0,
2λ (14)
2
Here,wecanfindthatP w(β∗(w˜))−D w(α∗(w˜)),which ∀i∈[n]: α i∗(w) = λmax{0,1−Xˇ i:β∗(w)}. (15)
we need to maximize in reality, is the sum of linear
function and convex quadratic function with respect
tow ∈W. (Since(α∗(w˜)□×Xˇ)(α∗(w˜)□×Xˇ)⊤ ispositive NoticingthatZ[σ∗]=(−λ,λ),byTheorem3.7,the
j
semidefinite,weknowthatitisconvexquadratic). Al- DRSfS rule for W is calculated as:
thoughconstrainedmaximizationofaconvexfunction
is difficult in general, for this case we can algorithmi-
L−NR>−λ, L+NR<λ,
cally maximize it (Section 4.3).
7where problems:
maxw⊤Aw+2b⊤w, (16)
L:=λ min(Xˇ ⊗α∗(w˜))⊤w,
:j w∈W
w∈W
where W :={w ∈Rn |∥w−w˜∥ ≤S},
L:=λmax(Xˇ ⊗α∗(w˜))⊤w, 2
w∈W :j w˜ ∈Rn, b∈Rn,
(cid:114)
N :=λ max∥w⊗Xˇ ∥2 A∈Rn×n : symmetric, positive semidefinite,
:j 2
w∈W
nonzero.
(cid:114)
=λ max{w⊤diag(Xˇ ⊗Xˇ )w},
:j :j
w∈W Lemma 4.1. The maximization (16) is achieved by
R:= maxr¯(w,γ,µ,β∗(w˜),α∗(w˜)), the following procedure. First, we define Q ∈ Rn×n
w∈W and Φ := diag(ϕ ,ϕ ,...,ϕ ) as the eigendecompo-
1 2 n
r¯(w,γ,µ,β∗(w˜),α∗(w˜)) sition of A such that A = Q⊤ΦQ, Q is orthogonal
(cid:115) (QQ⊤ =Q⊤Q=I). Also,letξ :=−ΦQw˜−Qb∈Rn,
2µ
:= [P (β∗(w˜))−D (α∗(w˜))], and
min w γ2 w w
i∈[n] i i
T(ν)=(cid:88)n (cid:18)
ξ
i
(cid:19)2
. (17)
ν−ϕ
i
i=1
P w(β∗(w˜))−D w(α∗(w˜)) Then, the maximization (16) is equal to the largest
=(cid:88)n
w
(cid:34)
ℓ (Xˇ
β∗(w˜))+λλ(α∗(w˜))2
i
−4α i∗(w˜)(cid:35) value among them:
i yi i: 4 • Foreachν suchthatT(ν)=S2 (seeLemma4.2),
i=1
+ρ(β∗(w˜)). the value νS2+(νw˜+b)⊤Q⊤(Φ−νI)−1ξ+b⊤w˜,
and
Here, the expressions in L and L are linear with • For each ν ∈ {ϕ 1,ϕ 2,...,ϕ n} (duplication re-
respecttow,andtheexpressioninN insidethesquare moved) such that “∀i ∈ [n] : ϕ i = ν ⇒ ξ i = 0”,
root is convex and quadratic with respect to w. Also, the value
R is decomposed to two maximizations 2µ
mini∈[n]wiγ i2 max[νS2+(νw˜ +b)⊤Q⊤τ +b⊤w˜],
and P w(β∗(w)) − D w(α∗(w)), where the former is τ∈Rn
easily computed while the latter is linear with respect ξ
subject to ∀i∈F : τ = i ,
to w. So, similar to L1-loss L2-regularized SVM, we ν i ϕ −ν
i
can obtain the maximization result by maximizing (cid:88) (cid:88)
τ2 =S2− τ2,
or minimizing the linear terms by Lemma A.4 in i i
Appendix A, and maximizing the convex quadratic
i∈Uν i∈Fν
where U :={i|i∈[n], ϕ =ν}, F :=[n]\U .
function by the method of Section 4.3. ν i ν ν
(Note that the maximization is easily computed
by Lemma A.4.)
4.3 Maximizing Linear and Convex
The proof is presented in Appendix A.6.
Quadratic Functions in Hyperball
Constraint Lemma 4.2. Under the same definitions as Lemma
4.1, The equation T(ν) = S2 can be solved by the
To derive DRSS rules of Sections 4.1 and 4.2, we following procedure: Let e:=[e ,e ,...,e ] (N ≤n,
1 2 N
need to compute the following forms of optimization k ≠ k′ ⇒e ̸=e ) be a sequence of indices such that
k k′
8𝑥
1
𝑥
2
𝑦
𝑆2 𝑥 𝑑
Final
𝜈 Feature extraction prediction
𝜙 𝑒1 𝜙 𝑒2 𝜙 𝑒3 𝜙 𝑒4 𝜙 𝑒𝑁 (Assum inie tid
a
lt o
le
b ae
rn
f ii nx ge )d after f( uC no cn tiv oe nx
)
Figure 2: An example of the expression T(ν) (black
Figure3: ConceptofhowtoapplySSfordeeplearning.
solid line) in Lemmas 4.1 and 4.2. Colored dash
SS is applied to the last layer for the final prediction.
lines denote terms in the summation (ξ /(ν−ϕ ))2.
ek ek
We can see that, given an interval (ϕ ,ϕ ) (k ∈
ek ek+1
[N −1]), the function is convex. 5 Application to Deep Learning
Sofar,ourdiscussionofSSruleshasprimarilyfocused
1. e ∈[n] for any k ∈[N],
k on ML models with linear predictions and convex loss
and regularization functions. However, there may be
2. i∈[n] is included in e if and only if ξ ̸=0, and
i scenarioswherewewouldliketoemploymorecomplex
ML models, such as deep learning (DL).
3. ϕ ≤ϕ ≤···≤ϕ .
e1 e2 eN For DL models, deriving SS rules for the entire
model can be challenging due to the complexity of
Note that, if ϕ < ϕ (k ∈ [N −1]), then T(ν)
ek ek+1 bounding the change in model parameters against
is a convex function in the interval (ϕ ,ϕ ) with
ek ek+1 changes in sample weights. However, we can simplify
lim = lim = +∞. Then, unless
ν→ϕek+0 ν→ϕek+1−0 the process by focusing on the fact that each layer of
N = 0, each of the following intervals contains just DL is often represented as a convex function. There-
one solution of T(ν)=S2: fore, we propose applying SS rules specifically to the
last layer of DL models.
• Intervals (−∞,ϕ ) and (ϕ ,+∞).
e1 eN Inthisformulation,thelayersprecedingthelastone
are considered as a fixed feature extraction process,
• Let ν#(k) := argmin T(ν). For each
ϕek<ν<ϕek+1 even when the sample weights change (see Figure
k ∈[N −1] such that ϕ <ϕ , 3). We believe that this approach is valid when the
ek ek+1
change in sample weights is not significant. We plan
– intervals (ϕ ,ν#(k)) and (ν#(k),ϕ ) if to experimentally evaluate the effectiveness of this
ek ek+1
T(ν#(k))<S2, formulation in Section 6.3.
– interval [ν#(k),ν#(k)] (i.e., point) if
T(ν#(k))=S2. 6 Numerical Experiment
It follows that T(ν)=S2 has at most 2n solutions. 6.1 Experimental Settings
By Lemma 4.2, in order to compute the solution of We evaluate the performances of DRSsS and DRSfS
T(ν)=S2,wehaveonlytocomputeν#(k) byNewton across different values of acceptable weight changes
method or the like, and to compute the solution for S and hyperparameters for regularization strength λ.
each interval by Newton method or the like. We show Performance is measured using safe screening rates,
an example of T(ν) in Figure 2, and the proof in representing the ratio of screened samples or features
Appendix A.7. to all samples or features. We consider three setups:
9DRSsSwithL1-lossL2-regularizedSVM(Section4.1),
Table 2: Datasets for DRSsS/DRSfS experiments.
DRSfSwithL2-lossL1-regularizedSVM(Section4.2),
All are binary classification datasets from LIBSVM
and DRSsS with deep learning (Section 5) where
dataset [22]. The mark † denotes datasets with one
the last layer incorporates DRSsS with L1-loss L2-
feature removed due to computational constraints.
regularized SVM.
See Appendix D.1 for details.
In these experiments, we set initialize the sample
weights before change (w˜) as w˜ =1 . Then, we set
n
S in DRSS for W := {w | ∥w−w˜∥ 2 ≤ S} (Section Task Name n n+ d
4) as follows:
DRSsS australian 690 307 15
• First we assume the weight change that the breast-cancer 683 239 11
heart 270 120 14
weights for positive samples ({i|y =+1}) from
i
ionosphere 351 225 35
1 to a, while retaining the weights for negative
sonar 208 97 61
samples ({i|y =−1}) as 1.
i
splice (train) 1000 517 61
• Then, we defined S as the size of weight change svmguide1 (train) 3089 2000 5
√
above; specifically, we set S = n+|a − 1| DRSsS madelon (train) 2000 1000 † 500
(n+: number of positive samples in the train- sonar 208 97 † 60
ing dataset). splice (train) 1000 517 61
We vary a within the range 0.9≤a≤1.1, assuming
a maximum change of up to 10% per sample weight.
6.3 Safe Sample Screening for Deep
6.2 Relationship between the Weight Learning Model
Changes and Safe Screening Rate
WeappliedDRSsStoDLmodels(Section5),assuming
First, we present safe screening rates for two SVM that all layers are fixed except for the last layer.
setups. The datasets used in these experiments are We utilized a neural network architecture compris-
detailed in Table 2. In this experiment, we adapt ing the following components: firstly, ResNet50 [23]
the regularization hyperparameter λ based on the with an output of 2,048 features, followed by a fully
characteristicsofthedata. Thesedetailsaredescribed connected layer to reduce the features to 10, and
in Appendix D.1. finally, L1-loss L2-regularized SVM (Section 4.1) ac-
As an example, for the “sonar” dataset, we show companied by the intercept feature (Remark 2.2).
the DRSsS result in Figure 4 and the DRSfS result in For the experiment, we employed the CIFAR-10
Figure 5. Results for other datasets are presented in dataset [24], a well-known benchmark dataset for
Appendix D.2. image classification tasks. We configured the net-
These plots allow us to assess the tolerance for work to classify images into two classes: “airplane”
changesinsampleweights. Forinstance,witha=0.98 and “automobile”. Given that there are 5,000 im-
(weight of each positive sample is reduced by two per- ages for each class, we split the dataset into train-
cent, or equivalent weight change in L2-norm), the ing:validation:testing=6:2:2, resulting in a total of
samplescreeningrateis0.31forL1-lossL2-regularized 6,000 images in the training dataset.
SVM with λ = 6.58e+1, and the feature screen- The resulting safe sample screening rates are illus-
ing rate is 0.29 for L2-loss L1-regularized SVM with trated in Figure 6. We observed similar outcomes to
λ=3.47e+1. This implies that, even if the weights those obtained with ordinary SVMs in Section 6.2.
are changed in such ranges, a number of samples or This experiment validates the feasibility of apply-
features are still identified as redundant in the sense ing DRSsS to DL models, demonstrating consistent
of prediction. results with traditional SVM setups.
107 Conclusion
no direct applications that might impact society or
ethical considerations.
In this paper, we discussed DR-SS, considering the
possible changes in sample weights to represent DR
Acknowledgements
setup. We developed a method for calculating SS
that can handle changes in sample weights by intro-
ducing nontrivial computational techniques, such as This work was partially supported by MEXT KAK-
constrained maximization of certain convex functions ENHI (20H00601), JST CREST (JPMJCR21D3 in-
(Section 4.3). Additionally, to address the constraint cluding AIP challenge program, JPMJCR22N2), JST
of SS, which typically applies to ML by minimizing MoonshotR&D(JPMJMS2033-05),JSTAIPAcceler-
convex functions, we provided an application to DL ation Research (JPMJCR21U2), NEDO (JPNP18002,
by applying SS to the last layer of DL model. While JPNP20006) and RIKEN Center for Advanced Intel-
this approach is an approximation, it holds certain ligence Project.
validity.
For the future work, we aim to explore different
References
environmental changes. In this paper, we focused on
weightconstraintbyL2-norm∥w−w˜∥ ≤S (Section
2 [1] Ruidi Chen and Ioannis Ch. Paschalidis. Dis-
4) due to computational considerations. However,
tributionally robust learning. arXiv Preprint,
when interpreting changes in weights, the constraint
2021.
of L1-norm ∥w−w˜∥ ≤ S may be more appropri-
1
ate, as it reflects changes in weights by altering the [2] Laurent El Ghaoui, Vivian Viallon, and Tarek
number of samples. Furthermore, in the context of Rabbani. Safe feature elimination for the lasso
DR-SS for DL, we are interested in loosening the and sparse supervised learning problems. Pacific
constraint of fixing the network except for the last Journal of Optimization, 8(4):667–698, 2012.
layer. Investigatingthisaspectcouldprovidevaluable
insights into the flexibility of DR-SS methodologies [3] Kohei Ogawa, Yoshiki Suzuki, and Ichiro
in DL applications. Takeuchi. Safe screening of non-support vectors
in pathwise svm computation. In Proceedings of
the 30th International Conference on Machine
Software and Data Learning, pages 1382–1390, 2013.
[4] Hidetoshi Shimodaira. Improving predictive in-
The code and the data to reproduce the experiments
ference under covariate shift by weighting the
are available as the attached file.
log-likelihoodfunction.Journalofstatisticalplan-
ning and inference, 90(2):227–244, 2000.
Potential Broader Impact
[5] Masashi Sugiyama, Matthias Krauledat, and
Klaus-Robert Mu¨ller. Covariate shift adaptation
This paper contributes to machine learning in dynam-
byimportanceweightedcrossvalidation. Journal
ically changing environments, a scenario increasingly
of Machine Learning Research, 8(35):985–1005,
prevalent in real-world data analyses. We believe
2007.
that, in such situations, ensuring prediction perfor-
manceagainstenvironmentalchangesandminimizing [6] C. Cortes and V. Vapnik. Support-vector net-
storage requirements for expanding datasets will be works. Machine Learning, 20:273–297, 1995.
beneficial. The method does not present significant
ethical concerns or foreseeable societal consequences [7] Robert Tibshirani. Regression shrinkage and
because this work is theoretical and, as of now, has selection via the lasso. Journal of the Royal Sta-
11tistical Society Series B: Statistical Methodology, [15] Atsushi Shibagaki, Masayuki Karasuyama, Ko-
58(1):267–288, 1996. hei Hatano, and Ichiro Takeuchi. Simultaneous
safe screening of features and samples in doubly
[8] Joel Goh and Melvyn Sim. Distributionally
sparse modeling. In International Conference on
robust optimization and its tractable approxi-
Machine Learning, pages 1577–1586, 2016.
mations. Operations Research, 58(4-1):902–917,
2010. [16] Kazuya Nakagawa, Shinya Suzumura, Masayuki
Karasuyama, Koji Tsuda, and Ichiro Takeuchi.
[9] Erick Delage and Yinyu Ye. Distributionally
Safe pattern pruning: An efficient approach for
robust optimization under moment uncertainty
predictive pattern mining. In Proceedings of the
with application to data-driven problems. Oper-
22nd ACM SIGKDD International Conference
ations Research, 58(3):595–612, 2010.
on Knowledge Discovery and Data Mining, pages
[10] Liyuan Wang, Xingxing Zhang, Kuo Yang, 1785–1794. ACM, 2016.
Longhui Yu, Chongxuan Li, Lanqing HONG,
[17] Shaogang Ren, Shuai Huang, Jieping Ye, and
Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun
Xiaoning Qian. Safe feature screening for gen-
Zhu. Memory replay with data compression for
eralized lasso. IEEE Transactions on Pattern
continual learning. In International Conference
Analysis and Machine Intelligence, 40(12):2992–
on Learning Representations, 2022.
3006, 2018.
[11] James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An- [18] Jiang Zhao, Yitian Xu, and Hamido Fujita. An
drei A. Rusu, Kieran Milan, John Quan, Tiago improved non-parallel universum support vec-
Ramalho, Agnieszka Grabska-Barwinska, Demis tor machine and its safe sample screening rule.
Hassabis, Claudia Clopath, Dharshan Kumaran, Knowledge-Based Systems, 170:79–88, 2019.
and Raia Hadsell. Overcoming catastrophic for-
[19] Zhou Zhai, Bin Gu, Xiang Li, and Heng Huang.
gettinginneuralnetworks. ProceedingsoftheNa-
Safe sample screening for robust support vector
tional Academy of Sciences, 114(13):3521–3526,
machine. InProceedings of the AAAI Conference
2017.
on Artificial Intelligence, volume 34, pages 6981–
[12] Olivier Fercoq, Alexandre Gramfort, and Joseph 6988, 2020.
Salmon. Mindthedualitygap: saferrulesforthe
[20] Hongmei Wang and Yitian Xu. A safe double
lasso. In Proceedings of the 32nd International
screening strategy for elastic net support vec-
Conference on Machine Learning, pages 333–342,
tor machine. Information Sciences, 582:382–397,
2015.
2022.
[13] Eugene Ndiaye, Olivier Fercoq, Alexandre Gram-
fort,andJosephSalmon.Gapsafescreeningrules [21] Takumi Yoshida, Hiroyuki Hanada, Kazuya Nak-
for sparse multi-task and multi-class models. In agawa, Kouichi Taji, Koji Tsuda, and Ichiro
Advances in Neural Information Processing Sys- Takeuchi. Efficient model selection for predictive
tems, pages 811–819, 2015. pattern mining model by safe pattern pruning.
Patterns, 4(12):100890, 2023.
[14] Shota Okumura, Yoshiki Suzuki, and Ichiro
Takeuchi. Quick sensitivity analysis for incre- [22] Chih-ChungChangandChih-JenLin.Libsvm: A
mental data modification and its application to libraryforsupportvectormachines. ACM Trans-
leave-one-out cv in linear classification problems. actions on Intelligent Systems and Technology
In Proceedings of the 21th ACM SIGKDD In- (TIST), 2(3):27, 2011. Datasets are provided in
ternational Conference on Knowledge Discovery authors’ website: https://www.csie.ntu.edu.
and Data Mining, pages 885–894, 2015. tw/~cjlin/libsvmtools/datasets/.
12[23] KaimingHe, XiangyuZhang, ShaoqingRen, and
Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
(CVPR), June 2016.
[24] Alex Krizhevsky. The cifar-10 dataset, 2009.
1.0
[25] Ralph Tyrell Rockafellar. Convex analysis.
0.8
Princeton university press, 1970.
0.6
[26] Jean-Baptiste Hiriart-Urruty and Claude 0.4
Lemar´echal. Convex Analysis and Minimization 0.2
Algorithms II: Advanced Theory and Bundle
0.0
Methods. Springer, 1993. 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
=0.208
=0.657
=2.08
=6.577
=20.8
=65.77
=208
Figure 4: Ratio of screened samples by DRSsS for
dataset “sonar”.
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
serutaef
deneercs
fo
oitaR
=7.48e 1
=1.61e+0
=3.47e+0
=7.48e+0
=1.61e+1
=3.47e+1
=7.48e+1
Figure 5: Ratio of screened features by DRSfS for
dataset “sonar”.
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo
oitaR
=6.00e+0
=1.90e+1
=6.00e+1
=1.90e+2
=6.00e+2
=1.90e+3
=6.00e+3
Figure 6: Ratio of screened samples by DRSsS
for dataset with CIFAR-10 dataset and DL model
ResNet50.
13A Proofs
A.1 General Lemmas
Lemma A.1. For a convex function f :Rd →R∪{+∞}, f∗∗ is equivalent to f if f is convex, proper (i.e.,
∃v ∈Rd : f(v)<+∞) and lower-semicontinuous.
Proof. See Section 12 of [25] for example.
Lemma A.1 is known as Fenchel-Moreau theorem. Especially, Lemma A.1 holds if f is convex and
∀v ∈Rd : f(v)<+∞.
Lemma A.2. For a convex function f :Rd →R∪{+∞},
• f∗ is (1/ν)-strongly convex if f is proper and ν-smooth.
• f∗ is (1/κ)-smooth if f is proper, lower-semicontinuous and κ-strongly convex.
Proof. See Section X.4.2 of [26] for example.
Lemma A.3. Supposethatf :Rd →R∪{+∞}isaκ-stronglyconvexfunction, andletv∗ =argmin f(v)
v∈Rd
be the minimizer of f. Then, for any v ∈Rd, we have
(cid:114)
2
∥v−v∗∥ ≤ [f(v)−f(v∗)].
2 κ
Proof. See [13] for example.
Lemma A.4. For any vector a,c∈Rn and S >0,
min a⊤v =a⊤c−S∥a∥ , max a⊤v =a⊤c+S∥a∥ .
2 2
v∈Rn: ∥v−c∥2≤S v∈Rn: ∥v−c∥2≤S
Proof. By Cauchy-Schwarz inequality,
−∥a∥ ∥v−c∥ ≤a⊤(v−c)≤∥a∥ ∥v−c∥ .
2 2 2 2
Noticing that the first inequality becomes equality if ∃ω >0: a=−ω(v−c), while the second inequality
becomes equality if ∃ω′ >0: a=ω′(v−c). Moreover, since ∥v−c∥ ≤S,
2
−S∥a∥ ≤a⊤(v−c)≤S∥a∥
2 2
also holds, with the equality holds if ∥v−c∥ =S.
2
On the other hand, if we take v that satisfies both of the equality conditions of Cauchy-Schwarz inequality
above, that is,
• (for the first inequality being equality) v =c−(S/∥a∥ )a,
2
• (for the second inequality being equality) v =c+(S/∥a∥ )a,
2
then the inequalities become equalities. This proves that −S∥a∥ and S∥a∥ are surely the minimum and
2 2
maximum of a⊤(v−c), respectively.
14A.2 Derivation of Dual Problem by Fenchel’s Duality Theorem
As the formulation of Fenchel’s duality theorem, we follow the one in Section 31 of [25].
Lemma A.5 (A special case of Fenchel’s duality theorem: f,g <+∞). Let f :Rn →R and g :Rd →R be
convex functions, and A∈Rn×d be a matrix. Moreover, we define
v∗ := min[f(Av)+g(v)], (18)
v∈Rd
u∗ := max[−f∗(−u)−g∗(A⊤u)]. (19)
u∈Rn
Then Fenchel’s duality theorem assures that
f(Av∗)+g(v∗)=−f∗(−u∗)−g∗(A⊤u∗),
−u∗ ∈∂f(Av∗),
v∗ ∈∂g∗(A⊤u∗).
Sketch of the proof. Introducing a dummy variable ψ ∈Rn and a Lagrange multiplier u∈Rn, we have
min[f(Av)+g(v)]= max min [f(ψ)+g(v)−u⊤(Av−ψ)] (20)
v∈Rd u∈Rnv∈Rd, ψ∈Rn
=− min max [−f(ψ)−g(v)+u⊤(Av−ψ)]=− min max [{(−u)⊤ψ−f(ψ)}+{(A⊤u)⊤v−g(v)}]
u∈Rnv∈Rd, ψ∈Rn u∈Rnv∈Rd, ψ∈Rn
=− min[f∗(−u)+g∗(A⊤u)]= max[−f∗(−u)−g∗(A⊤u)]. (21)
u∈Rn u∈Rn
Moreover, bytheoptimalityconditionofaproblemwithaLagrangemultiplier(20), theoptimaofit, denoted
by v∗, ψ∗ and u∗, must satisfy
Av∗ =ψ∗, A⊤u∗ ∈∂g(v∗), −u∗ ∈∂f(ψ∗)=∂f(Av∗).
Ontheotherhand, introducingadummyvariableϕ∈Rd andaLagrangemultiplierv ∈Rd for(21), wehave
max[−f∗(−u)−g∗(A⊤u)]= min max [−f∗(−u)−g∗(ϕ)−v⊤(A⊤u−ϕ)] (22)
u∈Rn v∈Rdu∈Rn,ϕ∈Rd
= min max [{(Av)⊤(−u)−f∗(−u)}+{v⊤ϕ−g∗(ϕ)}]
v∈Rdu∈Rn,ϕ∈Rd
= min[f∗∗(Av)+g∗∗(v)]= min[f(Av)+g(v)]. (∵ Lemma A.1)
v∈Rd v∈Rd
Likely above, by the optimality condition of a problem with a Lagrange multiplier (22), the optima of it,
denoted by u∗, ϕ∗ and v∗, must satisfy
A⊤u∗ =ϕ∗, v∗ ∈∂g∗(ϕ∗)=∂g∗(A⊤u∗), Av∗ ∈∂f(−u∗).
Lemma A.6 (Dual problem of weighted regularized empirical risk minimization (weighted RERM)). For
the minimization problem
n
β∗(w) :=argminP (β), where P (β):=(cid:88) w ℓ (Xˇ β)+ρ(β), ((1) restated)
w w i yi i:
β∈Rd
i=1
15we define the dual problem as the one obtained by applying Fenchel’s duality theorem (Lemma A.5), which is
defined as
n
α∗(w) :=argmaxD (α), where D (α):=−(cid:88) w ℓ∗ (−γ α )+ρ∗(((γ⊗w)□×Xˇ)⊤α). ((2) restated)
α∈Rn
w w i yi i i
i=1
Moreover, β∗(w) and α∗(w) must satisfy
P (β∗(w))=D (α∗(w)), ((3) restated)
w w
β∗(w) ∈∂ρ∗(((γ⊗w)□×Xˇ)⊤α∗(w)), ((4) restated)
∀i∈[n]: −γ α∗(w) ∈∂ℓ (Xˇ β∗(w)). ((5) restated)
i i yi i:
Proof. To apply Fenchel’s duality theorem, we have only to set f, g and A in Lemma A.5 as
n
f(u):=(cid:88) w ℓ (u ), g(β):=ρ(β), A:=Xˇ.
i yi i
i=1
Here, noticing that
n n
(cid:88) (cid:88)
f∗(u)= sup [u⊤u′− w ℓ (u′)]= sup [u u′ −w ℓ (u′)]
u′∈Rn
i yi i
u′∈Rn
i i i yi i
i=1 i=1
n (cid:20) (cid:21) n (cid:18) (cid:19)
= sup (cid:88) w u iu′ −ℓ (u′) =(cid:88) w ℓ∗ u i ,
u′∈Rn i w i i yi i i yi w i
i=1 i=1
from (19) we have
n (cid:18) (cid:19)
−f∗(−u)−g∗(A⊤u)=−(cid:88) w ℓ∗ −u i −ρ∗(Xˇ⊤u).
i yi w
i
i=1
Replacing u ←γ w α , that is, u←(γ⊗w⊗α), we have the dual problem (2).
i i i i
The relationships between the primal and the dual problem are described as follows:
−u∗ ∈∂f(Av∗) ⇒ −γ⊗w⊗α∗(w) ∈∂f(Xˇβ∗(w)) ⇒ −γ w α∗(w) ∈w ∂ℓ (Xˇ β∗(w))
i i i i yi i:
⇒−γ α∗(w) ∈∂ℓ (Xˇ β∗(w)),
i i yi i:
v∗ ∈∂g∗(A⊤u∗) ⇒ β∗(w) ∈∂g∗(Xˇ⊤γ⊗w⊗α∗(w))=∂g∗(((γ⊗w)□×Xˇ)⊤α∗(w)).
A.3 Proof of Lemma 3.1
Proof. [13]
(cid:114)
2
∥βˆ−β∗(w)∥ ≤ [P (βˆ)−P (β∗(w))] (∵ setting f ←P in Lemma A.3)
2 λ w w w
(cid:114)
2
= [P (βˆ)−D (α∗(w))] (∵ (3))
λ w w
(cid:114)
2
≤ [P (βˆ)−D (αˆ)]. (∵ α∗(w) is a maximizer of D )
λ w w w
16A.4 Proof of Lemma 3.2
Proof. Due to (5), if ∂ℓ (Xˇ β∗(w))={0} is assured, then α∗(w) =0 is assured. Since we do not know β∗(w)
yi i: i
but know B∗(w) (Lemma 3.1), we can assure α∗(w) = 0 if (cid:83) ∂ℓ (Xˇ β) = {0} is assured. Noticing
i β∈B∗(w) yi i:
that ∂ℓ is monotonically increasing2, we have
yi
(cid:91) ∂ℓ (Xˇ β)={0} ⇔ (cid:91) Xˇ β ⊆Z[ℓ ] ⇔ [ min Xˇ β, max Xˇ β]⊆Z[ℓ ]
yi i: i: yi
β∈B∗(w)
i:
β∈B∗(w)
i: yi
β∈B∗(w) β∈B∗(w)
(cid:104) (cid:105)
⇔ Xˇ βˆ−∥Xˇ ∥ r(w,γ,κ,βˆ,αˆ), Xˇ βˆ+∥Xˇ ∥ r(w,γ,κ,βˆ,αˆ) ⊆Z[ℓ ]. (∵ Lemma A.4)
i: i: 2 i: i: 2 yi
A.5 Proof of Lemma 3.3
Proof. The proof is almost the same as that for Lemma 3.1 (see Appendix A.3), but we additionally need to
show that −D is ((min w γ2)/µ)-strongly convex (in this case D is called strongly concave).
w i∈[n] i i w
As discussed in Lemma A.2, −ℓ∗ (t) is (1/µ)-strongly convex, that is, −ℓ∗ (t)−(1/2µ)t2 is convex. Thus,
yi yi
• −ℓ∗ (−γ α )−(1/2µ)(γ α )2 is convex with respect to α ,
yi i i i i i
• −w ℓ∗ (−γ α )−(w γ2/2µ)α2 is convex with respect to α ,
i yi i i i i i i
• −(cid:80)n w ℓ∗ (−γ α )−(cid:80)n (w γ2/2µ)α2 is convex with respect to α.
i=1 i yi i i i=1 i i i
So, −(cid:80)n w ℓ∗ (−γ α ) is convex with respect to α even subtracted by (cid:80)n [min (w γ2/2µ)]α2 =
i=1 i yi i i k=1 i∈[n] i i k
(1/2)[min (w γ2/µ)]∥α∥2.
i∈[n] i i 2
A.6 Proof of Lemma 4.1
Lemma A.7. For the optimization problem
maxw⊤Aw+2b⊤w, ((16) restated)
w∈W
subject to W :={w ∈Rn |∥w−w˜∥ ≤S},
2
where w˜ ∈Rn, b∈Rn,
A∈Rn×n : symmetric, positive semidefinite, nonzero,
its stationary points are obtained as the solution of the following equations with respect to w and ν ∈R:
Aw+b−ν(w−w˜)=0, (23)
∥w−w˜∥ =S. (24)
2
Also, when both (23) and (24) are satisfied, the function to be maximized is calculated as
w⊤Aw+2b⊤w =νS2+(νw˜ +b)⊤(w−w˜)+w˜⊤b. (25)
F
:2 RSi →nce 2R∂ℓ isyi mi os na otm onu il ct ai- llv yal iu ne cd reafu sin nc gti io fn ,, fot rh ae nm yo tn <ot to ′,n Ficit my um stu ss at tib sfe yd “e ∀fi sn ∈ed Fa (c t)c ,o ∀rd si ′n ∈gl Fy: (t′w ):e sc ≤all sa ′”.multi-valued function
17Proof. First, w⊤Aw +2b⊤w is convex and not constant. Then we can show that (16) is optimized in
{w ∈Rn |∥w−w˜∥ =S}, that is, at the surface of the hyperball W (Theorem 32.1 of [25]). This proves
2
(24). Moreover, with the fact, we write the Lagrangian function with Lagrange multiplier ν ∈R as:
L(w,ν):=w⊤Aw+2b⊤w−ν(∥w−w˜∥2−S2).
2
Then, due to the property of Lagrange multiplier, the stationary points of (16) are obtained as
∂L
=2Aw+2b−2ν(w−w˜)=0,
∂w
∂L
=∥w−w˜∥2−S2 =0,
∂ν 2
where the former derives (23).
Finally we show (25). If both (23) and (24) are satisfied,
w⊤Aw+2b⊤w =w⊤(ν(w−w˜)−b)+2b⊤w (∵ (23))
=νw⊤(w−w˜)+b⊤w
=ν(w−w˜)⊤(w−w˜)+νw˜⊤(w−w˜)+b⊤(w−w˜)+b⊤w˜
=νS2+νw˜⊤(w−w˜)+b⊤(w−w˜)+b⊤w˜ (∵ (24))
=νS2+(νw˜ +b)⊤(w−w˜)+b⊤w˜ ((25) restated)
Proof of Lemma 4.1. The condition (23) is calculated as
Aw+b=ν(w−w˜),
(A−νI)(w−w˜)=−Aw˜ −b.
Here, let us apply eigendecomposition of A, denoted by A = Q⊤ΦQ, where Q ∈ Rn×n is orthogonal
(QQ⊤ =Q⊤Q=I) and Φ:=diag(ϕ ,ϕ ,...,ϕ ) is a diagonal matrix consisting of eigenvalues of A. Such a
1 2 n
decomposition is assured to exist since A is assumed to be symmetric and positive semidefinite. Then,
(Q⊤ΦQ−νI)(w−w˜)=−Q⊤ΦQw˜ −b,
Q⊤(Φ−νI)Q(w−w˜)=−Q⊤ΦQw˜ −b,
(Φ−νI)τ =ξ, (where τ :=Q(w−w˜), ξ :=−ΦQw˜ −Qb∈Rn,) (26)
∀i∈[n]: (ϕ −ν)τ =ξ . (27)
i i i
Note that we have to be also aware of the constraint
√ (cid:113)
S =∥τ∥ = τ⊤τ = (w−w˜)⊤Q⊤Q(w−w˜)=∥w−w˜∥ . (28)
2 2
Here, we consider these two cases.
181. First,considerthecasewhen(Φ−νI)isnonsingular,thatis,whenν isdifferentfromanyofϕ ,ϕ ,...,ϕ .
1 2 n
Then, from (28) we have
S2 =∥τ∥
=(cid:88)n
τ2
=(cid:88)n (cid:18)
ξ i
(cid:19)2
(cid:0) =:T(ν)(cid:1) . (29)
2 i ν−ϕ
i
i=1 i=1
So, values of (16) for all stationary points with respect to w and ν (on condition that (Φ−νI) is
nonsingular) can be obtained by computing (25) for each ν satisfying (29), that is,
• for such ν computing τ by (27), and
• computing (25) as νS2+(νw˜ +b)⊤(w−w˜)+b⊤w˜ =νS2+(νw˜ +b)⊤Q⊤τ +b⊤w˜.
2. Secondly, considerthecasewhen(Φ−νI)isnonsingular, thatis, whenν isequaltooneofϕ ,ϕ ,...,ϕ .
1 2 n
First, given ν, let U :={i|i∈[n], ϕ =ν} be the indices of {ϕ } equal to ν (this may include more
ν i i i
than one indices), and F :=[n]\U . Note that, by assumption, U is not empty. Then, all stationary
ν ν ν
pointsof (16)withrespecttow andν (onconditionthat(Φ−νI)issingular)canbefoundbycomputing
the followings for each ν ∈{ϕ ,ϕ ,...,ϕ } (duplication excluded):
1 2 n
• If ξ ̸=0 for at least one i∈U , the equation (27) cannot hold.
i ν
• If ξ = 0 for all i ∈ N , the equation (27) may hold. So we calculate τ that maximizes (16) as
i ν
follows:
– Fix τ =ξ /(ϕ −ν) for i∈F .
i i i ν
– Set the constraint (cid:80) τ2 =S2−(cid:80) τ2 (due to (28)).
i∈Uν i i∈Fν i
– Maximize (16) with respect to {τ } under the constraints above. Here, by (25) we have
i i∈Uν
only to calculate
max[νS2+(νw˜ +b)⊤(w−w˜)+b⊤w˜], (30)
τ∈Rn
ξ
subject to ∀i∈F : τ = i ,
ν i ϕ −ν
i
(cid:88) (cid:88)
τ2 =S2− τ2,
i i
i∈Uν i∈Fν
which is easily computed by Lemma A.4. The value of the maximization result is equal to that
of (16) on condition that ν is specified above.
So, collecting these result and taking the largest one, the maximization (on condition that (Φ−νI)
is singular) is completed.
Taking the maximum of the two cases, we have the maximization result of (16).
A.7 Proof of Lemma 4.2
Proof. Weshowthestatementsinthelemmathat,ifϕ <ϕ (k ∈[N−1]),thenT(ν)isaconvexfunction
ek ek+1
intheinterval(ϕ ,ϕ )withlim =lim =+∞. Thentheconclusionimmediatelyfollows.
ek ek+1 ν→ϕek+0 ν→ϕek+1−0
19The latter statement clearly holds. The former statement is proved by directly computing the derivative.
d
T(ν)=
d (cid:88)n (cid:18) ξ
i
(cid:19)2 =−2(cid:88)n ξ i2
.
dν dν ν−ϕ (ν−ϕ )3
i i
i=1 i=1
It is an increasing function with respect to ν, as long as ν does not match any of {ϕ }n such that ξ ≠ 0.
i i=1 i
So it is convex in the interval ϕ <ν <ϕ .
ek ek+1
B Detailed Calculations
In this appendix we describe detailed calculations omitted in the main paper.
B.1 Calculations for L1-loss L2-regularized SVM (Section 4.1)
For this setup, we can calculate as

1
(cid:40)
t, (−1≤t≤0)
(cid:26)
1
(cid:27) {−1}, (t<1)
ρ∗(β):= ∥β∥2, ℓ∗(t):= ∂ρ∗(β):= β , ∂ℓ (t):= [−1,0], (t=1)
2λ 2 y +∞, (otherwise) λ y {0}.
(t>1)
Then we have the dual problem in the main paper (6).
B.2 Calculations for L2-loss L1-regularized SVM (Section 4.2)
For this setup, we can calculate as
(cid:40) (cid:40)
0, (β =0, ∀j ∈[d−1]: |β |≤λ) t2+4t, (t≤0)
ρ∗(β):= d j ℓ∗(t):= 4
+∞, (otherwise) y +∞, (otherwise)

− [−∞ ∞,
,0],
( (β
βj
j
< =− −λ λ)
)

−∞, (β
d
<0)
∀j ∈[d−1]: [∂ρ∗(β)] := 0, (|β |<λ) [∂ρ∗(β)] := [−∞,+∞], (β =0)
j j d d
[
+0, ∞+ ,∞], ( (β
βj
= >λ λ)
)
+∞,
(β
d
>0)
j
∂ℓ (t):=−2max{0,1−t}.
y
Then, setting γ =λ for all i∈[n], the dual objective function is described as
i
D
(α)=(cid:40) −(cid:80)n i=1w iλ2α2 i− 44λαi, (if (32) are satisfied)
(31)
w
+∞, (otherwise)
where
λα ≥0⇔α ≥0, (32a)
i i
∀j ∈[d−1]: |((λ1 ⊗w)⊗Xˇ )⊤α|≤λ⇔|(w⊗Xˇ )⊤α|≤1, (32b)
n :j :j
((λ1 ⊗w)⊗Xˇ )⊤α=0⇔(w⊗Xˇ )⊤α=0. (32c)
n :d :d
20Optimality conditions (4) and (5) are described as
∀j ∈[d−1]: |(λ1 ⊗w⊗Xˇ )⊤α∗(w)|<λ⇔|(w⊗Xˇ )⊤α∗(w)|<1⇒β∗(w) =0, (33)
n :j :j j
∀i∈[n]: λα∗(w) =2max{0,1−Xˇ β∗(w)}. (34)
i i:
C Application of Safe Sample Screening to Kernelized Features
The kernel method in ML means computation methods when the input variable vector of a sample x∈Rd
cannot be specifically obtained (this includes the case when d is infinite), but for the input variable vectors
for any two samples x,x′ ∈Rd its inner product x⊤x′ can be obtained. In such a case, we cannot discuss
SfS since we cannot obtain each feature specifically, however, we can discuss SsS.
We show that the SsS rules for L1-loss L2-regularized SVM (Section 4.1) can be applied even if the features
are kernelized.
First, if features are kernelized, we cannot obtain either X or β∗(w˜) specifically. However, since we can
obtain α∗(w˜), with (7) we have
n
∀x∈Rd : x⊤β∗(w˜) = 1 x⊤(w□×Xˇ)⊤α∗(w˜) = 1 (cid:88) w α∗(w˜)(x⊤Xˇ ). (35)
λ λ i i i:
i=1
This means that we can calculate the inner product of β∗(w˜) and any vector.
Then, in order to calculate the quantity (9) to conduct SsS, we have only to calculate
• Xˇ β∗(w˜) can be calculated by (35),
i:
(cid:113)
• ∥Xˇ ∥ = Xˇ⊤Xˇ is obtained as the kernel value, and
i: 2 i: i:
• P (β∗(w˜))−D (α∗(w˜)) can be calculated by (35) and kernel values since two variables whose values
w w
cannot be specifically obtained (X˜ and β∗(w˜)) appears only as inner products.
So, all values needed to derive SsS rules (9) can be computed even if features are kernelized.
D Details of Experiments
D.1 Detailed Experimental Setup
The criteria of selecting datasets (Table 2) and detailed setups are as follows:
• All of the datasets are downloaded from LIBSVM dataset [22]. We used scaled datasets for ones used
in DRSfS or only scaled datasets are provided (“ionosphere”, “sonar” and “splice”). We used training
datasets only if test datasets are provided separately (“splice”, “svmguide1” and “madelon”).
• For DRSsS, we selected datasets from LIBSVM dataset containing 100 to 10,000 samples, 100 or fewer
features, and the area under the curve (AUC) of the receiver operating characteristic (ROC) is 0.9 or
higher for the regularization strengths (λ) we examined so that they tend to facilitate more effective
sample screening.
21• For DRSfS, we selected datasets from LIBSVM dataset containing 50 to 1,000 features, 10,000 or fewer
samples, and containing no categorical features. Also, due to computational constraints, we excluded
features that have at least one zero (marked “†” in Table 2). As a result, one feature from “madelon”
and one from “sonar” have been excluded.
• In the table, the column “d” denotes the number of features including the intercept feature (Remark
2.2).
The choice of regularization hyperparameter λ, based on the characteristics of the data, is as follows:
• For DRSsS, we set λ as n, n×10−0.5, n×10−1.0, ..., n×10−3.0. (For DRSsS with DL, we set 1000
instead of n.) This is because the effect of λ gets weaker for larger n.
• For DRSfS, we determine λ based on λ , defined as the smallest λ for which β∗(w) = 0 for any
max j
j ∈[d−1] explained below. We then set λ as λ , λ ×10−1/3, λ ×10−2/3, ..., λ ×10−2.
max max max max
Finally, we show the calculation of λ for L2-loss L1-regularized SVM. By (14), we would like to find λ
max
so that |(w⊗Xˇ )⊤α∗(w)|<1 for all j ∈[d−1]. In order to judge this, we need α∗(w), which is calculated
:j
as follows:
• Solve the primal problem (1) for L2-loss L1-regularized SVM by fixing β∗(w) = 0 for any j ∈ [d−1],
j
that is,
n n
β∗(w) =argmin(cid:88) w ℓ (xˇ β )=argmin(cid:88) w (max{0,1−y β })2
d i yi id d i i d
βd
i=1
βd
i=1
(cid:88) (cid:88)
=argmin w (max{0,1−β })2+ w (max{0,1+β })2
i d i d
βd
i∈[n], yi=+1 i∈[n], yi=−1
(cid:80) (cid:80)
w − w
=
i∈[n], yi=+1 i i∈[n], yi=−1 i
.
(cid:80)n
w
i=1 i
• With β∗(w) computed above and β∗(w) =0 for any j ∈[d−1], calculate α$ =λα∗(w) =[2max{0,1−
d j
Xˇ β∗(w)}]n by (15).
i: i=1
• If |(w ⊗Xˇ )⊤α$)| < λ for all j ∈ [d−1], then β∗(w) = 0 for any j ∈ [d−1]. So, we set λ =
:j j max
max |(w⊗Xˇ )⊤α$)|.
j∈[d−1] :j
D.2 All Experimental Results of Section 6.2
For the experiment of Section 6.2, ratios of screened samples by DRSsS setup is presented in Figure 7, while
ratios of screened features by DRSfS setup in Figure 8.
22Dataset: australian
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
Dataset: breast-cancer
1.0
=0.690
=2.181 0.8
=6.9
=21.81 0.6
=69.0
0.4
=218
=690 0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
=0.683
=2.159
=6.83
=21.59
=68.3
=215
=683
Dataset: heart
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
Dataset: ionosphere
1.0
=0.27
=0.853 0.8
=2.7 =8.538 0.6
=27.0
=85.38 0.4
=270 0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
=0.351
=1.109
=3.510 =11.09
=35.1
=110
=351
Dataset: sonar
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
Dataset: splice
1.0
=0.208
=0.657 0.8
=2.08 =6.577 0.6
=20.8
=65.77 0.4
=208 0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
=1.0
=3.162
=10.0 =31.62
=100
=316
=1000
Dataset: svmguide1
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
selpmas
deneercs
fo oitaR
=3.089
=9.768
=30.89
=97.68
=308
=976
=3089
Figure 7: Ratios of screened samples by DRSsS.
23Dataset: madelon
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
serutaef
deneercs
fo oitaR
Dataset: sonar
1.0
=7.32e+2
=1.58e+3 0.8
=3.40e+3
=7.32e+3 0.6
=1.58e+4
0.4 =3.40e+4
=7.32e+4 0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
serutaef
deneercs
fo oitaR
=7.48e 1
=1.61e+0
=3.47e+0
=7.48e+0
=1.61e+1
=3.47e+1
=7.48e+1
Dataset: splice
1.0
0.8
0.6
0.4
0.2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
serutaef
deneercs
fo
oitaR
=7.34e+0
=1.58e+1
=3.41e+1
=7.34e+1
=1.58e+2
=3.41e+2
=7.34e+2
Figure 8: Ratios of screened features by DRSfS.
24