Distributionally Robust Safe Screening
Hiroyuki Hanadaâˆ—â€ Satoshi Akahaneâ€¡Tatsuya Aoyamaâ€¡Tomonari Tanakaâ€¡
Yoshito Okuraâ€¡Yu InatsuÂ§Noriaki Hashimotoâˆ—Taro MurayamaÂ¶Lee HanjuÂ¶
Shinya KojimaÂ¶Ichiro Takeuchiâ€¡âˆ—â€–
April 26, 2024
Abstract
In this study, we propose a method Distributionally
Robust Safe Screening (DRSS) , for identifying unnec-
essary samples and features within a DR covariate
shift setting. This method effectively combines DR
learning, a paradigm aimed at enhancing model ro-
bustness against variations in data distribution, with
safe screening (SS), a sparse optimization technique
designed to identify irrelevant samples and features
prior to model training. The core concept of the DRSS
method involves reformulating the DR covariate-shift
problem as a weighted empirical risk minimization
problem, where the weights are subject to uncertainty
within a predetermined range. By extending the SS
technique to accommodate this weight uncertainty,
the DRSS method is capable of reliably identifying
unnecessary samples and features under any future
distribution within a specified range. We provide a
theoretical guarantee of the DRSS method and vali-
date its performance through numerical experiments
on both synthetic and real-world datasets.
âˆ—RIKEN, Wako, Saitama, Japan
â€ hiroyuki.hanada@riken.jp
â€¡Nagoya University, Nagoya, Aichi, Japan
Â§Nagoya Institute of Technology, Nagoya, Aichi, Japan
Â¶DENSO CORPORATION, Kariya, Aichi, Japan
â€–ichiro.takeuchi@mae.nagoya-u.ac.jp1 Introduction
In this study, we consider the problem of identi-
fying unnecessary samples and features in a class
of supervised learning problems within dynamically
changing environments. Identifying unnecessary sam-
ples/features offers several benefits. It helps in de-
creasing the storage space required for keeping the
training data for updating the machine learning (ML)
models in the future. Moreover, in situations demand-
ing real-time adaptation of ML models to quick envi-
ronmental changes, the use of fewer samples/features
enables more efficient learning.
Our basic idea to tackle this problem is to effectively
combine distributionally robust (DR) learning and safe
screening (SS) . DR learning is a ML paradigm that
focuses on developing models robust to variations in
the data distribution, providing performance guaran-
tees across different distributions (see, e.g., [ 1]). On
the other hand, SS refers to sparse optimization tech-
niques that can identify irrelevant samples/features
before model training, ensuring computational effi-
ciency by avoiding unnecessary computations on cer-
tain samples/features which do not contribute to the
final solution [ 2,3]. The key technical idea of SS is
to identify a bound of the optimal solution before
solving the optimization problem. This allows for the
identification of unnecessary samples/features, even
without knowing the optimal solution.
As a specific scenario of dynamically changing en-
vironment, we consider covariate shift setting [ 4,5]
with unknown test distribution. In this setting, the
1arXiv:2404.16328v1  [stat.ML]  25 Apr 2024distribution of input features in the training data may
undergo changes in the test phase, yet the actual
nature of these changes remains unknown. A ML
problem (e.g., regression/classification problem) in
covariate shift setting can be formulated as a weighted
empirical risk minimization (weighted ERM) problem,
where weights are assigned based on the density ratio
of each sample in the training and test distributions.
Namely, by assigning higher weights to training sam-
ples that are important in the test distribution, the
model can focus on learning from relevant samples
and mitigate the impact of distribution differences
between the training and the test phases. If the distri-
bution during the test phase is known, the weights can
be uniquely fixed. However, if the test distribution
is unknown, it is necessary to solve a weighted ERM
problem with unknown weights.
Our main contribution is to propose a DRSS
method for covariate shift setting with unknown test
distribution. The proposed method can identify un-
necessary samples/features regardless of how the dis-
tribution changes within a certain range in the test
phase. To address this problem, we extend the exist-
ing SS methods in two stages. The first is to extend
the SS for ERM so that it can be applied to weighted
ERM. The second is to further extend the SS so that
it can be applied to weighted ERM when the weights
are unknown. While the first extension is relatively
straightforward, the second extension presents a non-
trivial technical challenge (Figure 1). To overcome
this challenge, we derive a novel bound of the optimal
solutions of the weighted ERM problem, which prop-
erly accounts for the uncertainty in weights stemming
from the uncertainty of the test distribution.
In this study, we consider DRSS for samples in
sample-sparse models such as SVM [ 6], and that for
features for feature-sparse models such as Lasso [ 7].
We denote the DRSS for samples as distributionally
robust safe sample screening (DRSsS) and that for fea-
tures as distributionally robust safe feature screening
(DRSfS), respectively.
Our contributions in this study are summarized
as follows. First, by effectively combining DR and
SS, we introduce a framework for identifying unnec-
essary samples/features under dynamically changing
uncertain environment. Second, We consider a DR
Safe Sample ScreeningUnknown
Test Distribution
DRSS Method (Proposed)Distributionally
Robust
Safe Screening
AB C
DFigure 1: Schematic illustration of the proposed Dis-
tributionally Robust Safe Screening (DRSS) method.
Panel Adisplays the training samples, each assigned
equal weight, as indicated by the uniform size of the
points. Panel Bdepicts various unknown test distri-
butions, highlighting how the significance of training
samples varies with different realizations of the test
distribution. Panel Cshows the outcomes of safe
sample screening (SsS) across multiple realizations
of test distributions. Finally, Panel Dpresents the
results of the proposed DRSS method, demonstrating
its capability to identify redundant samples regardless
of the observed test distribution.
covariate-shift setting where the input distribution
of an ERM problem changes within a certain range.
In this setting, we propose a novel method called
DRSS method that can identify samples/features that
are guaranteed not to affect the optimal solution, re-
gardless of how the distribution changes within the
specified range. Finally, through numerical exper-
iments, we verify the effectiveness of the proposed
DRSS method. Although the DRSS method is devel-
oped for convex ERM problems, in order to demon-
strate the applicability to deep learning models, we
also present results where the DRSS method is ap-
plied in a problem setting where the final layer of the
model is fine-tuned according to changes in the test
distribution.
21.1 Related Works
The DR setting has been explored in various ML prob-
lems, aiming to enhance model robustness against
data distribution variations. A DR learning problem
is typically formulated as a worst-case optimization
problem since the goal of DR learning is to ensure
model performance under the worst-case data dis-
tribution within a specified range. Hence, a variety
of optimization techniques tailored to DR learning
have been investigated within both the ML and opti-
mization communities [ 8,9,1]. The proposed DRSS
method is one of such DR learning methods, focusing
specifically on the problem of sample/feature deletion.
The ability to identify irrelevant samples/features is
of practical significance. For example, in the context
of continual learning (see, e.g., [ 10]), it is crucial to
effectively manage data by selectively retaining and
discarding samples/features, especially in anticipa-
tion of changes in future data distributions. Incorrect
deletion of essential data can lead to catastrophic for-
getting [11], a phenomenon where a ML model, after
being trained on new data, quickly loses information
previously learned from older datasets. The proposed
DRSS method tackles this challenge by identifying
samples/features that, regardless of future data dis-
tribution shifts, will not have any influence on all
possible newly trained model in the future.
SS refers to optimization techniques in sparse
learning that identify and exclude irrelevant sam-
ples or features from the learning process. SS can
reduce computational cost without changing the fi-
nal trained model. Initially, SfS was introduced by
[2] for the Lasso. Subsequently, SsS was proposed
by [3] for the SVM. Among various SS methods de-
veloped so far, the most commonly used is based
on the duality gap [ 12,13]. Our proposed DRSS
method also adopts this approach. Over the past
decade, SS has seen diverse developments, including
methodological improvements and expanded applica-
tion scopes [ 14,15,16,17,18,19,20,21]. Unlike
other SS studies that primarily focused on reducing
computational costs, this study adopts SS for a dif-
ferent purpose. We employ SS across scenarios where
data distribution varies within a defined range, aim-
ing to discard unnecessary samples/features. To ourTable 1: Notations used in the paper. R: all real num-
bers,N: all positive integers, n, m, p âˆˆN: integers,
f:Rnâ†’Râˆª {+âˆž}: convex function, MâˆˆRnÃ—m:
matrix, vâˆˆRn: vector.
mijâˆˆR(small case of matrix variable)
the element at the ithrow and
thejthcolumn of M
viâˆˆR(nonbold font of vector variable)
theithelement of v
Mi:âˆˆR1Ã—ntheithrow of M
M:jâˆˆRmÃ—1thejthcolumn of M
[n] {1,2, . . . , n }
Râ‰¥0 all nonnegative real numbers
âŠ— elementwise product
diag(v)âˆˆRnÃ—ndiagonal matrix; (diag( v))ii=vi
and (diag( v))ij= 0 ( iÌ¸=j)
vÃ—â–¡MâˆˆRnÃ—mdiag(v)M
0nâˆˆRn[0,0, . . . , 0]âŠ¤(vector of size n)
1nâˆˆRn[1,1, . . . , 1]âŠ¤(vector of size n)
âˆ¥vâˆ¥pâˆˆRâ‰¥0 (Pn
i=1vp
i)1/p(p-norm )
âˆ‚f(v)âŠ†RnallgâˆˆRns.t. â€œfor any vâ€²âˆˆRn,
f(vâ€²)âˆ’f(v)â‰¥gâŠ¤(vâ€²âˆ’v)â€
(subgradient )
Z[f]âŠ†Rn{vâ€²âˆˆRn|âˆ‚f(vâ€²) ={0n}}
fâˆ—(v)âˆˆRâˆª {+âˆž} supvâ€²âˆˆRn(vâŠ¤vâ€²âˆ’f(vâ€²))
(convex conjugate )
â€œfisÎº-strongly f(v)âˆ’Îºâˆ¥vâˆ¥2
2is convex with
convexâ€ ( Îº >0) respect to v
â€œfisÂµ-smoothâ€ âˆ¥f(v)âˆ’f(vâ€²)âˆ¥2â‰¤Âµâˆ¥vâˆ’vâ€²âˆ¥2
(Âµ >0) for any v,vâ€²âˆˆRn
knowledge, no existing studies have utilized SS within
the DR learning framework.
2 Preliminaries
Notations used in this paper are described in Table 1.
32.1 Weighted Regularized Empiri-
cal Risk Minimization (Weighted
RERM) for Linear Prediction
We mainly assume the weighted regularized empirical
risk minimization (weighted RERM) for linear pre-
diction. This may include kernelized versions, which
are discussed in Appendix C. Suppose that we learn
the model parameters as linear prediction coefficients,
that is, learn Î²âˆ—(w)âˆˆRdsuch that the outcome for a
sample xâˆˆRdis predicted as xâŠ¤Î²âˆ—(w).
Definition 2.1. Given ntraining samples of d-
dimensional input variables, scalar output variables
and scalar sample weights, denoted by XâˆˆRnÃ—d,
yâˆˆRnandwâˆˆRn
â‰¥0, respectively, the training com-
putation of weighted RERM for linear prediction is
formulated as follows:
Î²âˆ—(w):= argmin
Î²âˆˆRdPw(Î²),where
Pw(Î²) :=nX
i=1wiâ„“yi(Ë‡Xi:Î²) +Ï(Î²). (1)
Here, â„“y:Râ†’Ris a convex loss function1,Ï:Rdâ†’
Ris a convex regularization function , and Ë‡XâˆˆRnÃ—d
is a matrix calculated from Xandyand determined
depending on â„“. In this paper, unless otherwise noted,
we consider binary classifications ( yâˆˆ {âˆ’ 1,+1}n)
with Ë‡X:=yÃ—â–¡X. For regressions ( yâˆˆRn) we usually
setË‡X:=X.
Remark 2.2. We add that, we adopt the formulation
X:d=1nso that Î²âˆ—(w)
d(the last element) represents
the common coefficient for any sample (called the
intercept ).
Since â„“andÏare convex, we can easily confirm that
Pw(Î²) is convex with respect to Î².
Applying Fenchelâ€™s duality theorem (Appendix A.2),
1Forâ„“y(t), we assume that only tis a variable of the function
(yis assumed to be a constant) when we take its subgradient
or convex conjugate.we have the following dual problem of (1):
Î±âˆ—(w):= argmax
Î±âˆˆRnDw(Î±),where
Dw(Î±) := (2)
âˆ’nX
i=1wiâ„“âˆ—
yi(âˆ’Î³iÎ±i)âˆ’Ïâˆ—(((Î³âŠ—w)Ã—â–¡Ë‡X)âŠ¤Î±),
where Î³is a positive-valued vector. The relationship
between the original problem (1)(called the primal
problem) and the dual problem (2)are described as
follows:
Pw(Î²âˆ—(w)) =Dw(Î±âˆ—(w)), (3)
Î²âˆ—(w)âˆˆâˆ‚Ïâˆ—(((Î³âŠ—w)Ã—â–¡Ë‡X)âŠ¤Î±âˆ—(w)), (4)
âˆ€iâˆˆ[n] :âˆ’Î³iÎ±âˆ—(w)
iâˆˆâˆ‚â„“yi(Ë‡Xi:Î²âˆ—(w)).(5)
2.2 Sparsity-inducing Loss Functions
and Regularization Functions
In weighted RERM, we call that a loss function â„“
induces sample-sparsity if elements in Î±âˆ—(w)are easy
to become zero. Due to (5), this can be achieved by â„“
such that {tâˆˆR|0âˆˆâˆ‚â„“y(t)}is not a point but an
interval.
Similarly, we call that a regularization function Ï
induces feature-sparsity if elements in Î²âˆ—(w)are easy
to become zero. Due to (4), this can be achieved by
Ïsuch that {vâˆˆRd| âˆƒjâˆˆ[dâˆ’1] : 0 âˆˆ[âˆ‚Ïâˆ—(v)]j}is
not a point but a region.
For example, the hinge loss â„“y(t) =max{0,1âˆ’t}
(yâˆˆ {âˆ’ 1,+1}) is a sample-sparse loss function since
{tâˆˆR|0âˆˆâˆ‚â„“y(t)}= [1,+âˆž). Similarly, the L1-
regularization Ï(v) =Î»Pdâˆ’1
j=1|vj|(Î» > 0: hyperpa-
rameter) is a feature-sparse regularization function
since{vâˆˆRd| âˆƒjâˆˆ[dâˆ’1] : 0 âˆˆ[âˆ‚Ïâˆ—(v)]j}={vâˆˆ
Rd| âˆƒjâˆˆ[dâˆ’1] :|vj| â‰¤Î», v d= 0}. See Section 4
for examples of using them.
3Distributionally Robust Safe
Screening
In this section we show DRSS rules for weighted
RERM with two steps. First, in Sections 3.1 and
43.2, we show SS rules for weighted RERM but not
DR setup. To do this, we extended existing SS rules
in [13,15]. Then we derive DRSS rules in Section 3.3.
3.1 (Non-DR) Safe Sample Screening
We consider identifying training samples that do not
affect the training result Î²âˆ—(w). Due to the relation-
ship (4), if there exists iâˆˆ[n] such that Î±âˆ—(w)
i= 0,
then the ithrow (sample) in Ë‡Xdoes not affect Î²âˆ—(w).
However, since computing Î±âˆ—(w)is as costly as Î²âˆ—(w),
it is difficult to use the relationship as it is. To solve
the problem, the SsS first considers identifying the
possible region Bâˆ—(w)âŠ‚Rdsuch that Î²âˆ—(w)âˆˆ Bâˆ—(w)
is assured. Then, with Bâˆ—(w)and(5), we can conclude
that the ithtraining sample do not affect the training
result Î²âˆ—(w)ifS
Î²âˆˆBâˆ—(w)âˆ‚â„“yi(Ë‡Xi:Î²) ={0}.
First we show how to compute Bâˆ—(w). In this paper
we adopt the computation methods that is available
when the regularization function ÏinPw(and also
Pwitself) of (1) are strongly convex.
Lemma 3.1. Suppose that ÏinPw(and also Pw
itself) of (1)areÎº-strongly convex. Then, for any
Ë†Î²âˆˆRdandË†Î±âˆˆRn, we can assure Î²âˆ—(w)âˆˆ Bâˆ—(w)by
taking
Bâˆ—(w):=n
Î²âˆ¥Î²âˆ’Ë†Î²âˆ¥2â‰¤r(w,Î³, Îº,Ë†Î²,Ë†Î±)o
,
where r(w,Î³, Îº,Ë†Î²,Ë†Î±) :=r
2
Îº[Pw(Ë†Î²)âˆ’Dw(Ë†Î±)].
The proof is presented in Appendix A.3. The
amount Pw(Ë†Î²)âˆ’Dw(Ë†Î±) is known as the duality gap ,
which must be nonnegative due to (3). So we ob-
tain the following gap safe sample screening rule from
Lemma 3.1:
Lemma 3.2. Under the same assumptions as Lemma
3.1,Î±âˆ—(w)
i= 0is assured (i.e., the ithtraining sample
does not affect the training result Î²âˆ—(w)) if there exists
Ë†Î²âˆˆRdandË†Î±âˆˆRnsuch that
[Ë‡Xi:Ë†Î²âˆ’ âˆ¥Ë‡Xi:âˆ¥2r(w,Î³, Îº,Ë†Î²,Ë†Î±),
Ë‡Xi:Ë†Î²+âˆ¥Ë‡Xi:âˆ¥2r(w,Î³, Îº,Ë†Î²,Ë†Î±)]âŠ† Z[â„“yi].
The proof is presented in Appendix A.4.3.2 (Non-DR) Safe Feature Screening
We consider identifying jâˆˆ[d] such that Î²âˆ—(w)
j= 0,
that is, identifying that the jthfeature is not used in
the prediction, even when the sample weights ware
changed.
For simplicity, suppose that the regularization func-
tionÏis decomposable, that is, Ïis represented as
Ï(Î²) :=Pd
j=1Ïƒj(Î²j) (Ïƒ1, Ïƒ2, . . . , Ïƒ d:Râ†’R). Then,
since Ïâˆ—(v) =Pd
j=1Ïƒâˆ—
j(vj) and therefore [ âˆ‚Ïâˆ—(v)]j=
âˆ‚Ïƒâˆ—
j(vj), from (4) we have
Î²âˆ—(w)
jâˆˆâˆ‚Ïƒâˆ—
j((Î³âŠ—wâŠ—Ë‡X:j)âŠ¤Î±âˆ—(w))
=âˆ‚Ïƒâˆ—
j(Ë‡Ë‡X(Î³,w)âŠ¤
:j Î±âˆ—(w)),
whereË‡Ë‡X(Î³,w)
:j :=Î³âŠ—wâŠ—Ë‡X:j.
If we know Î±âˆ—(w), we can identify whether Î²âˆ—(w)
j = 0
holds. However, like SsS (Section 3.1), we would like
to check the condition without computing Î±âˆ—(w)or
Î²âˆ—(w).
So, like SsS, SfS first considers identifying the pos-
sible region Aâˆ—(w)âŠ‚Rnsuch that Î±âˆ—(w)âˆˆ Aâˆ—(w)is
assured. Then we can conclude that Î²âˆ—(w)
j = 0 is
assured ifS
Î±âˆˆAâˆ—(w)âˆ‚Ïƒâˆ—
j(Ë‡Ë‡X(Î³,w)âŠ¤
:j Î±) ={0}.
Then we show how to compute Aâˆ—(w). With Lemma
A.3, we can calculate Aâˆ—(w)as follows, if the loss
function â„“yinPwof (1) is smooth:
Lemma 3.3. Suppose that â„“yinPwof(1)isÂµ-
smooth. Then, for any Ë†Î²âˆˆRdand Ë†Î±âˆˆRn, we can
assure Î±âˆ—(w)âˆˆ Aâˆ—(w)by taking
Aâˆ—(w):=n
Î±âˆ¥Î±âˆ’Ë†Î±âˆ¥2â‰¤Â¯r(w,Î³, Âµ,Ë†Î²,Ë†Î±)o
,
where Â¯r(w,Î³, Âµ,Ë†Î²,Ë†Î±) :=
s
2Âµ
miniâˆˆ[n]wiÎ³2
i[Pw(Ë†Î²)âˆ’Dw(Ë†Î±)].
The proof is presented in Appendix A.5. Similar to
Lemma 3.2, we obtain the gap safe feature screening
rulefrom Lemma 3.3:
Lemma 3.4. Under the same assumptions as Lemma
3.3,Î²âˆ—(w)
j = 0is assured (i.e., the jthfeature does
5not affect prediction results) if there exists Ë†Î²âˆˆRd
andË†Î±âˆˆRnsuch that
[Ë‡Ë‡X(Î³,w)âŠ¤
:j Ë†Î±âˆ’ âˆ¥Ë‡Ë‡X(Î³,w)
:jâˆ¥2Â¯r(w,Î³, Âµ,Ë†Î²,Ë†Î±),
Ë‡Ë‡X(Î³,w)âŠ¤
:j Ë†Î±+âˆ¥Ë‡Ë‡X(Î³,w)
:jâˆ¥2Â¯r(w,Î³, Âµ,Ë†Î²,Ë†Î±)]âŠ† Z[Ïƒâˆ—
j].
The proof is almost same as Lemma 3.2.
3.3 Application to Distributionally Ro-
bust Setup
In Sections 3.1 and 3.2 we showed the conditions when
samples or features are screened out. In this section
we show how to use the conditions for the change of
sample weights w.
Definition 3.5 (weight-changing safe screening
(WCSS)) .Given XâˆˆRnÃ—d,yâˆˆRn,ËœwâˆˆRn
â‰¥0and
wâˆˆRn
â‰¥0, suppose that Î²âˆ—(Ëœw)in Definition 2.1 (and
alsoÎ±âˆ—(Ëœw)) are already computed, but Î²âˆ—(w)not.
Then WCSsS (resp. WCSfS) from Ëœwtowis de-
fined as finding iâˆˆ[n] satisfying Lemma 3.2 (resp.
jâˆˆ[dâˆ’1] satisfying Lemma 3.4).
Definition 3.6 (Distributionally robust safe screening
(DRSS)) .Given XâˆˆRnÃ—d,yâˆˆRn,ËœwâˆˆRn
â‰¥0and
W âŠ‚Rn
â‰¥0, suppose that Î²âˆ—(Ëœw)in Definition 2.1 (and
alsoÎ±âˆ—(Ëœw)) are already computed. Then the DRSsS
(resp. DRSfS) for Wis defined as finding iâˆˆ[n]
satisfying Lemma 3.2 (resp. jâˆˆ[dâˆ’1] satisfying
Lemma 3.4) for any wâˆˆ W.
For Definition 3.5, we have only to apply SS rules
in Lemma 3.2 or 3.4 by setting Ë†Î²â†Î²âˆ—(Ëœw)and Ë†Î±â†
Î±âˆ—(Ëœw). On the other hand, for Definition 3.6, we need
to maximize or minimize the interval in Lemma 3.2
or 3.4 in wâˆˆ W.
Theorem 3.7. The DRSsS rule for Wis calculated
as:
[Ë‡Xi:Î²âˆ—(Ëœw)âˆ’ âˆ¥Ë‡Xi:âˆ¥2R,Ë‡Xi:Î²âˆ—(Ëœw)+âˆ¥Ë‡Xi:âˆ¥2R]âŠ† Z[â„“yi],
where R:= max wâˆˆWr(w,Î³, Îº,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw)).Similarly, the DRSfS rule for Wis calculated as:
[Lâˆ’NR,L+NR]âŠ† Z[Ïƒâˆ—
j],where
L:= min
wâˆˆWË‡Ë‡X(Î³,w)âŠ¤
:j Î±âˆ—(Ëœw)= min
wâˆˆW(Î³âŠ—Ë‡X:jâŠ—Î±âˆ—(Ëœw))âŠ¤w,
L:= max
wâˆˆWË‡Ë‡X(Î³,w)âŠ¤
:j Î±âˆ—(Ëœw)= max
wâˆˆW(Î³âŠ—Ë‡X:jâŠ—Î±âˆ—(Ëœw))âŠ¤w,
N:= max
wâˆˆWâˆ¥Ë‡Ë‡X(Î³,w)
:jâˆ¥2=r
max
wâˆˆWâˆ¥wâŠ—Î³âŠ—Ë‡X:jâˆ¥2
2,
R:= max
wâˆˆWÂ¯r(w,Î³, Âµ,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw)).
Thus, solving the maximizations and/or minimiza-
tions in Theorem 3.7 provides DRSsS and DRSfS
rules. However, how to solve it largely depends on the
choice of â„“,ÏandW. In Section 4 we show specific
calculations of Theorem 3.7 for some typical setups.
4DRSS for Typical ML Setups
In this section we show DRSS rules derived in Section
3.3 for two typical ML setups: DRSsS for L1-loss L2-
regularized SVM (Section 4.1) and DRSfS for L2-loss
L1-regularized SVM (Section 4.2) under W:={w|
âˆ¥wâˆ’Ëœwâˆ¥2â‰¤S}.
In the processes, we need to solve constrained max-
imizations of convex functions. Although maximiza-
tions of convex functions are not easy in general (min-
imizations are easy), we show that the maximizations
need in the processes can be algorithmically solved in
Section 4.3.
4.1 DRSsS for L1-loss L2-regularized
SVM
L1-loss L2-regularized SVM is a sample-sparse model
for binary classification ( yâˆˆ {âˆ’ 1,+1}n) that satisfies
the preconditions to apply SsS (Lemma 3.1). Detailed
calculations are presented in Appendix B.1.
For L1-loss L2-regularized SVM, we set Ïandâ„“as:
Ï(Î²) :=Î»
2âˆ¥Î²âˆ¥2
2(Î» >0 : hyperparameter) ,
â„“y(t) := max {0,1âˆ’t}(where yâˆˆ {âˆ’ 1,+1}).
6Then ÏisÎ»-strongly convex. Setting Î³=1n, the dual
objective function is described as
Dw(Î±) =ï£±
ï£²
ï£³Pn
i=1wiÎ±iâˆ’1
2Î»Î±âŠ¤(wÃ—â–¡Ë‡X)(wÃ—â–¡Ë‡X)âŠ¤Î±,
(âˆ€iâˆˆ[n] : 0â‰¤Î±iâ‰¤1)
âˆ’âˆž. (otherwise)(6)
Here, in the viewpoint of minimization, we may con-
sider this problem as a maximization with the con-
straint â€œ âˆ€iâˆˆ[n] : 0â‰¤Î±iâ‰¤1â€.
Optimality conditions (4)and(5)are described as:
Î²âˆ—(w)=1
Î»(wÃ—â–¡Ë‡X)âŠ¤Î±âˆ—(w), (7)
âˆ€iâˆˆ[n] :Î±âˆ—(w)
iâˆˆï£±
ï£´ï£²
ï£´ï£³{1},(Ë‡Xi:Î²âˆ—(w)â‰¤1)
[0,1],(Ë‡Xi:Î²âˆ—(w)= 1)
{0}.(Ë‡Xi:Î²âˆ—(w)â‰¥1)(8)
Noticing that Z[â„“yi] = (1 ,+âˆž), by Theorem 3.7,
the DRSsS rule for Wis calculated as:
Ë‡Xi:Î²âˆ—(Ëœw)âˆ’ âˆ¥Ë‡Xi:âˆ¥2max
wâˆˆWr(w,Î³, Îº,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw))>1,
where (9)
r(w,Î³, Îº,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw))
:=r
2
Îº[Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw))],
Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw))
:=nX
i=1wi[â„“yi(Ë‡Xi:Î²âˆ—(Ëœw))âˆ’Î±âˆ—(Ëœw)
i] +Î»âˆ¥Î²âˆ—(Ëœw)âˆ¥2
2
+1
2Î»wâŠ¤(Î±âˆ—(Ëœw)Ã—â–¡Ë‡X)(Î±âˆ—(Ëœw)Ã—â–¡Ë‡X)âŠ¤w.
Here, we can find that Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw)), which
we need to maximize in reality, is the sum of linear
function and convex quadratic function with respect
towâˆˆ W. (Since ( Î±âˆ—(Ëœw)Ã—â–¡Ë‡X)(Î±âˆ—(Ëœw)Ã—â–¡Ë‡X)âŠ¤is positive
semidefinite, we know that it is convex quadratic). Al-
though constrained maximization of a convex function
is difficult in general, for this case we can algorithmi-
cally maximize it (Section 4.3).4.2 DRSfS for L2-loss L1-regularized
SVM
L2-loss L1-regularized SVM is a feature-sparse model
for binary classification ( yâˆˆ {âˆ’ 1,+1}n) that satisfies
the preconditions to apply SfS (Lemma 3.3). Detailed
calculations are presented in Appendix B.2.
For L2-loss L1-regularized SVM, we set Ïƒj(and
consequently Ï) and â„“as:
âˆ€jâˆˆ[dâˆ’1] :Ïƒj(Î²j) :=Î»|Î²j|(Î» >0 : hyperparameter) ,
Ïƒd(Î²d) := 0 ,
â„“y(t) := (max {0,1âˆ’t})2(where yâˆˆ {âˆ’ 1,+1}).
Notice that Ïƒd(Î²d) is not defined as Î»|Î²d|but 0: we
rarely regularize the intercept with L1-regularization.
Setting Î³=Î»1n, the dual objective function is
described as
Dw(Î±) =(
âˆ’Î»Pn
i=1wiÎ»Î±2
iâˆ’4Î±i
4,((11)â€“(13) are met)
âˆ’âˆž, (otherwise)
(10)
where Î±iâ‰¥0, (11)
âˆ€jâˆˆ[dâˆ’1] :|(wâŠ—Ë‡X:j)âŠ¤Î±| â‰¤1, (12)
(wâŠ—Ë‡X:d)âŠ¤Î±= (wâŠ—y)âŠ¤Î±= 0. (13)
Optimality conditions (4) and (5) are described as
âˆ€jâˆˆ[dâˆ’1] :|(wâŠ—Ë‡X:j)âŠ¤Î±âˆ—(w)|<1â‡’Î²âˆ—(w)
j= 0,
(14)
âˆ€iâˆˆ[n] :Î±âˆ—(w)
i=2
Î»max{0,1âˆ’Ë‡Xi:Î²âˆ—(w)}.(15)
Noticing that Z[Ïƒâˆ—
j] = (âˆ’Î», Î»), by Theorem 3.7, the
DRSfS rule for Wis calculated as:
Lâˆ’NR >âˆ’Î»,L+NR < Î»,
7where
L:=Î»min
wâˆˆW(Ë‡X:jâŠ—Î±âˆ—(Ëœw))âŠ¤w,
L:=Î»max
wâˆˆW(Ë‡X:jâŠ—Î±âˆ—(Ëœw))âŠ¤w,
N:=Î»r
max
wâˆˆWâˆ¥wâŠ—Ë‡X:jâˆ¥2
2
=Î»r
max
wâˆˆW{wâŠ¤diag( Ë‡X:jâŠ—Ë‡X:j)w},
R:= max
wâˆˆWÂ¯r(w,Î³, Âµ,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw)),
Â¯r(w,Î³, Âµ,Î²âˆ—(Ëœw),Î±âˆ—(Ëœw))
:=s
2Âµ
miniâˆˆ[n]wiÎ³2
i[Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw))],
Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw))
=nX
i=1wi"
â„“yi(Ë‡Xi:Î²âˆ—(Ëœw)) +Î»Î»(Î±âˆ—(Ëœw))2
iâˆ’4Î±âˆ—(Ëœw)
i
4#
+Ï(Î²âˆ—(Ëœw)).
Here, the expressions in LandLare linear with
respect to w, and the expression in Ninside the square
root is convex and quadratic with respect to w. Also,
Ris decomposed to two maximizations2Âµ
miniâˆˆ[n]wiÎ³2
i
andPw(Î²âˆ—(w))âˆ’Dw(Î±âˆ—(w)), where the former is
easily computed while the latter is linear with respect
tow. So, similar to L1-loss L2-regularized SVM, we
can obtain the maximization result by maximizing
or minimizing the linear terms by Lemma A.4 in
Appendix A, and maximizing the convex quadratic
function by the method of Section 4.3.
4.3 Maximizing Linear and Convex
Quadratic Functions in Hyperball
Constraint
To derive DRSS rules of Sections 4.1 and 4.2, we
need to compute the following forms of optimizationproblems:
max
wâˆˆWwâŠ¤Aw+ 2bâŠ¤w, (16)
where W:={wâˆˆRn| âˆ¥wâˆ’Ëœwâˆ¥2â‰¤S},
ËœwâˆˆRn,bâˆˆRn,
AâˆˆRnÃ—n: symmetric, positive semidefinite,
nonzero .
Lemma 4.1. The maximization (16) is achieved by
the following procedure. First, we define QâˆˆRnÃ—n
andÎ¦ := diag(Ï•1, Ï•2, . . . , Ï• n)as the eigendecompo-
sition of Asuch that A=QâŠ¤Î¦Q,Qis orthogonal
(QQâŠ¤=QâŠ¤Q=I). Also, let Î¾:=âˆ’Î¦QËœwâˆ’QbâˆˆRn,
and
T(Î½) =nX
i=1Î¾i
Î½âˆ’Ï•i2
. (17)
Then, the maximization (16) is equal to the largest
value among them:
â€¢For each Î½such that T(Î½) =S2(see Lemma 4.2),
the value Î½S2+(Î½Ëœw+b)âŠ¤QâŠ¤(Î¦âˆ’Î½I)âˆ’1Î¾+bâŠ¤Ëœw,
and
â€¢For each Î½âˆˆ {Ï•1, Ï•2, . . . , Ï• n}(duplication re-
moved) such that â€œ âˆ€iâˆˆ[n] :Ï•i=Î½â‡’Î¾i= 0â€,
the value
max
Ï„âˆˆRn[Î½S2+ (Î½Ëœw+b)âŠ¤QâŠ¤Ï„+bâŠ¤Ëœw],
subject to âˆ€iâˆˆ FÎ½:Ï„i=Î¾i
Ï•iâˆ’Î½,
X
iâˆˆUÎ½Ï„2
i=S2âˆ’X
iâˆˆFÎ½Ï„2
i,
where UÎ½:={i|iâˆˆ[n], Ï•i=Î½},FÎ½:= [n]\ UÎ½.
(Note that the maximization is easily computed
by Lemma A.4.)
The proof is presented in Appendix A.6.
Lemma 4.2. Under the same definitions as Lemma
4.1, The equation T(Î½) =S2can be solved by the
following procedure: Let e:= [e1, e2, . . . , e N](Nâ‰¤n,
kÌ¸=kâ€²â‡’ekÌ¸=ekâ€²) be a sequence of indices such that
8ðœ™ð‘’1ðœ™ð‘’2ðœ™ð‘’3ðœ™ð‘’4ðœ™ð‘’ð‘ðœˆð‘†2Figure 2: An example of the expression T(Î½) (black
solid line) in Lemmas 4.1 and 4.2. Colored dash
lines denote terms in the summation ( Î¾ek/(Î½âˆ’Ï•ek))2.
We can see that, given an interval ( Ï•ek, Ï•ek+1) (kâˆˆ
[Nâˆ’1]), the function is convex.
1.ekâˆˆ[n]for any kâˆˆ[N],
2.iâˆˆ[n]is included in eif and only if Î¾iÌ¸= 0, and
3.Ï•e1â‰¤Ï•e2â‰¤ Â·Â·Â· â‰¤ Ï•eN.
Note that, if Ï•ek< Ï•ek+1(kâˆˆ[Nâˆ’1]), then T(Î½)
is a convex function in the interval (Ï•ek, Ï•ek+1)with
limÎ½â†’Ï•ek+0=limÎ½â†’Ï•ek+1âˆ’0= +âˆž. Then, unless
N= 0, each of the following intervals contains just
one solution of T(Î½) =S2:
â€¢Intervals (âˆ’âˆž, Ï•e1)and(Ï•eN,+âˆž).
â€¢LetÎ½#(k):=argminÏ•ek<Î½<Ï• ek+1T(Î½). For each
kâˆˆ[Nâˆ’1]such that Ï•ek< Ï•ek+1,
â€“intervals (Ï•ek, Î½#(k))and(Î½#(k), Ï•ek+1)if
T(Î½#(k))< S2,
â€“interval [Î½#(k), Î½#(k)](i.e., point) if
T(Î½#(k)) =S2.
It follows that T(Î½) =S2has at most 2nsolutions.
By Lemma 4.2, in order to compute the solution of
T(Î½) =S2, we have only to compute Î½#(k)by Newton
method or the like, and to compute the solution for
each interval by Newton method or the like. We show
an example of T(Î½) in Figure 2, and the proof in
Appendix A.7.
Final 
prediction 
(Convex 
function)Feature extraction
(Assumed to be fixed after
initial learning)ð‘¥1
ð‘¥2
ð‘¥ð‘‘ð‘¦Figure 3: Concept of how to apply SS for deep learning.
SS is applied to the last layer for the final prediction.
5Application to Deep Learning
So far, our discussion of SS rules has primarily focused
on ML models with linear predictions and convex loss
and regularization functions. However, there may be
scenarios where we would like to employ more complex
ML models, such as deep learning (DL).
For DL models, deriving SS rules for the entire
model can be challenging due to the complexity of
bounding the change in model parameters against
changes in sample weights. However, we can simplify
the process by focusing on the fact that each layer of
DL is often represented as a convex function. There-
fore, we propose applying SS rules specifically to the
last layer of DL models.
In this formulation, the layers preceding the last one
are considered as a fixed feature extraction process,
even when the sample weights change (see Figure
3). We believe that this approach is valid when the
change in sample weights is not significant. We plan
to experimentally evaluate the effectiveness of this
formulation in Section 6.3.
6 Numerical Experiment
6.1 Experimental Settings
We evaluate the performances of DRSsS and DRSfS
across different values of acceptable weight changes
Sand hyperparameters for regularization strength Î».
Performance is measured using safe screening rates ,
representing the ratio of screened samples or features
to all samples or features. We consider three setups:
9DRSsS with L1-loss L2-regularized SVM (Section 4.1),
DRSfS with L2-loss L1-regularized SVM (Section 4.2),
and DRSsS with deep learning (Section 5) where
the last layer incorporates DRSsS with L1-loss L2-
regularized SVM.
In these experiments, we set initialize the sample
weights before change ( Ëœw) as Ëœw=1n. Then, we set
Sin DRSS for W:={w| âˆ¥wâˆ’Ëœwâˆ¥2â‰¤S}(Section
4) as follows:
â€¢First we assume the weight change that the
weights for positive samples ( {i|yi= +1}) from
1 toa, while retaining the weights for negative
samples ( {i|yi=âˆ’1}) as 1.
â€¢Then, we defined Sas the size of weight change
above; specifically, we set S=âˆš
n+|aâˆ’1|
(n+: number of positive samples in the train-
ing dataset).
We vary awithin the range 0 .9â‰¤aâ‰¤1.1, assuming
a maximum change of up to 10% per sample weight.
6.2 Relationship between the Weight
Changes and Safe Screening Rate
First, we present safe screening rates for two SVM
setups. The datasets used in these experiments are
detailed in Table 2. In this experiment, we adapt
the regularization hyperparameter Î»based on the
characteristics of the data. These details are described
in Appendix D.1.
As an example, for the â€œsonarâ€ dataset, we show
the DRSsS result in Figure 4 and the DRSfS result in
Figure 5. Results for other datasets are presented in
Appendix D.2.
These plots allow us to assess the tolerance for
changes in sample weights. For instance, with a= 0.98
(weight of each positive sample is reduced by two per-
cent, or equivalent weight change in L2-norm), the
sample screening rate is 0.31 for L1-loss L2-regularized
SVM with Î»=6.58e + 1 , and the feature screen-
ing rate is 0.29 for L2-loss L1-regularized SVM with
Î»=3.47e + 1 . This implies that, even if the weights
are changed in such ranges, a number of samples or
features are still identified as redundant in the sense
of prediction.Table 2: Datasets for DRSsS/DRSfS experiments.
All are binary classification datasets from LIBSVM
dataset [ 22]. The mark â€ denotes datasets with one
feature removed due to computational constraints.
See Appendix D.1 for details.
Task Name n n+d
DRSsS australian 690 307 15
breast-cancer 683 239 11
heart 270 120 14
ionosphere 351 225 35
sonar 208 97 61
splice (train) 1000 517 61
svmguide1 (train) 3089 2000 5
DRSsS madelon (train) 2000 1000 â€ 500
sonar 208 97 â€ 60
splice (train) 1000 517 61
6.3 Safe Sample Screening for Deep
Learning Model
We applied DRSsS to DL models (Section 5), assuming
that all layers are fixed except for the last layer.
We utilized a neural network architecture compris-
ing the following components: firstly, ResNet50 [23]
with an output of 2,048 features, followed by a fully
connected layer to reduce the features to 10, and
finally, L1-loss L2-regularized SVM (Section 4.1) ac-
companied by the intercept feature (Remark 2.2).
For the experiment, we employed the CIFAR-10
dataset [ 24], a well-known benchmark dataset for
image classification tasks. We configured the net-
work to classify images into two classes: â€œairplaneâ€
and â€œautomobileâ€. Given that there are 5,000 im-
ages for each class, we split the dataset into train-
ing:validation:testing=6:2:2, resulting in a total of
6,000 images in the training dataset.
The resulting safe sample screening rates are illus-
trated in Figure 6. We observed similar outcomes to
those obtained with ordinary SVMs in Section 6.2.
This experiment validates the feasibility of apply-
ing DRSsS to DL models, demonstrating consistent
results with traditional SVM setups.
107 Conclusion
In this paper, we discussed DR-SS, considering the
possible changes in sample weights to represent DR
setup. We developed a method for calculating SS
that can handle changes in sample weights by intro-
ducing nontrivial computational techniques, such as
constrained maximization of certain convex functions
(Section 4.3). Additionally, to address the constraint
of SS, which typically applies to ML by minimizing
convex functions, we provided an application to DL
by applying SS to the last layer of DL model. While
this approach is an approximation, it holds certain
validity.
For the future work, we aim to explore different
environmental changes. In this paper, we focused on
weight constraint by L2-norm âˆ¥wâˆ’Ëœwâˆ¥2â‰¤S(Section
4) due to computational considerations. However,
when interpreting changes in weights, the constraint
of L1-norm âˆ¥wâˆ’Ëœwâˆ¥1â‰¤Smay be more appropri-
ate, as it reflects changes in weights by altering the
number of samples. Furthermore, in the context of
DR-SS for DL, we are interested in loosening the
constraint of fixing the network except for the last
layer. Investigating this aspect could provide valuable
insights into the flexibility of DR-SS methodologies
in DL applications.
Software and Data
The code and the data to reproduce the experiments
are available as the attached file.
Potential Broader Impact
This paper contributes to machine learning in dynam-
ically changing environments, a scenario increasingly
prevalent in real-world data analyses. We believe
that, in such situations, ensuring prediction perfor-
mance against environmental changes and minimizing
storage requirements for expanding datasets will be
beneficial. The method does not present significant
ethical concerns or foreseeable societal consequences
because this work is theoretical and, as of now, hasno direct applications that might impact society or
ethical considerations.
Acknowledgements
This work was partially supported by MEXT KAK-
ENHI (20H00601), JST CREST (JPMJCR21D3 in-
cluding AIP challenge program, JPMJCR22N2), JST
Moonshot R&D (JPMJMS2033-05), JST AIP Acceler-
ation Research (JPMJCR21U2), NEDO (JPNP18002,
JPNP20006) and RIKEN Center for Advanced Intel-
ligence Project.
References
[1]Ruidi Chen and Ioannis Ch. Paschalidis. Dis-
tributionally robust learning. arXiv Preprint,
2021.
[2]Laurent El Ghaoui, Vivian Viallon, and Tarek
Rabbani. Safe feature elimination for the lasso
and sparse supervised learning problems. Pacific
Journal of Optimization , 8(4):667â€“698, 2012.
[3]Kohei Ogawa, Yoshiki Suzuki, and Ichiro
Takeuchi. Safe screening of non-support vectors
in pathwise svm computation. In Proceedings of
the 30th International Conference on Machine
Learning , pages 1382â€“1390, 2013.
[4]Hidetoshi Shimodaira. Improving predictive in-
ference under covariate shift by weighting the
log-likelihood function. Journal of statistical plan-
ning and inference , 90(2):227â€“244, 2000.
[5]Masashi Sugiyama, Matthias Krauledat, and
Klaus-Robert MÂ¨ uller. Covariate shift adaptation
by importance weighted cross validation. Journal
of Machine Learning Research , 8(35):985â€“1005,
2007.
[6]C. Cortes and V. Vapnik. Support-vector net-
works. Machine Learning , 20:273â€“297, 1995.
[7]Robert Tibshirani. Regression shrinkage and
selection via the lasso. Journal of the Royal Sta-
11tistical Society Series B: Statistical Methodology ,
58(1):267â€“288, 1996.
[8]Joel Goh and Melvyn Sim. Distributionally
robust optimization and its tractable approxi-
mations. Operations Research , 58(4-1):902â€“917,
2010.
[9]Erick Delage and Yinyu Ye. Distributionally
robust optimization under moment uncertainty
with application to data-driven problems. Oper-
ations Research , 58(3):595â€“612, 2010.
[10]Liyuan Wang, Xingxing Zhang, Kuo Yang,
Longhui Yu, Chongxuan Li, Lanqing HONG,
Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun
Zhu. Memory replay with data compression for
continual learning. In International Conference
on Learning Representations , 2022.
[11]James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago
Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran,
and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the Na-
tional Academy of Sciences , 114(13):3521â€“3526,
2017.
[12]Olivier Fercoq, Alexandre Gramfort, and Joseph
Salmon. Mind the duality gap: safer rules for the
lasso. In Proceedings of the 32nd International
Conference on Machine Learning , pages 333â€“342,
2015.
[13]Eugene Ndiaye, Olivier Fercoq, Alexandre Gram-
fort, and Joseph Salmon. Gap safe screening rules
for sparse multi-task and multi-class models. In
Advances in Neural Information Processing Sys-
tems, pages 811â€“819, 2015.
[14]Shota Okumura, Yoshiki Suzuki, and Ichiro
Takeuchi. Quick sensitivity analysis for incre-
mental data modification and its application to
leave-one-out cv in linear classification problems.
InProceedings of the 21th ACM SIGKDD In-
ternational Conference on Knowledge Discovery
and Data Mining , pages 885â€“894, 2015.[15]Atsushi Shibagaki, Masayuki Karasuyama, Ko-
hei Hatano, and Ichiro Takeuchi. Simultaneous
safe screening of features and samples in doubly
sparse modeling. In International Conference on
Machine Learning , pages 1577â€“1586, 2016.
[16]Kazuya Nakagawa, Shinya Suzumura, Masayuki
Karasuyama, Koji Tsuda, and Ichiro Takeuchi.
Safe pattern pruning: An efficient approach for
predictive pattern mining. In Proceedings of the
22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining , pages
1785â€“1794. ACM, 2016.
[17]Shaogang Ren, Shuai Huang, Jieping Ye, and
Xiaoning Qian. Safe feature screening for gen-
eralized lasso. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 40(12):2992â€“
3006, 2018.
[18]Jiang Zhao, Yitian Xu, and Hamido Fujita. An
improved non-parallel universum support vec-
tor machine and its safe sample screening rule.
Knowledge-Based Systems , 170:79â€“88, 2019.
[19]Zhou Zhai, Bin Gu, Xiang Li, and Heng Huang.
Safe sample screening for robust support vector
machine. In Proceedings of the AAAI Conference
on Artificial Intelligence , volume 34, pages 6981â€“
6988, 2020.
[20]Hongmei Wang and Yitian Xu. A safe double
screening strategy for elastic net support vec-
tor machine. Information Sciences , 582:382â€“397,
2022.
[21]Takumi Yoshida, Hiroyuki Hanada, Kazuya Nak-
agawa, Kouichi Taji, Koji Tsuda, and Ichiro
Takeuchi. Efficient model selection for predictive
pattern mining model by safe pattern pruning.
Patterns , 4(12):100890, 2023.
[22]Chih-Chung Chang and Chih-Jen Lin. Libsvm: A
library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology
(TIST) , 2(3):27, 2011. Datasets are provided in
authorsâ€™ website: https://www.csie.ntu.edu.
tw/~cjlin/libsvmtools/datasets/ .
12[23]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
(CVPR) , June 2016.
[24] Alex Krizhevsky. The cifar-10 dataset, 2009.
[25]Ralph Tyrell Rockafellar. Convex analysis .
Princeton university press, 1970.
[26]Jean-Baptiste Hiriart-Urruty and Claude
LemarÂ´ echal. Convex Analysis and Minimization
Algorithms II: Advanced Theory and Bundle
Methods . Springer, 1993.
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.208
=0.657
=2.08
=6.577
=20.8
=65.77
=208
Figure 4: Ratio of screened samples by DRSsS for
dataset â€œsonarâ€.
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features
=7.48e1
=1.61e+0
=3.47e+0
=7.48e+0
=1.61e+1
=3.47e+1
=7.48e+1
Figure 5: Ratio of screened features by DRSfS for
dataset â€œsonarâ€.
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=6.00e+0
=1.90e+1
=6.00e+1
=1.90e+2
=6.00e+2
=1.90e+3
=6.00e+3
Figure 6: Ratio of screened samples by DRSsS
for dataset with CIFAR-10 dataset and DL model
ResNet50.
13A Proofs
A.1 General Lemmas
Lemma A.1. For a convex function f:Rdâ†’Râˆª {+âˆž},fâˆ—âˆ—is equivalent to fiffis convex, proper (i.e.,
âˆƒvâˆˆRd:f(v)<+âˆž) and lower-semicontinuous.
Proof. See Section 12 of [25] for example.
Lemma A.1 is known as Fenchel-Moreau theorem . Especially, Lemma A.1 holds if fis convex and
âˆ€vâˆˆRd:f(v)<+âˆž.
Lemma A.2. For a convex function f:Rdâ†’Râˆª {+âˆž},
â€¢fâˆ—is(1/Î½)-strongly convex if fis proper and Î½-smooth.
â€¢fâˆ—is(1/Îº)-smooth if fis proper, lower-semicontinuous and Îº-strongly convex.
Proof. See Section X.4.2 of [26] for example.
Lemma A.3. Suppose that f:Rdâ†’Râˆª{+âˆž}is aÎº-strongly convex function, and let vâˆ—=argminvâˆˆRdf(v)
be the minimizer of f. Then, for any vâˆˆRd, we have
âˆ¥vâˆ’vâˆ—âˆ¥2â‰¤r
2
Îº[f(v)âˆ’f(vâˆ—)].
Proof. See [13] for example.
Lemma A.4. For any vector a,câˆˆRnandS >0,
min
vâˆˆRn:âˆ¥vâˆ’câˆ¥2â‰¤SaâŠ¤v=aâŠ¤câˆ’Sâˆ¥aâˆ¥2, max
vâˆˆRn:âˆ¥vâˆ’câˆ¥2â‰¤SaâŠ¤v=aâŠ¤c+Sâˆ¥aâˆ¥2.
Proof. By Cauchy-Schwarz inequality,
âˆ’ âˆ¥aâˆ¥2âˆ¥vâˆ’câˆ¥2â‰¤aâŠ¤(vâˆ’c)â‰¤ âˆ¥aâˆ¥2âˆ¥vâˆ’câˆ¥2.
Noticing that the first inequality becomes equality if âˆƒÏ‰ >0 :a=âˆ’Ï‰(vâˆ’c), while the second inequality
becomes equality if âˆƒÏ‰â€²>0 :a=Ï‰â€²(vâˆ’c). Moreover, since âˆ¥vâˆ’câˆ¥2â‰¤S,
âˆ’Sâˆ¥aâˆ¥2â‰¤aâŠ¤(vâˆ’c)â‰¤Sâˆ¥aâˆ¥2
also holds, with the equality holds if âˆ¥vâˆ’câˆ¥2=S.
On the other hand, if we take vthat satisfies both of the equality conditions of Cauchy-Schwarz inequality
above, that is,
â€¢(for the first inequality being equality) v=câˆ’(S/âˆ¥aâˆ¥2)a,
â€¢(for the second inequality being equality) v=c+ (S/âˆ¥aâˆ¥2)a,
then the inequalities become equalities. This proves that âˆ’Sâˆ¥aâˆ¥2andSâˆ¥aâˆ¥2are surely the minimum and
maximum of aâŠ¤(vâˆ’c), respectively.
14A.2 Derivation of Dual Problem by Fenchelâ€™s Duality Theorem
As the formulation of Fenchelâ€™s duality theorem, we follow the one in Section 31 of [25].
Lemma A.5 (A special case of Fenchelâ€™s duality theorem: f, g < +âˆž).Letf:Rnâ†’Randg:Rdâ†’Rbe
convex functions, and AâˆˆRnÃ—dbe a matrix. Moreover, we define
vâˆ—:= min
vâˆˆRd[f(Av) +g(v)], (18)
uâˆ—:= max
uâˆˆRn[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)]. (19)
Then Fenchelâ€™s duality theorem assures that
f(Avâˆ—) +g(vâˆ—) =âˆ’fâˆ—(âˆ’uâˆ—)âˆ’gâˆ—(AâŠ¤uâˆ—),
âˆ’uâˆ—âˆˆâˆ‚f(Avâˆ—),
vâˆ—âˆˆâˆ‚gâˆ—(AâŠ¤uâˆ—).
Sketch of the proof. Introducing a dummy variable ÏˆâˆˆRnand a Lagrange multiplier uâˆˆRn, we have
min
vâˆˆRd[f(Av) +g(v)] = max
uâˆˆRnmin
vâˆˆRd,ÏˆâˆˆRn[f(Ïˆ) +g(v)âˆ’uâŠ¤(Avâˆ’Ïˆ)] (20)
=âˆ’min
uâˆˆRnmax
vâˆˆRd,ÏˆâˆˆRn[âˆ’f(Ïˆ)âˆ’g(v) +uâŠ¤(Avâˆ’Ïˆ)] =âˆ’min
uâˆˆRnmax
vâˆˆRd,ÏˆâˆˆRn[{(âˆ’u)âŠ¤Ïˆâˆ’f(Ïˆ)}+{(AâŠ¤u)âŠ¤vâˆ’g(v)}]
=âˆ’min
uâˆˆRn[fâˆ—(âˆ’u) +gâˆ—(AâŠ¤u)] = max
uâˆˆRn[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)]. (21)
Moreover, by the optimality condition of a problem with a Lagrange multiplier (20), the optima of it, denoted
byvâˆ—,Ïˆâˆ—anduâˆ—, must satisfy
Avâˆ—=Ïˆâˆ—, AâŠ¤uâˆ—âˆˆâˆ‚g(vâˆ—),âˆ’uâˆ—âˆˆâˆ‚f(Ïˆâˆ—) =âˆ‚f(Avâˆ—).
On the other hand, introducing a dummy variable Ï•âˆˆRdand a Lagrange multiplier vâˆˆRdfor(21), we have
max
uâˆˆRn[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)] = min
vâˆˆRdmax
uâˆˆRn,Ï•âˆˆRd[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(Ï•)âˆ’vâŠ¤(AâŠ¤uâˆ’Ï•)] (22)
= min
vâˆˆRdmax
uâˆˆRn,Ï•âˆˆRd[{(Av)âŠ¤(âˆ’u)âˆ’fâˆ—(âˆ’u)}+{vâŠ¤Ï•âˆ’gâˆ—(Ï•)}]
= min
vâˆˆRd[fâˆ—âˆ—(Av) +gâˆ—âˆ—(v)] = min
vâˆˆRd[f(Av) +g(v)].(âˆµLemma A.1)
Likely above, by the optimality condition of a problem with a Lagrange multiplier (22), the optima of it,
denoted by uâˆ—,Ï•âˆ—andvâˆ—, must satisfy
AâŠ¤uâˆ—=Ï•âˆ—,vâˆ—âˆˆâˆ‚gâˆ—(Ï•âˆ—) =âˆ‚gâˆ—(AâŠ¤uâˆ—), Avâˆ—âˆˆâˆ‚f(âˆ’uâˆ—).
Lemma A.6 (Dual problem of weighted regularized empirical risk minimization (weighted RERM)) .For
the minimization problem
Î²âˆ—(w):= argmin
Î²âˆˆRdPw(Î²),where Pw(Î²) :=nX
i=1wiâ„“yi(Ë‡Xi:Î²) +Ï(Î²), ((1) restated)
15we define the dual problem as the one obtained by applying Fenchelâ€™s duality theorem (Lemma A.5), which is
defined as
Î±âˆ—(w):= argmax
Î±âˆˆRnDw(Î±),where Dw(Î±) :=âˆ’nX
i=1wiâ„“âˆ—
yi(âˆ’Î³iÎ±i) +Ïâˆ—(((Î³âŠ—w)Ã—â–¡Ë‡X)âŠ¤Î±).((2) restated)
Moreover, Î²âˆ—(w)andÎ±âˆ—(w)must satisfy
Pw(Î²âˆ—(w)) =Dw(Î±âˆ—(w)), ((3) restated)
Î²âˆ—(w)âˆˆâˆ‚Ïâˆ—(((Î³âŠ—w)Ã—â–¡Ë‡X)âŠ¤Î±âˆ—(w)), ((4) restated)
âˆ€iâˆˆ[n] :âˆ’Î³iÎ±âˆ—(w)
iâˆˆâˆ‚â„“yi(Ë‡Xi:Î²âˆ—(w)). ((5) restated)
Proof. To apply Fenchelâ€™s duality theorem, we have only to set f,gandAin Lemma A.5 as
f(u) :=nX
i=1wiâ„“yi(ui), g(Î²) :=Ï(Î²), A :=Ë‡X.
Here, noticing that
fâˆ—(u) = sup
uâ€²âˆˆRn[uâŠ¤uâ€²âˆ’nX
i=1wiâ„“yi(uâ€²
i)] = sup
uâ€²âˆˆRnnX
i=1[uiuâ€²
iâˆ’wiâ„“yi(uâ€²
i)]
= sup
uâ€²âˆˆRnnX
i=1wiui
wiuâ€²
iâˆ’â„“yi(uâ€²
i)
=nX
i=1wiâ„“âˆ—
yiui
wi
,
from (19) we have
âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u) =âˆ’nX
i=1wiâ„“âˆ—
yi
âˆ’ui
wi
âˆ’Ïâˆ—(Ë‡XâŠ¤u).
Replacing uiâ†Î³iwiÎ±i, that is, uâ†(Î³âŠ—wâŠ—Î±), we have the dual problem (2).
The relationships between the primal and the dual problem are described as follows:
âˆ’uâˆ—âˆˆâˆ‚f(Avâˆ—)â‡’ âˆ’ Î³âŠ—wâŠ—Î±âˆ—(w)âˆˆâˆ‚f(Ë‡XÎ²âˆ—(w))â‡’ âˆ’ Î³iwiÎ±âˆ—(w)
iâˆˆwiâˆ‚â„“yi(Ë‡Xi:Î²âˆ—(w))
â‡’ âˆ’Î³iÎ±âˆ—(w)
iâˆˆâˆ‚â„“yi(Ë‡Xi:Î²âˆ—(w)),
vâˆ—âˆˆâˆ‚gâˆ—(AâŠ¤uâˆ—)â‡’Î²âˆ—(w)âˆˆâˆ‚gâˆ—(Ë‡XâŠ¤Î³âŠ—wâŠ—Î±âˆ—(w)) =âˆ‚gâˆ—(((Î³âŠ—w)Ã—â–¡Ë‡X)âŠ¤Î±âˆ—(w)).
A.3 Proof of Lemma 3.1
Proof. [13]
âˆ¥Ë†Î²âˆ’Î²âˆ—(w)âˆ¥2â‰¤r
2
Î»[Pw(Ë†Î²)âˆ’Pw(Î²âˆ—(w))] ( âˆµsetting fâ†Pwin Lemma A.3)
=r
2
Î»[Pw(Ë†Î²)âˆ’Dw(Î±âˆ—(w))] ( âˆµ(3))
â‰¤r
2
Î»[Pw(Ë†Î²)âˆ’Dw(Ë†Î±)]. (âˆµÎ±âˆ—(w)is a maximizer of Dw)
16A.4 Proof of Lemma 3.2
Proof. Due to (5), ifâˆ‚â„“yi(Ë‡Xi:Î²âˆ—(w)) ={0}is assured, then Î±âˆ—(w)
i= 0 is assured. Since we do not know Î²âˆ—(w)
but know Bâˆ—(w)(Lemma 3.1), we can assure Î±âˆ—(w)
i= 0 ifS
Î²âˆˆBâˆ—(w)âˆ‚â„“yi(Ë‡Xi:Î²) ={0}is assured. Noticing
thatâˆ‚â„“yiis monotonically increasing2, we have
[
Î²âˆˆBâˆ—(w)âˆ‚â„“yi(Ë‡Xi:Î²) ={0} â‡”[
Î²âˆˆBâˆ—(w)Ë‡Xi:Î²âŠ† Z[â„“yi]â‡” [ min
Î²âˆˆBâˆ—(w)Ë‡Xi:Î²,max
Î²âˆˆBâˆ—(w)Ë‡Xi:Î²]âŠ† Z[â„“yi]
â‡”h
Ë‡Xi:Ë†Î²âˆ’ âˆ¥Ë‡Xi:âˆ¥2r(w,Î³, Îº,Ë†Î²,Ë†Î±),Ë‡Xi:Ë†Î²+âˆ¥Ë‡Xi:âˆ¥2r(w,Î³, Îº,Ë†Î²,Ë†Î±)i
âŠ† Z[â„“yi]. (âˆµLemma A.4)
A.5 Proof of Lemma 3.3
Proof. The proof is almost the same as that for Lemma 3.1 (see Appendix A.3), but we additionally need to
show that âˆ’Dwis ((min iâˆˆ[n]wiÎ³2
i)/Âµ)-strongly convex (in this case Dwis called strongly concave ).
As discussed in Lemma A.2, âˆ’â„“âˆ—
yi(t) is (1 /Âµ)-strongly convex, that is, âˆ’â„“âˆ—
yi(t)âˆ’(1/2Âµ)t2is convex. Thus,
â€¢âˆ’â„“âˆ—
yi(âˆ’Î³iÎ±i)âˆ’(1/2Âµ)(Î³iÎ±i)2is convex with respect to Î±i,
â€¢âˆ’wiâ„“âˆ—
yi(âˆ’Î³iÎ±i)âˆ’(wiÎ³2
i/2Âµ)Î±2
iis convex with respect to Î±i,
â€¢âˆ’Pn
i=1wiâ„“âˆ—
yi(âˆ’Î³iÎ±i)âˆ’Pn
i=1(wiÎ³2
i/2Âµ)Î±2
iis convex with respect to Î±.
So,âˆ’Pn
i=1wiâ„“âˆ—
yi(âˆ’Î³iÎ±i) is convex with respect to Î±even subtracted byPn
k=1[miniâˆˆ[n](wiÎ³2
i/2Âµ)]Î±2
k=
(1/2)[min iâˆˆ[n](wiÎ³2
i/Âµ)]âˆ¥Î±âˆ¥2
2.
A.6 Proof of Lemma 4.1
Lemma A.7. For the optimization problem
max
wâˆˆWwâŠ¤Aw+ 2bâŠ¤w, ((16) restated)
subject to W:={wâˆˆRn| âˆ¥wâˆ’Ëœwâˆ¥2â‰¤S},
where ËœwâˆˆRn,bâˆˆRn,
AâˆˆRnÃ—n:symmetric, positive semidefinite, nonzero,
its stationary points are obtained as the solution of the following equations with respect to wandÎ½âˆˆR:
Aw+bâˆ’Î½(wâˆ’Ëœw) =0, (23)
âˆ¥wâˆ’Ëœwâˆ¥2=S. (24)
Also, when both (23) and(24) are satisfied, the function to be maximized is calculated as
wâŠ¤Aw+ 2bâŠ¤w=Î½S2+ (Î½Ëœw+b)âŠ¤(wâˆ’Ëœw) +ËœwâŠ¤b. (25)
2Since âˆ‚â„“yiis a multi-valued function, the monotonicity must be defined accordingly: we call a multi-valued function
F:Râ†’2Ris monotonically increasing if, for any t < tâ€²,Fmust satisfy â€œ âˆ€sâˆˆF(t),âˆ€sâ€²âˆˆF(tâ€²):sâ‰¤sâ€²â€.
17Proof. First, wâŠ¤Aw+ 2bâŠ¤wis convex and not constant. Then we can show that (16) is optimized in
{wâˆˆRn| âˆ¥wâˆ’Ëœwâˆ¥2=S}, that is, at the surface of the hyperball W(Theorem 32.1 of [ 25]). This proves
(24). Moreover, with the fact, we write the Lagrangian function with Lagrange multiplier Î½âˆˆRas:
L(w, Î½) :=wâŠ¤Aw+ 2bâŠ¤wâˆ’Î½(âˆ¥wâˆ’Ëœwâˆ¥2
2âˆ’S2).
Then, due to the property of Lagrange multiplier, the stationary points of (16) are obtained as
âˆ‚L
âˆ‚w= 2Aw+ 2bâˆ’2Î½(wâˆ’Ëœw) = 0 ,
âˆ‚L
âˆ‚Î½=âˆ¥wâˆ’Ëœwâˆ¥2
2âˆ’S2= 0,
where the former derives (23).
Finally we show (25). If both (23) and (24) are satisfied,
wâŠ¤Aw+ 2bâŠ¤w=wâŠ¤(Î½(wâˆ’Ëœw)âˆ’b) + 2bâŠ¤w (âˆµ(23))
=Î½wâŠ¤(wâˆ’Ëœw) +bâŠ¤w
=Î½(wâˆ’Ëœw)âŠ¤(wâˆ’Ëœw) +Î½ËœwâŠ¤(wâˆ’Ëœw) +bâŠ¤(wâˆ’Ëœw) +bâŠ¤Ëœw
=Î½S2+Î½ËœwâŠ¤(wâˆ’Ëœw) +bâŠ¤(wâˆ’Ëœw) +bâŠ¤Ëœw (âˆµ(24))
=Î½S2+ (Î½Ëœw+b)âŠ¤(wâˆ’Ëœw) +bâŠ¤Ëœw ((25) restated)
Proof of Lemma 4.1. The condition (23) is calculated as
Aw+b=Î½(wâˆ’Ëœw),
(Aâˆ’Î½I)(wâˆ’Ëœw) =âˆ’AËœwâˆ’b.
Here, let us apply eigendecomposition of A, denoted by A=QâŠ¤Î¦Q, where QâˆˆRnÃ—nis orthogonal
(QQâŠ¤=QâŠ¤Q=I) and Î¦ := diag(Ï•1, Ï•2, . . . , Ï• n) is a diagonal matrix consisting of eigenvalues of A. Such a
decomposition is assured to exist since Ais assumed to be symmetric and positive semidefinite. Then,
(QâŠ¤Î¦Qâˆ’Î½I)(wâˆ’Ëœw) =âˆ’QâŠ¤Î¦QËœwâˆ’b,
QâŠ¤(Î¦âˆ’Î½I)Q(wâˆ’Ëœw) =âˆ’QâŠ¤Î¦QËœwâˆ’b,
(Î¦âˆ’Î½I)Ï„=Î¾,(where Ï„:=Q(wâˆ’Ëœw),Î¾:=âˆ’Î¦QËœwâˆ’QbâˆˆRn,) (26)
âˆ€iâˆˆ[n] : ( Ï•iâˆ’Î½)Ï„i=Î¾i. (27)
Note that we have to be also aware of the constraint
S=âˆ¥Ï„âˆ¥2=âˆš
Ï„âŠ¤Ï„=q
(wâˆ’Ëœw)âŠ¤QâŠ¤Q(wâˆ’Ëœw) =âˆ¥wâˆ’Ëœwâˆ¥2. (28)
Here, we consider these two cases.
181.First, consider the case when (Î¦ âˆ’Î½I) is nonsingular, that is, when Î½is different from any of Ï•1, Ï•2, . . . , Ï• n.
Then, from (28) we have
S2=âˆ¥Ï„âˆ¥2=nX
i=1Ï„2
i=nX
i=1Î¾i
Î½âˆ’Ï•i2 
=:T(Î½)
. (29)
So, values of (16) for all stationary points with respect to wandÎ½(on condition that (Î¦ âˆ’Î½I) is
nonsingular) can be obtained by computing (25) for each Î½satisfying (29), that is,
â€¢for such Î½computing Ï„by (27), and
â€¢computing (25) as Î½S2+ (Î½Ëœw+b)âŠ¤(wâˆ’Ëœw) +bâŠ¤Ëœw=Î½S2+ (Î½Ëœw+b)âŠ¤QâŠ¤Ï„+bâŠ¤Ëœw.
2.Secondly, consider the case when (Î¦ âˆ’Î½I) is nonsingular, that is, when Î½is equal to one of Ï•1, Ï•2, . . . , Ï• n.
First, given Î½, letUÎ½:={i|iâˆˆ[n], Ï•i=Î½}be the indices of {Ï•i}iequal to Î½(this may include more
than one indices), and FÎ½:= [n]\ UÎ½. Note that, by assumption, UÎ½is not empty. Then, all stationary
points of (16)with respect to wandÎ½(on condition that (Î¦ âˆ’Î½I) is singular) can be found by computing
the followings for each Î½âˆˆ {Ï•1, Ï•2, . . . , Ï• n}(duplication excluded):
â€¢IfÎ¾iÌ¸= 0 for at least one iâˆˆ UÎ½, the equation (27) cannot hold.
â€¢IfÎ¾i= 0 for all iâˆˆ N Î½, the equation (27) may hold. So we calculate Ï„that maximizes (16) as
follows:
â€“FixÏ„i=Î¾i/(Ï•iâˆ’Î½) for iâˆˆ FÎ½.
â€“Set the constraintP
iâˆˆUÎ½Ï„2
i=S2âˆ’P
iâˆˆFÎ½Ï„2
i(due to (28)).
â€“Maximize (16)with respect to {Ï„i}iâˆˆUÎ½under the constraints above. Here, by (25)we have
only to calculate
max
Ï„âˆˆRn[Î½S2+ (Î½Ëœw+b)âŠ¤(wâˆ’Ëœw) +bâŠ¤Ëœw], (30)
subject to âˆ€iâˆˆ FÎ½:Ï„i=Î¾i
Ï•iâˆ’Î½,
X
iâˆˆUÎ½Ï„2
i=S2âˆ’X
iâˆˆFÎ½Ï„2
i,
which is easily computed by Lemma A.4. The value of the maximization result is equal to that
of (16) on condition that Î½is specified above.
So, collecting these result and taking the largest one, the maximization (on condition that (Î¦ âˆ’Î½I)
is singular) is completed.
Taking the maximum of the two cases, we have the maximization result of (16).
A.7 Proof of Lemma 4.2
Proof. We show the statements in the lemma that, if Ï•ek< Ï•ek+1(kâˆˆ[Nâˆ’1]), then T(Î½) is a convex function
in the interval ( Ï•ek, Ï•ek+1) with limÎ½â†’Ï•ek+0=limÎ½â†’Ï•ek+1âˆ’0= +âˆž. Then the conclusion immediately follows.
19The latter statement clearly holds. The former statement is proved by directly computing the derivative.
d
dÎ½T(Î½) =d
dÎ½nX
i=1Î¾i
Î½âˆ’Ï•i2
=âˆ’2nX
i=1Î¾2
i
(Î½âˆ’Ï•i)3.
It is an increasing function with respect to Î½, as long as Î½does not match any of {Ï•i}n
i=1such that Î¾iÌ¸= 0.
So it is convex in the interval Ï•ek< Î½ < Ï• ek+1.
B Detailed Calculations
In this appendix we describe detailed calculations omitted in the main paper.
B.1 Calculations for L1-loss L2-regularized SVM (Section 4.1)
For this setup, we can calculate as
Ïâˆ—(Î²) :=1
2Î»âˆ¥Î²âˆ¥2
2, â„“âˆ—
y(t) :=(
t, (âˆ’1â‰¤tâ‰¤0)
+âˆž,(otherwise)âˆ‚Ïâˆ—(Î²) :=1
Î»Î²
, âˆ‚â„“ y(t) :=ï£±
ï£´ï£²
ï£´ï£³{âˆ’1},(t <1)
[âˆ’1,0],(t= 1)
{0}. (t >1)
Then we have the dual problem in the main paper (6).
B.2 Calculations for L2-loss L1-regularized SVM (Section 4.2)
For this setup, we can calculate as
Ïâˆ—(Î²) :=(
0, (Î²d= 0,âˆ€jâˆˆ[dâˆ’1] :|Î²j| â‰¤Î»)
+âˆž,(otherwise)â„“âˆ—
y(t) :=(
t2+4t
4,(tâ‰¤0)
+âˆž,(otherwise)
âˆ€jâˆˆ[dâˆ’1] : [ âˆ‚Ïâˆ—(Î²)]j:=ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³âˆ’âˆž, (Î²j<âˆ’Î»)
[âˆ’âˆž,0],(Î²j=âˆ’Î»)
0, (|Î²j|< Î»)
[0,+âˆž],(Î²j=Î»)
+âˆž, (Î²j> Î»)[âˆ‚Ïâˆ—(Î²)]d:=ï£±
ï£´ï£²
ï£´ï£³âˆ’âˆž, (Î²d<0)
[âˆ’âˆž,+âˆž],(Î²d= 0)
+âˆž, (Î²d>0)
âˆ‚â„“y(t) :=âˆ’2 max{0,1âˆ’t}.
Then, setting Î³i=Î»for all iâˆˆ[n], the dual objective function is described as
Dw(Î±) =(
âˆ’Pn
i=1wiÎ»2Î±2
iâˆ’4Î»Î±i
4,(if (32) are satisfied)
+âˆž, (otherwise)(31)
where
Î»Î±iâ‰¥0â‡”Î±iâ‰¥0, (32a)
âˆ€jâˆˆ[dâˆ’1] :|((Î»1nâŠ—w)âŠ—Ë‡X:j)âŠ¤Î±| â‰¤Î»â‡” |(wâŠ—Ë‡X:j)âŠ¤Î±| â‰¤1, (32b)
((Î»1nâŠ—w)âŠ—Ë‡X:d)âŠ¤Î±= 0â‡”(wâŠ—Ë‡X:d)âŠ¤Î±= 0. (32c)
20Optimality conditions (4) and (5) are described as
âˆ€jâˆˆ[dâˆ’1] :|(Î»1nâŠ—wâŠ—Ë‡X:j)âŠ¤Î±âˆ—(w)|< Î»â‡” |(wâŠ—Ë‡X:j)âŠ¤Î±âˆ—(w)|<1â‡’Î²âˆ—(w)
j= 0, (33)
âˆ€iâˆˆ[n] :Î»Î±âˆ—(w)
i= 2 max {0,1âˆ’Ë‡Xi:Î²âˆ—(w)}. (34)
C Application of Safe Sample Screening to Kernelized Features
The kernel method in ML means computation methods when the input variable vector of a sample xâˆˆRd
cannot be specifically obtained (this includes the case when dis infinite), but for the input variable vectors
for any two samples x,xâ€²âˆˆRdits inner product xâŠ¤xâ€²can be obtained. In such a case, we cannot discuss
SfS since we cannot obtain each feature specifically, however, we can discuss SsS.
We show that the SsS rules for L1-loss L2-regularized SVM (Section 4.1) can be applied even if the features
are kernelized.
First, if features are kernelized, we cannot obtain either XorÎ²âˆ—(Ëœw)specifically. However, since we can
obtain Î±âˆ—(Ëœw), with (7) we have
âˆ€xâˆˆRd:xâŠ¤Î²âˆ—(Ëœw)=1
Î»xâŠ¤(wÃ—â–¡Ë‡X)âŠ¤Î±âˆ—(Ëœw)=1
Î»nX
i=1wiÎ±âˆ—(Ëœw)
i(xâŠ¤Ë‡Xi:). (35)
This means that we can calculate the inner product of Î²âˆ—(Ëœw)and any vector.
Then, in order to calculate the quantity (9) to conduct SsS, we have only to calculate
â€¢Ë‡Xi:Î²âˆ—(Ëœw)can be calculated by (35),
â€¢âˆ¥Ë‡Xi:âˆ¥2=q
Ë‡XâŠ¤
i:Ë‡Xi:is obtained as the kernel value, and
â€¢Pw(Î²âˆ—(Ëœw))âˆ’Dw(Î±âˆ—(Ëœw)) can be calculated by (35)and kernel values since two variables whose values
cannot be specifically obtained ( ËœXandÎ²âˆ—(Ëœw)) appears only as inner products.
So, all values needed to derive SsS rules (9) can be computed even if features are kernelized.
D Details of Experiments
D.1 Detailed Experimental Setup
The criteria of selecting datasets (Table 2) and detailed setups are as follows:
â€¢All of the datasets are downloaded from LIBSVM dataset [ 22]. We used scaled datasets for ones used
in DRSfS or only scaled datasets are provided (â€œionosphereâ€, â€œsonarâ€ and â€œspliceâ€). We used training
datasets only if test datasets are provided separately (â€œspliceâ€, â€œsvmguide1â€ and â€œmadelonâ€).
â€¢For DRSsS, we selected datasets from LIBSVM dataset containing 100 to 10,000 samples, 100 or fewer
features, and the area under the curve (AUC) of the receiver operating characteristic (ROC) is 0.9 or
higher for the regularization strengths ( Î») we examined so that they tend to facilitate more effective
sample screening.
21â€¢For DRSfS, we selected datasets from LIBSVM dataset containing 50 to 1,000 features, 10,000 or fewer
samples, and containing no categorical features. Also, due to computational constraints, we excluded
features that have at least one zero (marked â€œ â€ â€ in Table 2). As a result, one feature from â€œmadelonâ€
and one from â€œsonarâ€ have been excluded.
â€¢In the table, the column â€œ dâ€ denotes the number of features including the intercept feature (Remark
2.2).
The choice of regularization hyperparameter Î», based on the characteristics of the data, is as follows:
â€¢For DRSsS, we set Î»asn,nÃ—10âˆ’0.5,nÃ—10âˆ’1.0,. . .,nÃ—10âˆ’3.0. (For DRSsS with DL, we set 1000
instead of n.) This is because the effect of Î»gets weaker for larger n.
â€¢For DRSfS, we determine Î»based on Î»max, defined as the smallest Î»for which Î²âˆ—(w)
j = 0 for any
jâˆˆ[dâˆ’1] explained below. We then set Î»asÎ»max,Î»maxÃ—10âˆ’1/3,Î»maxÃ—10âˆ’2/3,. . .,Î»maxÃ—10âˆ’2.
Finally, we show the calculation of Î»maxfor L2-loss L1-regularized SVM. By (14), we would like to find Î»
so that |(wâŠ—Ë‡X:j)âŠ¤Î±âˆ—(w)|<1 for all jâˆˆ[dâˆ’1]. In order to judge this, we need Î±âˆ—(w), which is calculated
as follows:
â€¢Solve the primal problem (1)for L2-loss L1-regularized SVM by fixing Î²âˆ—(w)
j = 0 for any jâˆˆ[dâˆ’1],
that is,
Î²âˆ—(w)
d= argmin
Î²dnX
i=1wiâ„“yi(Ë‡xidÎ²d) = argmin
Î²dnX
i=1wi(max{0,1âˆ’yiÎ²d})2
= argmin
Î²dX
iâˆˆ[n], yi=+1wi(max{0,1âˆ’Î²d})2+X
iâˆˆ[n], yi=âˆ’1wi(max{0,1 +Î²d})2
=P
iâˆˆ[n], yi=+1wiâˆ’P
iâˆˆ[n], yi=âˆ’1wiPn
i=1wi.
â€¢With Î²âˆ—(w)
d computed above and Î²âˆ—(w)
j= 0 for any jâˆˆ[dâˆ’1], calculate Î±$=Î»Î±âˆ—(w)= [2max{0,1âˆ’
Ë‡Xi:Î²âˆ—(w)}]n
i=1by (15).
â€¢If|(wâŠ—Ë‡X:j)âŠ¤Î±$)|< Î» for all jâˆˆ[dâˆ’1], then Î²âˆ—(w)
j = 0 for any jâˆˆ[dâˆ’1]. So, we set Î»max=
max jâˆˆ[dâˆ’1]|(wâŠ—Ë‡X:j)âŠ¤Î±$)|.
D.2 All Experimental Results of Section 6.2
For the experiment of Section 6.2, ratios of screened samples by DRSsS setup is presented in Figure 7, while
ratios of screened features by DRSfS setup in Figure 8.
22Dataset: australian
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.690
=2.181
=6.9
=21.81
=69.0
=218
=690
 Dataset: breast-cancer
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.683
=2.159
=6.83
=21.59
=68.3
=215
=683
Dataset: heart
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.27
=0.853
=2.7
=8.538
=27.0
=85.38
=270
 Dataset: ionosphere
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.351
=1.109
=3.510
=11.09
=35.1
=110
=351
Dataset: sonar
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=0.208
=0.657
=2.08
=6.577
=20.8
=65.77
=208
 Dataset: splice
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=1.0
=3.162
=10.0
=31.62
=100
=316
=1000
Dataset: svmguide1
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened samples
=3.089
=9.768
=30.89
=97.68
=308
=976
=3089
Figure 7: Ratios of screened samples by DRSsS.
23Dataset: madelon
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features
=7.32e+2
=1.58e+3
=3.40e+3
=7.32e+3
=1.58e+4
=3.40e+4
=7.32e+4
 Dataset: sonar
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features
=7.48e1
=1.61e+0
=3.47e+0
=7.48e+0
=1.61e+1
=3.47e+1
=7.48e+1
Dataset: splice
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)0.00.20.40.60.81.0Ratio of screened features
=7.34e+0
=1.58e+1
=3.41e+1
=7.34e+1
=1.58e+2
=3.41e+2
=7.34e+2
Figure 8: Ratios of screened features by DRSfS.
24