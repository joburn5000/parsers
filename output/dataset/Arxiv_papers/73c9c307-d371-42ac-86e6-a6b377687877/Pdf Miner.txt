4
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

1
v
7
7
2
6
1
.
4
0
4
2
:
v
i
X
r
a

Causally Inspired Regularization Enables Domain General
Representations

Olawale Salaudeen∗1 and Sanmi Koyejo2

1University of Illinois at Urbana Champaign
2Stanford University

Abstract

Given a causal graph representing the data-generating process shared across diﬀerent domains/distributions,

enforcing suﬃcient graph-implied conditional independencies can identify domain-general (non-spurious)
feature representations. For the standard input-output predictive setting, we categorize the set of graphs
considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer
across training domains gives domain-general representations and (ii) those where it does not. For the
latter case (ii), we propose a novel framework with regularizations, which we demonstrate are suﬃ-
cient for identifying domain-general feature representations without a priori knowledge (or proxies) of
the spurious features. Empirically, our proposed method is eﬀective for both (semi) synthetic and real-
world data, outperforming other state-of-the-art methods in average and worst-domain transfer accuracy.

1 Introduction

A key feature of machine learning is its capacity to generalize across new domains. When these domains
present diﬀerent data distributions, the algorithm must leverage shared structural concepts to achieve out-
of-distribution (OOD) or out-of-domain generalization. This capability is vital in numerous important real-
world machine learning applications. For example, in safety-critical settings such as autonomous driving,
a lack of resilience to unfamiliar distributions could lead to human casualties. Likewise, in the healthcare
sector, where ethical considerations are critical, an inability to adjust to shifts in data distribution can result
in unfair biases, manifesting as inconsistent performance across diﬀerent demographic groups.

An inﬂuential approach to domain generalization is Invariant Causal Prediction (ICP; [Peters et al.,
2016]). ICP posits that although some aspects of data distributions (like spurious or non-causal mechanisms
[Pearl, 2010]) may change across domains, certain causal mechanisms remain constant.
ICP suggests fo-
cusing on these invariant mechanisms for prediction. However, the estimation method for these invariant
mechanisms suggested by [Peters et al., 2016] struggles with scalability in high-dimensional feature spaces.
To overcome this, Arjovsky et al. [2019] introduced Invariant Risk Minimization (IRM), designed to identify
these invariant mechanisms by minimizing an objective. However, requires strong assumptions for identify-
ing the desired domain-general solutions [Ahuja et al., 2021, Rosenfeld et al., 2022]; for instance, observing
a number of domains proportional to the spurious features’ dimensions is necessary, posing a signiﬁcant
challenge in these high-dimensional settings.

Subsequent variants of IRM have been developed with improved capabilities for identifying domain-
general solutions [Ahuja et al., 2020, Krueger et al., 2021, Robey et al., 2021, Wang et al., 2022, Ahuja et al.,
2021]. Additionally, regularizers for Distributionally Robust Optimization with subgroup shift have been
proposed (GroupDRO) [Sagawa et al., 2019]. However, despite their solid theoretical motivation, empirical
evidence suggests that these methods may not consistently deliver domain-general solutions in practice
Gulrajani and Lopez-Paz [2020], Kaur et al. [2022], Rosenfeld et al. [2022].

∗Contact: oes2@illinois.edu

1

 
 
 
 
 
 
Kaur et al. [2022] demonstrated that regularizing directly for conditional independencies implied by the
generative process can give domain-general solutions, including conditional independencies beyond those
considered by IRM. However, their experimental approach involves regularization terms that require direct
observation of spurious features, a condition not always feasible in real-world applications. Our proposed
methodology also leverages regularizers inspired by the conditional independencies indicated by causal graphs
but, crucially, it does so without necessitating prior knowledge (or proxies) of the spurious features.

1.1 Contributions

In this work,

• we outline suﬃcient properties to uniquely identify domain-general predictors for a general set of

generative processes that include domain-correlated spurious features,

• we propose regularizers to implement these constraints without independent observations of the spuri-

ous features, and

• ﬁnally, we show that the proposed framework outperforms the state-of-the-art on semi-synthetic and

real-world data.

The code for our proposed method is provided at https://github.com/olawalesalaudeen/tcri.

Notation: Capital letters denote bounded random variables, and corresponding lowercase letters denote
their value. Unless otherwise stated, we represent latent domain-general features as Zdg ∈ Zdg ≡ Rm and
spurious latent features as Zspu ∈ Zspu ≡ Ro. Let X ∈ X ≡ Rd be the observed feature space and the output
space of an invertible function Γ : Zdg × Zspu 7→ X and Y ∈ Y ≡ {0, 1, . . . , K − 1} be the observed label
space for a K-class classiﬁcation task. We then deﬁne feature extractors aimed at identifying latent features
Φdg : X 7→ Rm, Φspu : X 7→ Ro so that Φ : X 7→ Rm+o
. We deﬁne
e as a discrete random variable denoting domains and E = {P e(Zdg, Zspu, X, Y ) : e = 1, 2, . . .} to be the set
of possible domains. Etr ⊂ E is the set of observed domains available during training.

that is Φ(x) = [Φdg(x); Φspu(x)]∀x ∈ X

(cid:1)

(cid:0)

2 Related Work

The source of distribution shift can be isolated to components of the joint distribution. One special case
of distribution shift is covariate shift [Shimodaira, 2000, Zadrozny, 2004, Huang et al., 2006, Gretton et al.,
2009, Sugiyama et al., 2007, Bickel et al., 2009, Chen et al., 2016, Schneider et al., 2020], where only the co-
variate distribution P (X) changes across domains. Ben-David et al. [2009] give upper-bounds on target error
based on the H-divergence between the source and target covariate distributions, which motivates domain
alignment methods like the Domain Adversarial Neural Networks [Ganin et al., 2016] and others [Long et al.,
2015, Blanchard et al., 2017]. Others have followed up on this work with other notions of covariate distance
for domain adaptation, such as mean maximum discrepancy (MMD) [Long et al., 2016], Wasserstein dis-
tance [Courty et al., 2017], etc. However, Kpotufe and Martinet [2018] show that these divergence metrics
fail to capture many important properties of transferability, such as asymmetry and non-overlapping support.
Furthermore, Zhao et al. [2019] shows that even with the alignment of covariates, large distances between
label distributions can inhibit transfer; they propose a label conditional importance weighting adjustment to
address this limitation. Other works have also proposed conditional covariate alignment [des Combes et al.,
2020, Li et al., 2018c,b].

Another form of distribution shift is label shift, where only the label distribution changes across domains.
Lipton et al. [2018] propose a method to address this scenario. Schrouﬀ et al. [2022] illustrate that many
real-world problems exhibit more complex ’compound’ shifts than just covariate or label shifts alone.

One can leverage domain adaptation to address distribution shifts; however, these methods are contingent
on having access to unlabeled or partially labeled samples from the target domain during training. When such
samples are available, more sophisticated domain adaptation strategies aim to leverage and adapt spurious
feature information to enhance performance [Liu et al., 2021, Zhang et al., 2021, Kirichenko et al., 2022].

2

However, domain generalization, as a problem, does not assume access to such samples [Muandet et al.,
2013].

To address the domain generalization problem, Invariant Causal Predictors (ICP) leverage shared causal
structure to learn domain-general predictors [Peters et al., 2016]. Previous works, enumerated in the intro-
duction (Section 1), have proposed various algorithms to identify domain-general predictors. Arjovsky et al.
[2019]’s proposed invariance risk minimization (IRM) and its variants motivated by domain invariance:

min
w,Φ

1
|Etr|

e∈Etr
X

Re(w ◦ Φ) s.t. w ∈ argmin

ew

Re(

w · Φ), ∀e ∈ Etr,

e

(cid:2)

ℓ(y, w · Φ(x))

where Re(w ◦ Φ) = E
, with loss function ℓ, feature extractor Φ, and linear predictor w. This
objective aims to learn a representation Φ such that predictor w that minimizes empirical risks on average
across all domains also minimizes within-domain empirical risk for all domains. However, Rosenfeld et al.
[2020], Ahuja et al. [2020] showed that this objective requires unreasonable constraints on the number of
observed domains at train times, e.g., observing distinct domains on the order of the rank of spurious
features. Follow-up works have attempted to improve these limitations with stronger constraints on the
problem – enumerated in the introduction section.

(cid:3)

Our method falls under domain generalization; however, unlike the domain-general solutions previously
discussed, our proposed solution leverages diﬀerent conditions than domain invariance directly, which we
show may be more suited to learning domain-general representations.

3 Causality and Domain Generalization

We often represent causal relationships with a causal graph. A causal graph is a directed acyclic graph
(DAG), G = (V, E), with nodes V representing random variables and directed edges E representing causal
relationships, i.e., parents are causes and children are eﬀects. A structural equation model (SEM) provides
a mathematical representation of the causal relationships in its corresponding DAG. Each variable Y ∈ V
is given by Y = fY (X) + εY , where X denotes the parents of Y in G, fY is a deterministic function, and
εY is an error capturing exogenous inﬂuences on Y . The main property we need here is that fY is invariant
to interventions to V \{Y } and is consequently invariant to changes in P (V ) induced by these interventions.
Interventions refer to changes to fZ, Z ∈ V \{Y }.

In this work, we focus on domain-general predictors dg that are linear functions of features with domain-
general mechanisms, denoted as gdg := w ◦ Φdg, where w is a linear predictor and Φdg identiﬁes features with
domain-general mechanisms. We use domain-general rather than domain-invariant since domain-invariance
is strongly tied to the property: Y ⊥⊥ e | Zdg [Arjovsky et al., 2019]. As shown in the subsequent sections,
this work leverages other properties of appropriate causal graphs to obtain domain-general features. This
distinction is crucial given the challenges associated with learning domain-general features through domain-
invariance methods [Rosenfeld et al., 2020].

Given the presence of a distribution shift, it’s essential to identify some common structure across domains
that can be utilized for out-of-distribution (OOD) generalization. For example, Shimodaira [2000] assume
P (Y |X) is shared across all domains for the covariate shift problem. In this work, we consider a setting
where each domain is composed of observed features and labels, X ∈ X , Y ∈ Y, where X is given by an
invertible function Γ of two latent random variables: domain-general Zdg ∈ Zdg and spurious Zspu ∈ Zspu.
By construction, the conditional expectation of the label Y given the domain-general features Zdg is the
same across domains, i.e.,

Eei [Y |Zdg = zdg] = Eej [Y |Zdg = zdg]
∀zdg ∈ Zdg, ∀ei 6= ej ∈ E.

(1)

Conversely, this robustness to e does not necessarily extend to spurious features Zspu; in other words, Zspu
may assume values that could lead a predictor relying on it to experience arbitrarily high error rates. Then,
a sound strategy for learning a domain-general predictor – one that is robust to distribution shifts – is to
identify the latent domain-general Zdg from the observed features X.

3

e

X

Zdg

Y

Zspu

Figure 1: Partial Ancestral Graph representing all non-trivial and valid generative processes (DAGs); dashed
edges indicate that an edge may or may not exist.

The approach we take to do this is motivated by the Reichenbach Common Cause Principle, which claims
that if two events are correlated, there is either a causal connection between the correlated events that is
responsible for the correlation or there is a third event, a so-called (Reichenbachian) common cause, which
brings about the correlation [Hitchcock and Rédei, 2021, Rédei, 2002]. This principle allows us to posit the
class of generative processes or causal mechanisms that give rise to the correlated observed features and
labels, where the observed features are a function of domain-general and spurious features. We represent
Importantly, the mapping from a node’s causal parents to
these generative processes as causal graphs.
itself is preserved in all distributions generated by the causal graph (Equation 1), and distributions can vary
arbitrarily so long as they preserve the conditional independencies implied by the DAG (Markov Property
[Pearl, 2010]).

We now enumerate DAGs that give observe features with spurious correlations with the label.

Valid DAGs. We consider generative processes, where both latent features, Zspu, Zdg, and observed X
are correlated with Y , and the observed X is a function of only Zdg and Zspu (Figure 1).

Given this setup, there is an enumerable set of valid generative processes. Such processes are (i) without
cycles, (ii) are feature complete – including edges from Zdg and Zspu to X, i.e., Zdg → X ← Zspu, and
(iii) where the observed features mediate domain inﬂuence, i.e., there is no direct domain inﬂuence on the
label e 6→ Y . We discuss this enumeration in detail in Appendix B. The result of our analysis is identifying
a representative set of DAGs that describe valid generative processes – these DAGs come from orienting
the partial ancestral graph (PAG) in Figure 1. We compare the conditional independencies implied by the
DAGs deﬁned by Figure 1 as illustrated in Figure 2, resulting in three canonical DAGs in the literature (see
Appendix B for further discussion). Other DAGs that induce spurious correlations are outside the scope of
this work.

e

X

Zdg

Y

Zspu

e

X

Zdg

Y

Zspu

e

X

Zdg

Y

Zspu

(a) Causal [Arjovsky et al., 2019].

(b) Anticausal
2020].

[Rosenfeld et al.,

Fully

(c)
[Ahuja et al., 2021].

Informative

Causal

Figure 2: Generative Processes. Graphical models depicting the structure of possible data-generating
processes – shaded nodes indicate observed variables. X represents the observed features, Y represents
observed targets, and e represents domain inﬂuences (domain indexes in practice). There is an explicit
separation of domain-general Zdg and domain-speciﬁc Zspu features; they are combined to generate observed
X. Dashed edges indicate the possibility of an edge.

Conditional independencies implied by identiﬁed DAGs (Figure 2).

4

Table 1: Generative Processes and Suﬃcient Conditions for Domain-Generality

Graphs in Figure 2
(a)
✓
Identifying Zdg is necessary ✓

Zdg ⊥⊥ Zspu | {Y, e}

(b)
✓
✓

(c)
✗
✗

Fig. 2a: Zdg ⊥⊥ Zspu | {Y, e}; Y ⊥⊥ e | Zdg.

This causal graphical model implies that the mapping from Zdg to its causal child Y is preserved and
consequently, Equation 1 holds [Pearl, 2010, Peters et al., 2016]. As an example, consider the task of
predicting the spread of a disease. Features may include causes (vaccination rate and public health
policies) and eﬀects (coughing). e is the time of month; the distribution of coughing changes depending
on the season.

Fig. 2b: Zdg ⊥⊥ Zspu | {Y, e}; Zdg ⊥⊥ Zspu | Y ; Y ⊥⊥ e | Zdg, Zdg ⊥⊥ e.

The causal graphical model does not directly imply that Zdg → Y is preserved across domains. However,
in this work, it represents the setting where the inverse of the causal direction is preserved (inverse:
Zdg → Y ), and thus Equation 1 holds. A context where this setting is relevant is in healthcare where
medical conditions (Y ) cause symptoms (Zdg), but the prediction task is often predicting conditions
from symptoms, and this mapping Zdg → Y , opposite of the causal direction, is preserved across
distributions. Again, we may consider e as the time of month; the distribution of coughing changes
depending on the season.

Fig. 2c: Y ⊥⊥ e | Zdg; Zdg ⊥⊥ e.

Similar to Figure 2a, this causal graphical model implies that the mapping from Zdg to its causal child Y
is preserved, so Equation 1 holds [Pearl, 2010, Peters et al., 2016]. This setting is especially interesting
because it represents a Fully Informative Invariant Features setting, that is Zspu ⊥⊥ Y | Zdg
[Ahuja et al., 2021]. Said diﬀerently, Zspu does not induce a backdoor path from e to Y that Zdg does
not block. As an example of this, we can consider the task of predicting hospital readmission rates.
Features may include the severity of illness, which is a direct cause of readmission rates, and also
include the length of stay, which is also caused by the severity of illness. However, length of stay may
not be a cause of readmission; the correlation between the two would be a result of the confounding
eﬀect of a common cause, illness severity. e is an indicator for distinct hospitals.

We call the condition Y ⊥⊥ e | Zdg the domain invariance property. This condition is common to all
the DAGs in Figure 2. We call the condition Zdg ⊥⊥ Zspu | {Y, e} the target conditioned representation
independence (TCRI) property. This condition is common to the DAGs in Figure 2a, 2b. In the settings
considered in this work, the TCRI property is equivalently Zdg ⊥⊥ Zspu | Y∀e ∈ E since e will simply index
the set of empirical distributions available at training.

Domain generalization with conditional independencies. Kaur et al. [2022] showed that suﬃciently
regularizing for the correct conditional independencies described by the appropriate DAGs can give domain-
general solutions, i.e., identiﬁes Zdg. However, in practice, one does not (partially) observe the latent features
independently to regularize directly. Other works have also highlighted the need to consider generative pro-
cesses when designing robust algorithms to distribute shifts [Veitch et al., 2021, Makar et al., 2022]. However,
previous work has largely focused on regularizing for the domain invariance property, ignoring the conditional
independence property Zdg ⊥⊥ Zspu | Y, e.

Suﬃciency of ERM under Fully Informative Invariant Features. Despite the known challenges of
learning domain-general features from the domain-invariance properties in practice, this approach persists,

5

likely due to it being the only property shared across all DAGs. We alleviate this constraint by observing
that Graph (Fig. 2c) falls under what Ahuja et al. [2021] refer to as the fully informative invariant features
settings, meaning that Zspu is redundant, having only information about Y that is already in Zdg. Ahuja et al.
[2021] show that the empirical risk minimizer is domain-general for bounded features.

Easy vs. hard DAGs imply the generality of TCRI. Consequently, we categorize the generative
processes into easy and hard cases Table 1: (i) easy meaning that minimizing average risk gives domain-
general solutions, i.e., ERM is suﬃcient (Fig. 2c), and (ii) hard meaning that one needs to identify Zdg to
obtain domain-general solutions (Figs. 2a-2b). We show empirically that regularizing for Zdg ⊥⊥ Zspu | Y ∀e ∈
E also gives a domain-general solution in the easy case. The generality of TCRI follows from its suﬃciency
for identifying domain-general Zdg in the hard cases while still giving domain-general solutions empirically
in the easy case.

4 Proposed Learning Framework

We have now clariﬁed that hard DAGs (i.e., those not solved by ERM) share the TCRI property. The
challenge is that Zdg and Zspu are not independently observed; otherwise, one could directly regularize.
Existing work such as Kaur et al. [2022] empirically study semi-synthetic datasets where Zspu is (partially)
observed and directly learn Zdg by regularizing that Φ(X) ⊥⊥ Zspu | Y, e for feature extractor Φ. To our
knowledge, we are the ﬁrst to leverage the TCRI property without requiring observation of Zspu. Next, we
set up our approach with some key assumptions. The ﬁrst is that the observed distributions are Markov to
an appropriate DAG.

Assumption 4.1. All distributions, sources and targets, are generated by one of the structural causal
models SCM that follow:

Z (e)
dg ∼ P (e)
,
Zdg
dg, Z (e)
Y (e) ← hw∗
dg i + ηY ,
spu, Y i + η(e)
Z (e)
spu ← hw∗
Zspu
X ← Γ(Zdg, Zspu),

,

causal

SCM(e) :=

z

}|

{






F IIF

SCM(e) :=

z

}|

{

anticausal

(2)

SCM(e) :=

z

}|

{

dg ∼ P (e)
Z (e)
,
Zdg
dg, Z (e)
Y (e) ← hw∗
dg i + ηY ,
Z (e)
spu, Zdgi + η(e)
spu ← hw∗
Zspu
X ← Γ(Zdg, Zspu),

,











Y (e) ∼ PY ,
Z (e)
dg ← h
Z (e)
spu ← hw∗
e
X ← Γ(Zdg, Zspu),

wdg, Y i + η(e)
,
Zdg
spu, Y i + η(e)
Zspu

,

(3)

(4)

where PZdg is the causal covariate distribution, w’s are linear generative mechanisms, η’s are exogenous
independent noise variables, and Γ : Zdg × Zspu → X is an invertible function. It follows from having causal
mechanisms that we can learn a predictor w∗
dg inverts
the mapping

dg for Zdg that is domain-general (Equation 2-4) – w∗

wdg in the anticausal case.

These structural causal models (Equation 2-4) correspond to causal graphs Figures 2a-2c, respectively.

e

Assumption 4.2 (Structural). Causal Graphs and their distributions are Markov and Faithful [Pearl, 2010].

Given Assumption 4.2, we aim to leverage TCRI property (Zdg ⊥⊥ Zspu | Y ∀e ∈ Etr) to learn the latent
Zdg without observing Zspu directly. We do this by learning two feature extractors that, together, recover
Zdg and Zspu and satisfy TCRI (Figure 3). We formally deﬁne these properties as follows.

Deﬁnition 4.3 (Total Information Criterion (TIC)). Φ = Φdg ⊕ Φspu satisﬁes TIC with respect to random
variables X, Y, e if for Φ(X e) = [Φdg(X e); Φspu(X e)], there exists a linear operator T s.t., T (Φ(X e)) =
[Z e

dg; Z e

spu]∀e ∈ Etr.

6

Φdg

X e

Φspu

Zdg

b
⊕

Zspu

θc

θe

yc

b
ye

b

Figure 3: Modeling approach. During training, both representations, Φdg, and Φspu, generate domain-
general and domain-speciﬁc predictions, respectively. However, only the domain-invariant representa-
tions/predictions are used during testing – indicated by the solid red arrows.

b

In other words, a feature extractor that satisﬁes the total information criterion recovers the complete
latent feature sets Zdg, Zspu. This allows us to deﬁne the proposed implementation of the TCRI property
non-trivially – the conditional independence of subsets of the latents may not have the same implications
on domain generalization. We note that X ⊥⊥ Y |Zdg, Zspu, so X has no information about Y that is not in
Zdg, Zspu.

Deﬁnition 4.4 (Target Conditioned Representation Independence). Φ = Φdg ⊕ Φspu satisﬁes TCRI with
respect to random variables X, Y, e if Φdg(X) ⊥⊥ Φspu(X) | Y ∀e ∈ E.

Proposition 4.5. Assume that Φdg(X) and Φspu(X) are correlated with Y . Given Assumptions 4.1-4.2 and
a representation Φ = Φdg ⊕ Φspu that satisﬁes TIC, Φdg(X) = Zdg ⇐⇒ Φ satisﬁes TCRI. (see Appendix C
for proof ).

Proposition 4.5 shows that TCRI is necessary and suﬃcient to identify Zdg from a set of training domains.
We note that we can verify if Φdg(X) and Φspu(X) are correlated with Y by checking if the learned predictors
are equivalent to chance. Next, we describe our proposed algorithm to implement the conditions to learn
such a feature map. Figure 3 illustrates the learning framework.

Learning Objective: The ﬁrst term in our proposed objective is

LΦdg = Re(θc ◦ Φdg),

(cid:2)

where Φdg : X 7→ Rm is a feature extractor, θc : Rm 7→ Y is a linear predictor, and Re(θc ◦ Φdg) =
E
is the empirical risk achieved by the feature extractor and predictor pair on samples from
domain e. Φdg and θc are designed to capture the domain-general portion of the framework.

ℓ(y, θc · Φ(x))

Next, to implement the total information criterion, we use another feature extractor Φspu : X 7→ Ro,
designed to capture the domain-speciﬁc information in X that is not captured by Φdg. Together, we have
Φ = Φdg ⊕ Φspu where Φ has domain-speciﬁc predictors θe : Rm+o 7→ Y for each training domain, allowing
the feature extractor to utilize domain-speciﬁc information to learn distinct optimal domain-speciﬁc (non-
general) predictors:

(cid:3)

LΦ = Re

θe ◦ Φ

.

LΦ aims to ensure that Φdg and Φspu capture all of the information about Y in X – total information
(cid:0)
criterion. Since we do not know o, m, we select them to be the same size on our experiments; o, m could be
treated as hyperparameters though we do not treat them as such.

(cid:1)

Finally, we implement the TCRI property (Deﬁnition 4.4). We denote LT CRI to be a conditional
independence penalty for Φdg and Φspu. We utilize the Hilbert Schmidt independence Criterion (HSIC)
[Gretton et al., 2007] as LT CRI . However, in principle, any conditional independence penalty can be used
in its place. HSIC:

LT CRI (Φdg, Φspu) =

1
2

\HSIC

Φdg(X), Φspu(X)

Xk∈{0,1}

(cid:16)

(cid:17)

7

y=k

=

1
2

1
n2
k

tr

KΦdg
(cid:16)

Xk∈{0,1}

Hnk

KΦspu

Hnk

y=k

,

(cid:17)

where k, indicates which class the examples in the estimate correspond to, C is the number of classes, KΦdg ∈
Rnk×nk , KΦspu ∈ Rnk×nk are Gram matrices, Ki,j
Φspu = ω(Φspu(X)i, Φspu(X)j)
with kernels κ, ω are radial basis functions, Hnk = Ink − 1
11⊤ is a centering matrix, Ink is the nk × nk
n2
k
dimensional identity matrix, 1nk is the nk-dimensional vector whose elements are all 1, and ⊤ denotes the
transpose. We condition on the label by taking only examples of each label and computing the empirical
HSIC; then, we take the average.

Φ = κ(Φdg(X)i, Φdg(X)j), Ki,j

Taken together, the full objective to be minimized is as follows:

L =

1
Etr

Re(θc ◦ Φdg) + Re(θe ◦ Φ) + βLT CRI (Φdg, Φspu)
#

e∈Etr "
X

,

where β > 0 is a hyperparameter and Etr is the number of training domains. Figure 3 shows the full
framework. We note that when β = 0, this loss reduces to ERM.

Note that while we minimize this objective with respect to Φ, θc, θ1, . . . , θEtr , only the domain-general

representation and its predictor, θc · Φdg are used for inference.

5 Experiments

We begin by evaluating with simulated data, i.e., with known ground truth mechanisms; we use Equation 5
to generate our simulated data, with domain parameter σei ; code is provided in the supplemental materials.

SCM(ei) := 




Z (ei)
dg ∼ N
y(ei) = Z (ei)
(cid:0)
Z (ei)

0, σ2
ei
dg + N
(cid:1)
spu = Y (ei) + N
(cid:0)
(cid:0)

0, σ2
y
0, σ2
(cid:1)
ei

Table 2: Continuous Simulated Results – Feature Extractor
with a dummy predictor θc = 1., i.e.,
y = x · Φdg · w, where
x ∈ RN ×2, Φdg, Φspu ∈ R2×1, w ∈ R. Oracle indicates the
coeﬃcients achieved by regressing y on zc directly.

b

,

.
(5)
(cid:1)

Algorithm

ERM
IRM
TCRI
Oracle

(Φdg)0
(i.e., Zdg weight)
0.29
0.28
1.01
1.04

(Φdg)1
(i.e., Zspu weight)
0.71
0.71
0.06
0.00

We observe 2 domains with parameters σe=0 = 0.1, σe=1 = 0.2 with σy = 0.25, 5000 samples, and linear
feature extractors and predictors. We use partial covariance as our conditional independence penalty LT CRI .
Table 2 shows the learned value of Φdg, where ‘Oracle’ indicates the true coeﬃcients obtained by regressing
Y on domain-general Zdg directly. The ideal Φdg recovers Zdg and puts zero weight on Zspu.

Now, we evaluate the eﬃcacy of our proposed objective on non-simulated datasets.

5.1 Semisynthetic and Real-World Datasets

Algorithms: We compare our method to baselines corresponding to DAG properties: Empirical Risk Mini-
mization (ERM, [Vapnik, 1991]), Invariant Risk Minimization (IRM [Arjovsky et al., 2019]), Variance Risk
Extrapolation (V-REx, [Krueger et al., 2021]), [Li et al., 2018a]), Group Distributionally Robust Optimiza-
tion (GroupDRO), [Sagawa et al., 2019]), and Information Bottleneck methods (IB_ERM/IB_IRM,
[Ahuja et al., 2021]). Additional baseline methods are provided in the Appendix A.

We evaluate our proposed method on the semisynthetic ColoredMNIST [Arjovsky et al., 2019] and real-
world Terra Incognita dataset [Beery et al., 2018]. Given observed domains Etr = {e : 1, 2, . . . , Etr}, we train
on Etr \ ei and evaluate the model on the unseen domain ei, for each e ∈ Etr.

ColoredMNIST: The ColoredMNIST dataset [Arjovsky et al., 2019] is composed of 7000 (2 × 28 × 28, 1)
images of a hand-written digit and binary-label pairs. There are three domains with diﬀerent correlations
between image color and label, i.e., the image color is spuriously related to the label by assigning a color to

8

each of the two classes (0: digits 0-4, 1: digits 5-9). The color is then ﬂipped with probabilities {0.1, 0.2, 0.9}
to create three domains, making the color-label relationship domain-speciﬁc because it changes across do-
mains. There is also label ﬂip noise of 0.25, so we expect that the best accuracy a domain-general model
can achieve is 75%, while a non-domain general model can achieve higher. In this dataset, Zdg corresponds
to the original image, Zspu the color, e the label-color correlation, Y the image label, and X the observed
colored image. This DAG follows the generative process of Figure 2a [Arjovsky et al., 2019].

Spurrious PACS: Variables. X: images, Y : non-urban (elephant, giraﬀe, horse) vs. urban (dog, guitar,
house, person). Domains. {{cartoon, art painting}, {art painting, cartoon}, {photo}} [Li et al., 2017]. The
photo domain is the same as in the original dataset. In the {cartoon, art painting} domain, urban examples
are selected from the original cartoon domain, while non-urban examples are selected from the original art
painting domain. In the {art painting, cartoon} domain, urban examples are selected from the original art
painting domain, while non-urban examples are selected from the original cartoon domain. This sampling
encourages the model to use spurious correlations (domain-related information) to predict the labels; however,
since these relationships are ﬂipped between domains {{cartoon, art painting} and {art painting, cartoon},
these predictions will be wrong when generalized to other domains.

Terra Incognita: The Terra Incognita dataset contains subsets of the Caltech Camera Traps dataset
[Beery et al., 2018] deﬁned by [Gulrajani and Lopez-Paz, 2020]. There are four domains representing diﬀer-
ent locations {L100, L38, L43, L46} of cameras in the American Southwest. There are 9 species of wild
animals {bird, bobcat, cat, coyote, dog, empty, opossum, rabbit, raccoon, squirrel} and a ‘no-animal’ class
to be predicted. Like Ahuja et al. [2021], we classify this dataset as following the generative process in
Figure 2c, the Fully Informative Invariant Features (FIIF) setting. Additional details on model architecture,
training, and hyperparameters are detailed in Appendix 5.

Model Selection. The standard approach for model selection is a training-domain hold-out validation
set accuracy. We ﬁnd that model selection across hyperparameters using this held-out training domain
validation accuracy often returns non-domain-general models in the ‘hard’ cases. One advantage of our
model is that we can do model selection based on the TCRI condition (conditional independence between the
two representations) on held-out training domain validation examples to mitigate this challenge. In the easy
case, we expect the empirical risk minimizer to be domain-general, so selecting the best-performing training-
domain model is sound – we additionally do this for all baselines (see Appendix A.1 for further discussion).
We ﬁnd that, empirically, this heuristic works in the examples we study in this work. Nevertheless, model
selection under distribution shift remains a signiﬁcant bottleneck for domain generalization.

5.2 Results and Discussion

Table 3: E\etest → etest (model selection on held-out source domains validation set). The ‘mean’ column
indicates the average generalization accuracy over all three domains as the etest distinctly; the ‘min’ column
indicates the worst generalization accuracy.

ColoredMNIST

Spurious PACS

Terra Incognita

average
Algorithm
51.6 ± 0.1
ERM
51.7 ± 0.1
IRM
52.0 ± 0.1
GroupDRO
VREx
51.7 ± 0.2
51.5 ± 0.2
IB_ERM
51.7 ± 0.0
IB_IRM
TCRI_HSIC 59.6 ± 1.8

worst-case
10.0 ± 0.1
9.9 ± 0.1
9.9 ± 0.1
10.2 ± 0.0
10.0 ± 0.1
9.9 ± 0.0
45.1 ± 6.7

average
57.2 ± 0.7
54.7 ± 0.8
58.5 ± 0.4
58.8 ± 0.4
56.3 ± 1.1
55.9 ± 1.2
63.4 ± 0.2

worst-case
31.2 ± 1.3
30.3 ± 0.3
37.7 ± 0.7
37.5 ± 1.1
35.5 ± 0.4
33.8 ± 2.2
62.3 ± 0.2

average
44.2 ± 1.8
38.9 ± 3.7
47.8 ± 0.9
45.1 ± 0.4
46.0 ± 1.4
37.0 ± 2.8
49.2 ± 0.3

worst-case
35.1 ± 2.8
32.6 ± 4.7
39.9 ± 0.7
38.1 ± 1.3
39.3 ± 1.1
29.6 ± 4.1
40.4 ± 1.6

9

Table 4: Total Information Criterion: Domain General (DG) and Domain Speciﬁc (DS) Accuracies. The DG
classiﬁer is shared across all training domains, and the DS classiﬁers are trained on each domain. The ﬁrst
row indicates the domain from which the held-out examples are sampled, and the second indicates which
domain-speciﬁc predictor is used. {+90%, +80%, -90%} indicate domains – {0.1, 0.2, 0.9} digit label and
color correlation, respectively.

Test Domain
No DS clf.
+90%
+80%
-90%

DG Classiﬁer

DS Classiﬁer on +90 DS Classiﬁer on +80 DS Classiﬁer on -90
+90% +80% -90% +90% +80% -90% +90% +80% -90% +90% +80% -90%

68.7
63.1
65.6

69.0
62.4
63.4

68.5
64.4
44.1

-
76.3
75.3

90.1
-
75.3

9.8
24.3
-

-
70.0
69.2

79.9
-
69.5

20.1
30.4
-

-
24.5
29.3

10.4
-
26.0

89.9
76.3
-

Table 5: TIC ablation for ColoredMNIST.

Algorithm
TCRI_HSIC (No TIC)
TCRI_HSIC

average
51.8 ± 5.9
59.6 ± 1.8

worst-case
27.7 ± 8.9
45.1 ± 6.7

Worst-domain Accuracy. A critical implication of domain generality is stability – robustness in worst-
domain performance up to domain diﬃculty. While average accuracy across domains provides some insight
into an algorithm’s ability to generalize to new domains, the average hides the variance of performance
across domains. Average improvement can be increased while the worst-domain accuracy stays the same or
decreases, leading to incorrect conclusions about domain generalization. Additionally, in real-world challenges
such as algorithmic fairness where worst-group performance is considered, some metrics or fairness are
analogous to achieving domain generalization [Creager et al., 2021].

Results. TCRI achieves the highest average and worst-case accuracy across all baselines (Table 3). We
ﬁnd no method recovers the exact domain-general model’s accuracy of 75%. However, TCRI achieves over
7% increase in both average accuracy and worst-case accuracy. Appendix A.2 shows transfer accuracies
with cross-validation on held-out test domain examples (oracle) and TCRI again outperforms all baselines,
achieving an average accuracy of 70.0% ± 0.4% and a worst-case accuracy of 65.7% ± 1.5, showing that
regularizing for TCRI gives very close to optimal domain-general solutions.

Similarly, for the Spurious-PACS dataset, we observe that TCRI outperforms the baselines. TRCI
achieves the highest average accuracy of 63.4% ± 0.2 and worst-case accuracy of 62.3% ± 0.1 with the
next best, VREx, achieving 58.8 ± 1.0 and 33.8 ± 0.0, respectively. Additionally, for the Terra-Incognita
dataset, TCRI achieves the highest average and worst-case accuracies of 49.2% ± 0.3% and 40.4% ± 1.6%
with the next best, GroupDRO, achieving 47.8 ± 0.9 and 39.9 ± 0.7, respectively.

Appendix A.2 shows transfer accuracies with cross-validation held-out target domain examples (oracle)
where we observe that TCRI also obtains the highest average and worst-case accuracy for Spurrious-PACS
and Terra Incognita.

Overall, regularizing for TCRI gives the most domain-general solutions compared to our baselines, achiev-
ing the highest worst-case accuracy on all benchmarks. Additionally, TCRI achieves the highest average
accuracy on ColoredMNIST and Spurious-PAC and the second highest on Terra Incognita, where we expect
the empirical risk minimizer to be domain-general.

Additional results are provided in the Appendix A.

The Eﬀect of the Total Information Criterion. Without the TIC loss term, our proposed method is
less eﬀective. Table 5 shows that for Colored MNIST, the hardest ‘hard’ case we encounter, removing the
TIC criteria, performs worse in average and worst case accuracy, dropping over 8% and 18, respectively.

. In the case of Colored MNIST, we
Separation of Domain General and Domain Speciﬁc Features
can reason about the extent of feature disentanglement from the accuracies achieved by the domain-general
and domain-speciﬁc predictors. Table 4 shows how much each component of Φ, Φdg and Φspu, behaves as

10

expected. For each domain, we observe that the domain-speciﬁc predictors’ accuracies follow the same trend
as the color-label correlation, indicating that they capture the color-label relationship. The domain-general
predictor, however, does not follow such a trend, indicating that it is not using color as the predictor.

For example, when evaluating the domain-speciﬁc predictors from the +90% test domain experiment
(row +90%) on held-out examples from the +80% training domain (column "DS Classiﬁer on +80%"), we
ﬁnd that the +80% domain-speciﬁc predictor achieves an accuracy of nearly 79.9% – exactly what one
would expect from a predictor that uses a color correlation with the same direction ‘+’. Conversely, the
-90% predictor achieves an accuracy of 20.1%, exactly what one would expect from a predictor that uses a
color correlation with the opposite direction ‘-’. The -90% domain has the opposite label-color pairing, so a
color-based classiﬁer will give the opposite label in any ‘+’ domain.

Another advantage of this method, exempliﬁed by Table 4, is that if one believes a particular domain is
close to one of the training domains, one can opt to use the close domain’s domain-speciﬁc predictor and
leverage spurious information to improve performance.

On Benchmarking Domain Generalization. Previous work on benchmarking domain generalization
showed that across standard benchmarks, the domain-unaware empirical risk minimizer outperforms or
achieves equivalent performance to the state-of-the-art domain generalization methods [Gulrajani and Lopez-Paz,
2020]. Additionally, Rosenfeld et al. [2022] gives results that show weak conditions that deﬁne regimes where
the empirical risk minimizer across domains is optimal in both average and worst-case accuracy. Conse-
quently, to accurately evaluate our work and baselines, we focus on settings where it is clear that (i) the
empirical risk minimizer fails, (ii) spurious features, as we have deﬁned them, do not generalize across the
observed domains, and (iii) there is room for improvement via better domain-general predictions. We discuss
this point further in the Appendix A.1.

Oracle Transfer Accuracies. While model selection is an integral part of the machine learning develop-
ment cycle, it remains a non-trivial challenge when there is a distribution shift. While we have proposed a
selection process tailored to our method that can be generalized to other methods with an assumed causal
graph, we acknowledge that model selection under distribution shift is still an important open problem. Con-
sequently, we disentangle this challenge from the learning problem and evaluate an algorithm’s capacity to
give domain-general solutions independently of model selection. We report experimental reports using held-
out test-set examples for model selection in Appendix A Table 6. We ﬁnd that our method, TCRI_HSIC,
also outperforms baselines in this setting.

6 Conclusion and Future Work

We reduce the gap in learning domain-general predictors by leveraging conditional independence properties
implied by generative processes to identify domain-general mechanisms. We do this without independent
observations of domain-general and spurious mechanisms and show that our framework outperforms other
state-of-the-art domain-generalization algorithms on real-world datasets in average and worst-case across
domains. Future work includes further improvements to the framework to fully recover the strict set of
domain-general mechanisms and model selection strategies that preserve desired domain-general properties.

Acknowledgements

OS was partially supported by the UIUC Beckman Institute Graduate Research Fellowship, NSF-NRT
1735252. This work is partially supported by the NSF III 2046795, IIS 1909577, CCF 1934986, NIH
1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc.

References

Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization

games. In International Conference on Machine Learning, pages 145–155. PMLR, 2020.

11

Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioan-
nis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution
generalization. Advances in Neural Information Processing Systems, 34:3438–3450, 2021.

Martín Arjovsky, L. Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. ArXiv,

abs/1907.02893, 2019.

Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita.

In Proceedings of the

European conference on computer vision (ECCV), pages 456–473, 2018.

Shai Ben-David, John Blitzer, K. Crammer, A. Kulesza, Fernando C Pereira, and Jennifer Wortman Vaughan.

A theory of learning from diﬀerent domains. Machine Learning, 79:151–175, 2009.

Steﬀen Bickel, Michael Brückner, and Tobias Scheﬀer. Discriminative learning under covariate shift. Journal

of Machine Learning Research, 10(9), 2009.

Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain general-

ization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017.

Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D Ziebart. Robust covariate shift regression.

In

Artiﬁcial Intelligence and Statistics, pages 1270–1279. PMLR, 2016.

Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. Advances in Neural Information Processing Systems, 30, 2017.

Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In

International Conference on Machine Learning, pages 2189–2200. PMLR, 2021.

Rémi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoﬀrey J. Gordon. Domain adaptation with

conditional distribution matching and generalized label shift. ArXiv, abs/2003.04475, 2020.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of
machine learning research, 17(1):2096–2030, 2016.

A. Gretton, K. Fukumizu, C. Teo, Le Song, B. Schölkopf, and Alex Smola. A kernel statistical test of

independence. In NIPS, 2007.

Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Schölkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009.

Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. CoRR, abs/2007.01434,

2020. URL https://arxiv.org/abs/2007.01434.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.

Christopher Hitchcock and Miklós Rédei. Reichenbach’s Common Cause Principle.

In Edward N. Zalta,
editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer
2021 edition, 2021.

Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Schölkopf, and Alex Smola. Correcting
sample selection bias by unlabeled data. Advances in Neural Information Processing Systems, 19, 2006.

Jivat Neet Kaur, Emre Kiciman, and Amit Sharma. Modeling the data-generating process is necessary for

out-of-distribution generalization. arXiv preprint arXiv:2206.07837, 2022.

Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is suﬃcient for

robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.

12

Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the beneﬁts of labels in covariate-shift.
In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference
On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1882–1886. PMLR,
06–09 Jul 2018. URL https://proceedings.mlr.press/v75/kpotufe18a.html.

David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,
Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In
International Conference on Machine Learning, pages 5815–5826. PMLR, 2021.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain gen-
eralization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550,
2017.

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for

domain generalization. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018a.

Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature
learning. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5400–5409,
2018b. doi: 10.1109/CVPR.2018.00566.

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and D. Tao. Deep domain

generalization via conditional invariant adversarial networks. In ECCV, 2018c.

Zachary Chase Lipton, Yu-Xiang Wang, and Alex Smola. Detecting and correcting for label shift with black

box predictors. ArXiv, abs/1802.03916, 2018.

Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang,
and Chelsea Finn. Just train twice: Improving group robustness without training group information. In
International Conference on Machine Learning, pages 6781–6792. PMLR, 2021.

Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferable features with deep

adaptation networks. ArXiv, abs/1502.02791, 2015.

Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with

residual transfer networks. Advances in neural information processing systems, 29, 2016.

Maggie Makar, Ben Packer, Dan Moldovan, Davis Blalock, Yoni Halpern, and Alexander D’Amour. Causally
motivated shortcut removal using auxiliary labels. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel
Valera, editors, Proceedings of The 25th International Conference on Artiﬁcial Intelligence and Statistics,
volume 151 of Proceedings of Machine Learning Research, pages 739–766. PMLR, 28–30 Mar 2022. URL
https://proceedings.mlr.press/v151/makar22a.html.

Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant feature

representation. In International conference on machine learning, pages 10–18. PMLR, 2013.

J. Pearl. Causal inference. In NIPS Causality: Objectives and Assessment, 2010.

Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction:
identiﬁcation and conﬁdence intervals. Journal of the Royal Statistical Society. Series B (Statistical
Methodology), pages 947–1012, 2016.

Miklós Rédei. Reichenbach’s Common Cause Principle and Quantum Correlations, pages 259–270. Springer
ISBN 978-94-010-0385-8. doi: 10.1007/978-94-010-0385-8_17. URL

Netherlands, Dordrecht, 2002.
https://doi.org/10.1007/978-94-010-0385-8_17.

Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. Advances in

Neural Information Processing Systems, 34:20210–20229, 2021.

Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv

preprint arXiv:2010.05761, 2020.

13

Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation
In International Conference on Artiﬁcial Intelligence and

and extrapolation in domain generalization.
Statistics, pages 2641–2657. PMLR, 2022.

Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint
arXiv:1911.08731, 2019.

Steﬀen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge.
Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural
Information Processing Systems, 33:11539–11551, 2020.

Jessica Schrouﬀ, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-
Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Maintaining fairness across distri-
bution shift: do we have viable solutions for real-world applications? arXiv preprint arXiv:2202.01034,
2022.

Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood

function. Journal of statistical planning and inference, 90(2):227–244, 2000.

Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki Kawanabe. Direct
importance estimation with model selection and its application to covariate shift adaptation. Advances in
Neural Information Processing Systems, 20, 2007.

Vladimir Vapnik. Principles of risk minimization for learning theory. In NIPS, volume 91, pages 831–840,

1991.

Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invariance to

spurious correlations: Why and how to pass stress tests. arXiv preprint arXiv:2106.00545, 2021.

Haoxiang Wang, Haozhe Si, Bo Li, and Han Zhao. Provable domain generalization via invariant-feature

subspace recovery. In ICML, 2022.

Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias.

In Proceedings of the

twenty-ﬁrst international conference on Machine learning, page 114, 2004.

Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adap-
tive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing
Systems, 34, 2021.

H. Zhao, Rémi Tachet des Combes, Kun Zhang, and Geoﬀrey J. Gordon. On learning invariant representa-

tions for domain adaptation. In ICML, 2019.

14

A Additional Results and Discussion

A.1 On Benchmarking Domain Generalization

Table 6: Oracle (model selection on held-out target domain validation set) E\etest → etest. The ‘mean’
column indicates the average generalization accuracy over all three domains as the etest distinctly; the ‘min’
column indicates the worst generalization accuracy.

ColoredMNIST

Spurious PACS

Terra Incognita

average
Algorithm
57.8 ± 0.2
ERM
68.9 ± 1.6
IRM
61.1 ± 1.3
GroupDRO
68.0 ± 2.5
VREx
65.0 ± 0.1
IB_ERM
IB_IRM
68.4 ± 1.0
TCRI_HSIC 70.4 ± 0.4

worst-case
38.4 ± 1.4
62.0 ± 4.9
37.6 ± 3.6
59.4 ± 7.3
50.6 ± 0.3
58.5 ± 2.8
65.7 ± 1.5

average
59.2 ± 1.3
67.5 ± 5.8
61.8 ± 1.8
62.8 ± 2.4
67.3 ± 3.7
69.0 ± 1.3
69.5 ± 1.1

worst-case
38.4 ± 1.4
53.9 ± 6.6
40.0 ± 1.6
38.7 ± 0.9
53.1 ± 8.0
62.3 ± 0.3
62.3 ± 0.2

average
52.9 ± 0.8
42.6 ± 4.0
50.7 ± 1.0
43.2 ± 2.0
49.0 ± 0.3
32.8 ± 6.6
51.2 ± 0.1

worst-case
42.0 ± 0.6
42.7 ± 1.2
42.7 ± 1.2
34.9 ± 4.2
39.9 ± 0.8
20.4 ± 7.5
43.0 ± 0.4

Oracle Transfer Accuracies. While model selection is an integral part of the machine learning develop-
ment cycle, it remains a non-trivial challenge when there is a distribution shift. While we have proposed a
selection process tailored to our method that can be generalized to other methods with an assumed causal
graph, we acknowledge that model selection under distribution shift is still an important open problem.
Consequently, we disentangle this challenge from the learning problem and evaluate an algorithm’s capacity
to give domain-general solutions independently of model selection. We report experimental reports using
held-out test-set examples for model selection in Appendix A Table 6.

In this case, we ﬁnd that there is indeed a separation between ERM and some domain-generalization
algorithms, suggesting that model selection might be a substantial bottleneck for learning domain-general
predictors. Nevertheless, we still ﬁnd that our method, TCRI_HSIC, also outperforms baselines in this
setting.

Challenges of Benchmarking Domaing Generalization. We show some results below that illustrate
the challenge of accurately evaluating the eﬃcacy of an algorithm for domain generalization. We ﬁrst note
that we expect ERM (naive) to perform poorly in domain generalization tasks, certainly so when we observe
worst-case shifts at test time. However, like other works [Gulrajani and Lopez-Paz, 2020], we observe that
ERM performs as well as other baselines during transfer on various benchmark datasets. Previous theoretical
results [Rosenfeld et al., 2022] suggest that this observation is indicative of properties of the benchmark
domains that may be suﬃcient for ERM to give domain-general solutions - speciﬁcally that the distribution
(and equivalently the loss) of the target domain can be written as a convex combination of the those in the
source domains.

To further investigate this, we develop additional experiments motivated by the ColoredMNIST [Arjovsky et al.,

2019] – since its generative process is well understood. We note that in the +90%, +80%, and -90% domains
of ColoredMNIST, the -90% domain has the opposite relationship between the spurious correlation and the
label, so the use of spurious correlations from {+90%, +80%} generalizes catastrophically to the -90% do-
main. In this setting, the baseline algorithms we present, including ERM, achieve poor accuracy in the -90%
domain while maintaining high accuracy in the +90% and +80% domains. Consequently, we investigate two
settings, setting a: observe {+90%, +80%, +70%, -90%} domains and setting b: observe {+90%, +80%,
-80%, -90%} domains – we focus on generalizing to the -90% domain. In setting a, we add another domain
In setting b, we
with the majority direction in the relationship between spurious correlation and labels.
add another domain with the minority direction. Note that in setting a, the closest domain to -90% that
can be generated with a convex combination of the other domains still has a ‘+’ correlation between the
color and label. In setting b, however, one can generate a domain with a ‘-’ correlation between color and
label with a convex combination of the other domains. Thus, we expect the empirical risk minimizer to give
domain-general solutions in setting b but not in setting a.

15

We use Oracle model selection (held-out target data) to remove the eﬀect of model selection for all
methods in the results. We ﬁnd that in setting a, where we add a domain (+70%), we observe that the
generalization accuracy to the -90% domain is still very diﬀerent from the other domains (Table 7).

Table 7: ColoredMNIST setting a.
-90%} indicate domains –
{0.1, 0.2, 0.3, 0.9} digit label and color correlation, respectively. We report domain accuracies over 3 tri-
als each. We use the oracle selection method – held out target data. E\etest → etest.

Columns {+90%, +80%, +70%,

+90%
Algorithm
72.8 ± 0.3
ERM
IRM
49.0 ± 0.1
71.0 ± 0.6
GroupDRO
74.1 ± 1.3
VREx
TCRI_HSIC 72.1 ± 1.5

+80%
74.7 ± 0.3
54.2 ± 2.0
72.2 ± 0.3
72.6 ± 0.5
73.6 ± 0.4

+70%
73.3 ± 0.1
50.3 ± 0.3
70.7 ± 0.9
72.1 ± 0.5
72.6 ± 0.4

-90%
16.3 ± 1.5
43.8 ± 2.8
36.4 ± 4.2
19.5 ± 5.5
49.9 ± 0.3

However, in setting b, where we add a domain (-80%), we observe that the generalization accuracy to

the -90% domain is on par with the other domains (Table 8).

Table 8: ColoredMNIST setting b.
-90%} indicate domains –
{0.1, 0.2, 0.8, 0.9} digit label and color correlation, respectively. We report the average domain accuracies
over 3 trials each. We use the oracle selection method – held out target data. E\etest → etest.

Columns {+90%, +80%,

-80%,

Algorithm
+90%
58.4 ± 1.3
ERM
56.7 ± 3.3
IRM
69.7 ± 0.8
GroupDRO
VREx
67.4 ± 1.9
TCRI_HSIC 62.2 ± 4.4

+80%
67.0 ± 0.5
56.6 ± 2.8
71.7 ± 0.3
70.4 ± 0.1
70.0 ± 1.3

-80%
64.2 ± 2.0
51.6 ± 0.7
72.0 ± 0.2
71.2 ± 0.2
67.9 ± 1.4

-90%
52.6 ± 3.2
51.7 ± 0.7
71.4 ± 1.9
59.4 ± 4.3
65.4 ± 2.8

This illustrates the challenge of accurately evaluating an algorithm’s ability to give domain-general pre-
dictions. We note that it is generally diﬃcult to distinguish between setting a and setting b. The pri-
mary signature we see is some consistency between the empirical risk minimizer and the other baselines.
Gulrajani and Lopez-Paz [2020] observe a similar trend for standard benchmarks for domain generalization.
Hence, we focus our empirical evaluations in this work on settings where we know that the ERM solution
fails by design.

A.2 ColoredMNIST

ColoredMNIST: The ColoredMNIST dataset [Arjovsky et al., 2019] is composed of 7000 (2×28×28, 1) images
of a hand-written digit and binary-label pairs. There are three domains with diﬀerent correlations between
image color and label, i.e., the image color is spuriously related to the label by assigning a color to each of the
two classes (0: digits 0-4, 1: digits 5-9). The color is then ﬂipped with probabilities {0.1, 0.2, 0.9} to create
three domains, making the color-label relationship domain-speciﬁc because it changes across domains. There
is also label ﬂip noise of 0.25, so we expect that the best accuracy a domain-general model can achieve is
75%, while a non-domain general model can achieve higher. In this dataset, Zdg corresponds to the original
image, Zspu the color, e the label-color correlation, Y the image label, and X the observed colored image.
This DAG follows the generative process of Figure 2a

We use MNIST-ConvNet [Gulrajani and Lopez-Paz, 2020] backbones for the MNIST datasets (Table 10).
Both Φdg and Φspu are linear layers of size 128 × 128 that are appended to the backbone. The predictors
(classiﬁcation hyperplanes) θc, {θ1, θ2} are also parameterized to be linear and appended to the Φdg and Φ,
respectively.

We do a random search to select hyperparameters using the same scheme as Gulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 25 hyperparameters with 5 random
restarts each to generate error bars.

16

9:

Table
https://github.com/olawalesalaudeen/tcri.

ColoredMNIST Hyperparameters.

Additional hyperparameters

are provided in

Algorithm Hyperparameter Default Random Distribution

All

TCRI β

Learning Rate
Batch Size
penalty weight
annealing steps

1−3
64
100
500

10Uniform(−4.5,−2.5)
2Uniform(3,9)
10Uniform(−1, 5)
10Uniform(2.5, 5)

Table 10: MNIST ConvNet architecture. All convolutions use 3×3 kernels and "same" padding.

#
1
2
3
4
5
6
7
8
9
10
11
12
13

Layer
Conv2D (in=d, out=64)
ReLU
GroupNorm (groups=8)
Conv2D (in=64, out=128, stride=2)
ReLU
GroupNorm (groups=8)
Conv2D (in=128, out=128)
ReLU
GroupNorm (groups=8)
Conv2D (in=128, out=128)
ReLU
GroupNorm (8 groups)
Global average-pooling

We show transfer accuracies with both source and target domain validation for model selection in Tables

11-12. We ﬁnd that TCRI outperforms all baselines in average and worst-case accuracy.

Table 11: ColoredMNIST Transfer Accuracy – model selection on held-out source validation set. Columns
{+90%, +80%, -90%} indicate domains – {0.1, 0.2, 0.9} digit label and color correlation, respectively.
E\etest → etest.

+90%
Algorithm
71.6 ± 0.3
ERM
72.1 ± 0.1
IRM
72.6 ± 0.2
GroupDRO
72.2 ± 0.2
VREx
71.0 ± 0.4
IB_ERM
IB_IRM
71.7 ± 0.2
TCRI_HSIC 67.2 ± 2.3

Domains
+80%
73.1 ± 0.1
73.0 ± 0.3
73.4 ± 0.2
72.7 ± 0.3
73.4 ± 0.3
73.4 ± 0.1
65.6 ± 3.4

-90%
10.0 ± 0.1
9.9 ± 0.1
9.9 ± 0.1
10.2 ± 0.0
10.0 ± 0.1
9.9 ± 0.0
45.9 ± 6.9

Domain Accuracy Statistics
Std
29.4 ± 0.1
29.5 ± 0.1
29.8 ± 0.1
29.3 ± 0.1
29.4 ± 0.1
29.5 ± 0.0
11.4 ± 3.3

Avg
51.6 ± 0.1
51.7 ± 0.1
52.0 ± 0.1
51.7 ± 0.2
51.5 ± 0.2
51.7 ± 0.0
59.6 ± 1.8

Min
10.0 ± 0.1
9.9 ± 0.1
9.9 ± 0.1
10.2 ± 0.0
10.0 ± 0.1
9.9 ± 0.0
45.1 ± 6.7

A.3 Spurrious PACS

images, Y : non-urban (elephant, giraﬀe, horse) vs. urban (dog, guitar,
Spurious–PACS. Variables. X:
house, person). Domains. {{cartoon, art painting}, {art painting, cartoon}, {photo}} [Li et al., 2017]. The
photo domain is the same as in the original dataset. In the {cartoon, art painting} domain, urban examples
are selected from the original cartoon domain, while non-urban examples are selected from the original art
painting domain. In the {art painting, cartoon} domain, urban examples are selected from the original art
painting domain, while non-urban examples are selected from the original cartoon domain. This sampling
encourages the model to use spurious correlations (domain-related information) to predict the labels; however,
since these relationships are ﬂipped between domains {{cartoon, art painting} and {art painting, cartoon},

17

Table 12: Oracle ColoredMNIST Transfer Accuracy – model selection on held-out target validation set
accuracy. Columns {+90%, +80%, -90%} indicate domains – {0.1, 0.2, 0.9} digit label and color correlation,
respectively. E\etest → etest.

ColoredMNIST

Spurious PACS

Terra Incognita

average
Algorithm
57.8 ± 0.2
ERM
IRM
68.9 ± 1.6
61.1 ± 1.3
GroupDRO
68.0 ± 2.5
VREx
65.0 ± 0.1
IB_ERM
IB_IRM
68.4 ± 1.0
TCRI_HSIC 70.4 ± 0.4

worst-case
38.4 ± 1.4
62.0 ± 4.9
37.6 ± 3.6
59.4 ± 7.3
50.6 ± 0.3
58.5 ± 2.8
65.7 ± 1.5

average
59.2 ± 1.3
67.5 ± 5.8
61.8 ± 1.8
62.8 ± 2.4
67.3 ± 3.7
69.0 ± 1.3
69.5 ± 1.1

worst-case
38.4 ± 1.4
53.9 ± 6.6
40.0 ± 1.6
38.7 ± 0.9
53.1 ± 8.0
62.3 ± 0.3
62.3 ± 0.2

average
52.9 ± 0.8
42.6 ± 4.0
50.7 ± 1.0
43.2 ± 2.0
49.0 ± 0.3
32.8 ± 6.6
51.2 ± 0.1

worst-case
42.0 ± 0.6
42.7 ± 1.2
42.7 ± 1.2
34.9 ± 4.2
39.9 ± 0.8
20.4 ± 7.5
43.0 ± 0.4

these predictions will be wrong when generalized to other domains.

13:

Table
https://github.com/olawalesalaudeen/tcri.

Spurrious PACS Hyperparameters.

Additional

hyperparameters

provided

in

Algorithm Hyperparameter Default

All

TCRI β

Learning Rate
Batch Size
penalty weight
annealing steps

1−3
64
100
500

Range
10Uniform(−4.5,−2.5)
2Uniform(3,9)
10Uniform(−1, 5)
10Uniform(2.5, 5)

We use a ResNet-50 backbone [He et al., 2016]. Φdg and Φspu are linear layers of size 2048 × 2048 that
are appended to the backbone. The predictors (classiﬁcation hyperplanes) θc, {θ1, θ2, θ3} are linear and
appended to Φdg and Φ layers, respectively.

Hyperparameters: We do a random search to select hyperparameters using the same scheme as Gulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 5 hyperparameters with 3 random
restarts each to generate error bars.

We show transfer accuracies with both source and target domain validation for model selection in Tables

14-15. We ﬁnd that TCRI outperforms all baselines in average and worst-case accuracy.

Table 14: Spurious–PACS Transfer Accuracy – model selection on held-out source validation set. E\etest →
etest.

Domain Accuracy Statistics
std
29.0 ± 0.4
28.6 ± 0.8
26.4 ± 0.3
26.2 ± 1.0
20.8 ± 0.6
27.8 ± 1.5
1.2± 0.2

mean
57.2 ± 0.7
54.7 ± 0.8
58.5 ± 0.4
58.8 ± 0.4
56.3 ± 1.1
55.9 ± 1.2
63.4 ± 0.2

min
31.2 ± 1.3
30.3 ± 0.3
37.7 ± 0.
37.5 ± 1.1
35.5 ± 0.4
33.8 ± 0.4
62.3 ± 0.2

Algorithm
C x A
ERM
31.2 ± 1.3
IRM
30.3 ± 0.3
GroupDRO
37.7 ± 0.7
37.5 ± 1.1
VREx
IB_ERM
35.5 ± 0.4
33.8 ± 2.2
IB_IRM
TCRI_HSIC 62.8 ± 0.1

Domains
A x C
42.8 ± 0.7
39.0 ± 1.3
42.1 ± 1.6
43.0 ± 0.5
48.6 ± 3.3
38.8 ± 3.0
62.3 ± 0.2

P
97.6 ± 0.2
94.9 ± 1.4
95.7 ± 0.5
95.7 ± 1.5
84.8 ± 0.6
95.1 ± 1.5
65.0 ± 0.4

18

Table 15: Oracle Spurious–PACS Transfer Accuracy – model selection on held-out target validation set.
E\etest → etest.

Domains
C x A
Algorithm
38.4 ± 1.4
ERM
62.8 ± 0.1
IRM
40.0 ± 1.6
GroupDRO
55.8 ± 5.5
VREx
53.1 ± 8.0
IB_ERM
IB_IRM
62.8 ± 0.1
TCRI_HSIC 64.0 ± 0.7

Domain Accuracy Statistics

A x C
43.4 ± 1.9
53.9 ± 6.6
49.7 ± 2.9
38.7 ± 0.9
55.4 ± 5.7
62.3 ± 0.3
62.3 ± 0.2

P
95.9 ± 0.6
85.8 ± 8.2
95.7 ± 0.6
93.8 ± 0.8
93.5 ± 1.8
81.8 ± 7.0
82.4 ± 5.7

mean
59.2
67.5
61.8
62.8
67.3
69.0
69.5

std min
38.4
26.0
53.9
13.4
40.0
24.3
38.7
23.0
53.1
18.5
62.3
9.1
62.3
9.1

A.4 Terra Incognita

The Terra Incognita dataset contains subsets of the Caltech Camera Traps dataset [Beery et al., 2018] deﬁned
by [Gulrajani and Lopez-Paz, 2020]. Four domains represent diﬀerent locations {L100, L38, L43, L46} of
cameras in the American Southwest. There are 10 diﬀerent species of wild animals {bird, bobcat, cat, coyote,
dog, empty, opossum, rabbit, raccoon, squirrel} (classes) to be predicted. Like Ahuja et al. [2021], we classify
this dataset as following the generative process in Figure 2c, the Fully Informative Invariant Features (FIIF)
setting.

Table
https://github.com/olawalesalaudeen/tcri.

Terra

Incognita Hyperparameters.

16:

Additional

hyperparameters

provided

in

Algorithm Hyperparameter Default

All

TCRI β

Learning Rate
Batch Size
penalty weight
annealing steps

1−3
64
100
500

Range
10Uniform(−4.5,−2.5)
2Uniform(3,9)
10Uniform(−1, 5)
10Uniform(0, 4)

We use a ResNet-50 backbone [He et al., 2016]. Φdg and Φspu are linear layers of size 2048 × 2048 that
are appended to the backbone. The predictors (classiﬁcation hyperplanes) θc, {θ1, θ2, θ3, θ4} are linear and
appended to Φdg and Φ layers, respectively.

Hyperparameters: We do a random search to select hyperparameters using the same scheme as Gulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 5 hyperparameters with 3 random
restarts each to generate error bars.

We show transfer accuracies with both source and target domain validation for model selection in Tables
17-18. We ﬁnd that TCRI outperforms all baselines except ERM on average and outperforms all baselines
in worst-case accuracy.

19

Table 17: Terra Incognita Transfer Accuracy – model selection on held-out source validation set. E\etest →
etest.

Domains

L100
Algorithm
43.6 ± 3.9
ERM
IRM
43.9 ± 3.3
53.8 ± 4.6
GroupDRO
48.8 ± 2.0
VREx
IB_ERM
46.1 ± 4.5
IB_IRM
39.7 ± 7.3
TCRI_HSIC 54.6 ± 2.4

L38
45.2 ± 0.6
35.7 ± 4.0
40.5 ± 0.7
38.1 ± 1.3
40.7 ± 0.7
40.8 ± 2.3
48.6 ± 2.0

L43
53.0 ± 1.2
37.7 ± 7.8
55.3 ± 1.5
54.4 ± 0.6
55.2 ± 0.8
34.7 ± 4.3
53.2 ± 1.0

L46
35.1 ± 2.8
38.3 ± 2.4
41.8 ± 1.1
39.0 ± 1.4
42.2 ± 1.1
32.9 ± 2.6
40.4 ± 1.6

Domain Accuracy Statistics
Std
6.8 ± 1.0
5.4 ± 1.8
7.7 ± 0.9
7.0 ± 0.9
6.4 ± 0.8
6.7 ± 1.3
6.1 ± 1.1

Avg
44.2 ± 1.8
38.9 ± 3.7
47.8 ± 0.9
45.1 ± 0.4
46.0 ± 1.4
37.0 ± 2.8
49.2 ± 0.3

Min
35.1 ± 2.8
32.6 ± 4.7
39.9 ± 0.7
38.1 ± 1.3
39.3 ± 1.1
29.6 ± 4.1
40.4 ± 1.6

Table 18: Oracle Terra Incognita Transfer Accuracy – model selection on held-out target validation set.
E\etest → etest.

Domain Accuracy Statistics
Std
7.0 ± 0.5
9.6 ± 1.7
6.9 ± 0.9
6.5 ± 1.8
6.4 ± 0.5
8.2 ± 1.0
5.8 ± 0.7

Min
42.0 ± 0.6
30.8 ± 5.4
42.7 ± 1.2
34.9 ± 4.2
39.9 ± 0.8
20.4 ± 7.5
43.0 ± 0.4

Avg
52.9 ± 0.8
42.6 ± 4.0
50.7 ± 1.0
43.2 ± 2.0
49.0 ± 0.3
32.8 ± 6.6
51.2 ± 0.1

L100
Algorithm
58.5 ± 1.8
ERM
53.0 ± 0.9
IRM
56.2 ± 3.0
GroupDRO
VREx
43.2 ± 1.5
55.6 ± 1.7
IB_ERM
IB_IRM
40.2 ± 8.2
TCRI_HSIC 57.7 ± 1.8

Domains

L38
52.0 ± 1.3
48.0 ± 1.8
45.2 ± 2.3
49.3 ± 1.2
47.2 ± 1.1
31.9 ± 11.8
50.1 ± 1.8

L43
59.2 ± 0.2
36.3 ± 9.6
58.0 ± 0.2
41.5 ± 7.8
53.4 ± 0.7
29.4 ± 4.4
54.1 ± 0.6

L46
42.0 ± 0.6
33.2 ± 3.9
43.3 ± 0.7
38.9 ± 1.1
39.9 ± 0.8
29.7 ± 3.8
43.0 ± 0.4

B DAGs

e

X

Zdg

Y

Zspu

Figure 4: Partial Ancestral Graph (PAG). Dashed edges indicate that the edge may or may not exist. The
combination of Y → Zdg → Zspu, and Y → Zdg, e → Zdg is not allowed.

B.1 On Valid DAGS:

We consider other edges that could be introduced to Figure 4 where Zdg 6⊥⊥ Zspu | Y, e, Zspu 6⊥⊥ Y | Zdg, or
are not included in Figure 5. We then show that these edges either make the problem intractable or require
new assumptions about the generative process – note we do not discuss edges that induce a cycle, thus, are
invalid.

(i) e − Y : we cannot have a direct edge in either direction e between Y otherwise, Y is always dependent

on e and the problem becomes intractable.

(ii) e − X: we cannot have a direct edge from e − X without making additional parametric assumptions

about the role of e in Γ(Zdg, Zspu, e).

(iii) Zspu → Y : we cannot have both Zdg → Y and Zspu → Y , since then, both mechanisms are domain

general. WLOG, we let Zspu denote the features that never have domain-general mechanisms to Y .

20

(iv) Y → Zdg → Zspu and Y → Zdg ← e: conditioning on Zdg and/or Zspu make Y dependent on e, so Y

is always dependent on e and the problem becomes intractable.

e

X

Zdg

Y

Zspu

(a)

e

X

Zdg

Y

Zspu

(b)

e

X

Zdg

Y

Zspu

(c)

Figure 5: Generative Processes. Graphical model depicting the structure of our data-generating process -
shaded nodes indicate observed variables. X represents the observed features, Y represents observed targets,
and e represents domain inﬂuences. There is an explicit separation of domain-general Zdg and domain-
speciﬁc Zspu features combined to generate observed X. Dashed edges indicate the possibility of an edge.

Table 19: Generative Processes and Suﬃcient Conditions for Domain-Generality

Graphs in Figure 5
(a)
✓
Identifying Zdg is necessary ✓

Zdg ⊥⊥ Zspu | {Y, e}

(b)
✓
✓

(c)
✗
✗

Table 20: Generative Processes and Suﬃcient Algorithms

Graphs in Figure 5
(a)
Solved by ERM ✗
Solved by TCRI ✓

(b)
✗
✓

(c)
✓
✓

B.2 Fully Informative Invariant Features

We brieﬂy summarize Ahuja et al. [2021]’s results on minimax-optimality of Empirical Risk Minimization in
the Fully Informative Invariant Features setting (their Lemma 4). First, we informally state their assump-
tions.

Assumption 2: Linear structural equation model.

Assumption 3-4: Bounded Features.

Assumption 8: wdg partitions Z up to noise ηY .

These assumptions are implied by our Assumption 4.1.

B.2.1 Proof Suﬃciency of ERM [Ahuja et al., 2021]

If Assumptions 2, 4, and 8 hold, then there exists a classiﬁer that puts a non-zero weight on the spurious
feature and continues to be Bayes optimal in all the training environments.

21

Proof. Choose an arbitrary non-zero vector and derive a bound on the margin of (wdg, γ), where wdg is
the true (optimal) linear predictor of Y from Zdg. Recall domain-general and domain-speciﬁc features
zdg ∈ Zdg, zspu ∈ Zspu, respectively. Let y∗ = sign(wdg · zdg). The margin of (wdg, γ)) at point (zdg, zspu)
with respect to y∗ is deﬁned as:

y∗(wdg · zdg) + y∗(γ · zspu).

Using Cauchy-Schwartz inequality, we get

|y∗(γ · zspu)| = |γ · zspu| ≤ kkγkzspuk.

|γ · zspu| ≤ c

Since Zspu is bounded, one can set γ suﬃciently small enough to control y∗(γ · Zspu). If kγk ≤ c
2 , where zsup satisﬁes that kzk ≤ zsup∀z ∈ Zspu. From Assumption 8, ∃ c > 0 s.t.,
y∗(wdg · zdg) ≥ c.

2zsup , then

Using |γ · zspu| ≤ c

2 , the margin becomes

y∗(wdg · zdg) + y∗(γ · zspu) ≥ c − |γ · zspu| ≥

c
2

.

From the above equation, it follows that sign

(wdg, γ) · (zdg, zspu)

= sign

(wdg, 0) · (zdg, zspu)

∀zdg ∈

Zdg, zspu ∈ Zspu.

Now, this condition is used to compute the error of a spurious classiﬁer, i.e., based on (, γ). Deﬁne
gspu = I ◦ (wdg, γ) ◦ Γ−1, where I(·) is an indicator function that returns 1 if its input is ≥ 0. The error
achieved by gspu is

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Re(gspu) = E
= E
(cid:2)
h
= E[ηy].
(cid:0)

I

Y e ⊕ I((wdg, γ) · (zdg, zspu)

(wdg, 0) · (zdg, zspu)

⊕ ηy ⊕ I
(cid:3)

(wdg, γ) · (zdg, zspu)

(cid:1)

(cid:0)

(cid:1)i

The error achieved by gspu is then due to the noise in observed Y and is, therefore, optimal in all

domains.

It follows from above that since gspu is Bayes optimal in every domain, it is also the empirical risk

minimizer (ERM) as it minimizes the sum of risks across training domains.

C Proof of Proposition 4.5

Assume that Φdg(X) and Φspu(X) are correlated with Y . Given Assumptions 4.1-4.2 and a representation
Φ = Φdg ⊕ Φspu that satisﬁes TIC, Φdg(X) = Zdg ⇐⇒ Φ satisﬁes TCRI.

Proof. ‘only if’. Assume that Φdg(X) = Zdg. By the Total Information Criterion, we have that Φspu(X) =
Zspu. We observe the following paths from Zdg to Zspu: (i) Zdg → Y → Zspu, (ii) Zdg ← e → Zspu, and
(iii) Zdg → X → Zspu. Conditioning on Y, e blocks both paths (i) and path (ii); path (iii) contains a collider
(Zdg and Zspu are common causes of X), so this path is blocked when X is not in the conditioning set. So,
Zspu ⊥⊥ Zdg | Y, e and therefore Φdg(X) ⊥⊥ Φspu(X) | Y, e, which completes this direction.

‘if’. Assume that Φ satisﬁes TCRI. We proceed by contradiction. Let Φ = [Φdg; Φspu]. We consider the

following scenario for Φdg 6= Zdg.

Scenario 1 (causal aggregation): Assume that Φdg(X) ⊂ Zdg. From TIC, we have that Z †

dg ⊂ Zdg is the subset of Zdg not captured by Φdg. Since Φdg(X) and Z †

dg ⊂ Φspu(X),
where Z †
dg are colliders on Y , given
both are subsets of Zdg, Φdg(X) 6⊥⊥ Φspu(X)|Y, e, violating TCRI and giving a contradiction. So, Zdg ⊂ Φ(X)
spu ⊂ Φspu(X),
spu ⊂ Zspu is the subset of Zspu not captured by Φdg. From Assumption 4.2 (faithfulness), we have

Scenario 2 (anticausal exclusion): Assume that Φdg(X) ⊂ Zspu. From TIC, we have that Z †

where Z †
that Φdg(X) 6⊥⊥ Φspu(X)|Y, e, violating TCRI and giving a contradiction. So, Zspu 6⊂ Φdg(X).

Combining scenarios 1-2, it follows that Φdg(X) = Zdg.

22

