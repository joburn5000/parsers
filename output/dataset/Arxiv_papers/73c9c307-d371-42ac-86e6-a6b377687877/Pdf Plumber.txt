4202
rpA
52
]GL.sc[
1v77261.4042:viXra
Causally Inspired Regularization Enables Domain General
Representations
Olawale Salaudeen∗1 and Sanmi Koyejo2
1University of Illinois at Urbana Champaign
2Stanford University
Abstract
Givenacausalgraphrepresentingthedata-generatingprocesssharedacrossdifferentdomains/distributions,
enforcingsufficientgraph-impliedconditionalindependenciescanidentifydomain-general(non-spurious)
featurerepresentations. Forthestandardinput-outputpredictivesetting,wecategorizethesetofgraphs
considered in the literature into two distinct groups: (i) those in which the empirical risk minimizer
across training domains gives domain-general representations and (ii) those where it does not. For the
latter case (ii), we propose a novel framework with regularizations, which we demonstrate are suffi-
cient for identifying domain-general feature representations without a priori knowledge (or proxies) of
the spurious features. Empirically, our proposed method is effective for both (semi) synthetic and real-
worlddata,outperformingotherstate-of-the-artmethodsinaverageandworst-domaintransferaccuracy.
1 Introduction
A key feature of machine learning is its capacity to generalize across new domains. When these domains
present different data distributions, the algorithm must leverage shared structural concepts to achieve out-
of-distribution (OOD) or out-of-domain generalization. This capability is vital in numerous important real-
world machine learning applications. For example, in safety-critical settings such as autonomous driving,
a lack of resilience to unfamiliar distributions could lead to human casualties. Likewise, in the healthcare
sector,whereethicalconsiderationsarecritical,aninabilitytoadjusttoshiftsindatadistributioncanresult
in unfair biases, manifesting as inconsistent performance across different demographic groups.
An influential approach to domain generalization is Invariant Causal Prediction (ICP; [Peters et al.,
2016]). ICPposits thatalthoughsomeaspectsofdatadistributions (likespuriousornon-causalmechanisms
[Pearl, 2010]) may change across domains, certain causal mechanisms remain constant. ICP suggests fo-
cusing on these invariant mechanisms for prediction. However, the estimation method for these invariant
mechanisms suggested by [Peters et al., 2016] struggles with scalability in high-dimensional feature spaces.
To overcomethis, Arjovsky et al.[2019]introducedInvariantRiskMinimization(IRM), designedto identify
these invariant mechanisms by minimizing an objective. However, requires strong assumptions for identify-
ing the desired domain-general solutions [Ahuja et al., 2021, Rosenfeld et al., 2022]; for instance, observing
a number of domains proportional to the spurious features’ dimensions is necessary, posing a significant
challenge in these high-dimensional settings.
Subsequent variants of IRM have been developed with improved capabilities for identifying domain-
generalsolutions[Ahuja et al.,2020,Krueger et al.,2021,Robey et al.,2021,Wang et al.,2022,Ahuja et al.,
2021]. Additionally, regularizers for Distributionally Robust Optimization with subgroup shift have been
proposed (GroupDRO) [Sagawa et al., 2019]. However, despite their solid theoretical motivation, empirical
evidence suggests that these methods may not consistently deliver domain-general solutions in practice
Gulrajani and Lopez-Paz [2020], Kaur et al. [2022], Rosenfeld et al. [2022].
∗Contact: oes2@illinois.edu
1Kaur et al. [2022] demonstrated that regularizing directly for conditional independencies implied by the
generative process can give domain-general solutions, including conditional independencies beyond those
considered by IRM. However, their experimental approach involves regularization terms that require direct
observation of spurious features, a condition not always feasible in real-world applications. Our proposed
methodologyalsoleveragesregularizersinspiredbytheconditionalindependenciesindicatedbycausalgraphs
but, crucially, it does so without necessitating prior knowledge (or proxies) of the spurious features.
1.1 Contributions
In this work,
• we outline sufficient properties to uniquely identify domain-general predictors for a general set of
generative processes that include domain-correlatedspurious features,
• we propose regularizersto implement these constraints without independent observations of the spuri-
ous features, and
• finally, we show that the proposed framework outperforms the state-of-the-art on semi-synthetic and
real-worlddata.
The code for our proposed method is provided at https://github.com/olawalesalaudeen/tcri.
Notation: Capital letters denote bounded random variables, and corresponding lowercase letters denote
their value. Unless otherwise stated, we represent latent domain-general features as Z ∈ Z ≡ Rm and
dg dg
spuriouslatentfeaturesasZ ∈Z ≡Ro. LetX ∈X ≡Rd betheobservedfeaturespaceandtheoutput
spu spu
space of an invertible function Γ : Z ×Z 7→ X and Y ∈ Y ≡ {0,1,...,K −1} be the observed label
dg spu
spacefor a K-classclassificationtask. We then define feature extractorsaimedatidentifying latentfeatures
Φ : X 7→Rm, Φ :X 7→Ro so that Φ: X 7→Rm+o that is Φ(x)=[Φ (x);Φ (x)]∀x ∈X . We define
dg spu dg spu
e as a discrete random variable denoting domains and E ={Pe(Z ,Z ,X,Y):e=1,2,...} to be the set
dg spu
(cid:0) (cid:1)
of possible domains. E ⊂E is the set of observed domains available during training.
tr
2 Related Work
The source of distribution shift can be isolated to components of the joint distribution. One special case
of distribution shift is covariate shift [Shimodaira, 2000, Zadrozny, 2004, Huang et al., 2006, Gretton et al.,
2009, Sugiyama et al., 2007, Bickel et al., 2009, Chen et al., 2016, Schneider et al., 2020], where only the co-
variatedistributionP(X)changesacrossdomains. Ben-David et al.[2009]giveupper-boundsontargeterror
based on the H-divergence between the source and target covariate distributions, which motivates domain
alignmentmethodsliketheDomainAdversarialNeuralNetworks[Ganin et al.,2016]andothers[Long et al.,
2015, Blanchard et al., 2017]. Others have followedup on this work with other notions of covariatedistance
for domain adaptation, such as mean maximum discrepancy (MMD) [Long et al., 2016], Wasserstein dis-
tance [Courty et al., 2017], etc. However, Kpotufe and Martinet [2018] show that these divergence metrics
failtocapturemanyimportantpropertiesoftransferability,suchasasymmetryandnon-overlappingsupport.
Furthermore, Zhao et al. [2019] shows that even with the alignment of covariates, large distances between
labeldistributionscaninhibittransfer;theyproposealabelconditionalimportanceweightingadjustmentto
address this limitation. Other works have also proposed conditional covariate alignment [des Combes et al.,
2020, Li et al., 2018c,b].
Another formof distributionshift is label shift, where only the label distributionchangesacrossdomains.
Lipton et al. [2018] propose a method to address this scenario. Schrouff et al. [2022] illustrate that many
real-worldproblems exhibit more complex ’compound’ shifts than just covariate or label shifts alone.
Onecanleveragedomain adaptationtoaddressdistributionshifts;however,thesemethodsarecontingent
onhavingaccesstounlabeledorpartiallylabeledsamplesfromthetargetdomainduringtraining. Whensuch
samples are available, more sophisticated domain adaptation strategies aim to leverage and adapt spurious
feature information to enhance performance [Liu et al., 2021, Zhang et al., 2021, Kirichenko et al., 2022].
2However, domain generalization, as a problem, does not assume access to such samples [Muandet et al.,
2013].
Toaddress the domaingeneralizationproblem, InvariantCausalPredictors(ICP) leveragesharedcausal
structure to learn domain-general predictors [Peters et al., 2016]. Previous works, enumerated in the intro-
duction (Section 1), have proposedvarious algorithms to identify domain-generalpredictors. Arjovsky et al.
[2019]’s proposed invariance risk minimization (IRM) and its variants motivated by domain invariance:
1
min Re(w◦Φ) s.t. w ∈argminRe(w·Φ),∀e∈E ,
tr
w,Φ |E tr| we
e X∈Etr
e
where Re(w◦Φ)=E ℓ(y,w·Φ(x)) , with loss function ℓ, feature extractor Φ, and linear predictor w. This
objective aims to learn a representation Φ such that predictor w that minimizes empirical risks on average
(cid:2) (cid:3)
across all domains also minimizes within-domain empirical risk for all domains. However, Rosenfeld et al.
[2020], Ahuja et al. [2020] showed that this objective requires unreasonable constraints on the number of
observed domains at train times, e.g., observing distinct domains on the order of the rank of spurious
features. Follow-up works have attempted to improve these limitations with stronger constraints on the
problem – enumerated in the introduction section.
Our method falls under domain generalization; however, unlike the domain-general solutions previously
discussed, our proposed solution leverages different conditions than domain invariance directly, which we
show may be more suited to learning domain-generalrepresentations.
3 Causality and Domain Generalization
We often represent causal relationships with a causal graph. A causal graph is a directed acyclic graph
(DAG), G = (V,E), with nodes V representing random variables and directed edges E representing causal
relationships, i.e., parents are causes and children are effects. A structural equation model (SEM) provides
a mathematical representation of the causal relationships in its corresponding DAG. Each variable Y ∈ V
is given by Y = f (X)+ε , where X denotes the parents of Y in G, f is a deterministic function, and
Y Y Y
ε is an error capturing exogenousinfluences on Y. The main property we need here is that f is invariant
Y Y
to interventions to V\{Y} and is consequently invariantto changes in P(V) induced by these interventions.
Interventions refer to changes to f , Z ∈V\{Y}.
Z
Inthis work,we focus on domain-generalpredictorsd that are linear functions offeatures with domain-
g
generalmechanisms,denotedasg :=w◦Φ ,wherew isalinearpredictorandΦ identifiesfeatureswith
dg dg dg
domain-general mechanisms. We use domain-general rather than domain-invariant since domain-invariance
is strongly tied to the property: Y ⊥⊥e|Z [Arjovsky et al., 2019]. As shown in the subsequent sections,
dg
this work leverages other properties of appropriate causal graphs to obtain domain-general features. This
distinction is crucial given the challenges associated with learning domain-generalfeatures through domain-
invariance methods [Rosenfeld et al., 2020].
Giventhepresenceofadistributionshift,it’sessentialtoidentifysomecommonstructureacrossdomains
that can be utilized for out-of-distribution (OOD) generalization. For example, Shimodaira [2000] assume
P(Y|X) is shared across all domains for the covariate shift problem. In this work, we consider a setting
where each domain is composed of observed features and labels, X ∈ X,Y ∈ Y, where X is given by an
invertible function Γ of two latent random variables: domain-general Z ∈ Z and spurious Z ∈ Z .
dg dg spu spu
By construction, the conditional expectation of the label Y given the domain-general features Z is the
dg
same across domains, i.e.,
E [Y|Z =z ]=E [Y|Z =z ] (1)
ei dg dg ej dg dg
∀z ∈Z ,∀e 6=e ∈E.
dg dg i j
Conversely, this robustness to e does not necessarily extend to spurious features Z ; in other words, Z
spu spu
may assume values that could lead a predictor relying on it to experience arbitrarily high error rates. Then,
a sound strategy for learning a domain-general predictor – one that is robust to distribution shifts – is to
identify the latent domain-general Z from the observed features X.
dg
3e Z dg Y
X Z spu
Figure1: PartialAncestralGraphrepresentingallnon-trivialandvalidgenerativeprocesses(DAGs);dashed
edges indicate that an edge may or may not exist.
TheapproachwetaketodothisismotivatedbytheReichenbach Common CausePrinciple, whichclaims
that if two events are correlated, there is either a causal connection between the correlated events that is
responsible for the correlation or there is a third event, a so-called (Reichenbachian) common cause, which
brings about the correlation[Hitchcock and Rédei, 2021, Rédei, 2002]. This principle allows us to posit the
class of generative processes or causal mechanisms that give rise to the correlated observed features and
labels, where the observed features are a function of domain-general and spurious features. We represent
these generative processes as causal graphs. Importantly, the mapping from a node’s causal parents to
itself is preserved in all distributions generated by the causal graph (Equation 1), and distributions can vary
arbitrarily so long as they preserve the conditional independencies implied by the DAG (Markov Property
[Pearl, 2010]).
We now enumerate DAGs that give observe features with spurious correlations with the label.
Valid DAGs. We consider generative processes, where both latent features, Z spu,Z dg, and observed X
are correlatedwith Y, and the observed X is a function of only Z and Z (Figure 1).
dg spu
Giventhis setup, there is an enumerableset of valid generativeprocesses. Such processesare (i) without
cycles, (ii) are feature complete – including edges from Z and Z to X, i.e., Z → X ← Z , and
dg spu dg spu
(iii) where the observed features mediate domain influence, i.e., there is no direct domain influence on the
label e6→Y. We discuss this enumeration in detail in Appendix B. The result of our analysis is identifying
a representative set of DAGs that describe valid generative processes – these DAGs come from orienting
the partial ancestral graph (PAG) in Figure 1. We compare the conditional independencies implied by the
DAGs defined by Figure 1 as illustrated inFigure 2, resulting in three canonicalDAGs in the literature (see
Appendix B for further discussion). Other DAGs that induce spurious correlations are outside the scope of
this work.
e Z dg Y e Z dg Y e Z dg Y
X Z spu X Z spu X Z spu
(a) Causal [Arjovsky et al., 2019]. (b) Anticausal [Rosenfeld et al., (c) Fully Informative Causal
2020]. [Ahuja et al., 2021].
Figure 2: Generative Processes. Graphical models depicting the structure of possible data-generating
processes – shaded nodes indicate observed variables. X represents the observed features, Y represents
observed targets, and e represents domain influences (domain indexes in practice). There is an explicit
separationofdomain-generalZ anddomain-specificZ features;theyarecombinedtogenerateobserved
dg spu
X. Dashed edges indicate the possibility of an edge.
Conditional independencies implied by identified DAGs (Figure 2).
4Table 1: Generative Processes and Sufficient Conditions for Domain-Generality
Graphs in Figure 2
(a) (b) (c)
Z ⊥⊥Z |{Y,e} ✓ ✓ ✗
dg spu
Identifying Z is necessary ✓ ✓ ✗
dg
Fig. 2a: Z dg ⊥⊥Z spu|{Y,e}; Y ⊥⊥e|Z dg.
This causal graphical model implies that the mapping from Z to its causal child Y is preserved and
dg
consequently, Equation 1 holds [Pearl, 2010, Peters et al., 2016]. As an example, consider the task of
predicting the spread of a disease. Features may include causes (vaccination rate and public health
policies)andeffects(coughing). eisthetimeofmonth;thedistributionofcoughingchangesdepending
on the season.
Fig. 2b: Z dg ⊥⊥Z spu|{Y,e}; Z dg ⊥⊥Z spu|Y; Y ⊥⊥e|Z dg, Z dg ⊥⊥e.
ThecausalgraphicalmodeldoesnotdirectlyimplythatZ →Y ispreservedacrossdomains. However,
dg
in this work, it represents the setting where the inverse of the causal direction is preserved (inverse:
Z →Y), and thus Equation 1 holds. A context where this setting is relevant is in healthcare where
dg
medical conditions (Y) cause symptoms (Z ), but the prediction task is often predicting conditions
dg
from symptoms, and this mapping Z → Y, opposite of the causal direction, is preserved across
dg
distributions. Again, we may consider e as the time of month; the distribution of coughing changes
depending on the season.
Fig. 2c: Y ⊥⊥e|Z dg; Z dg ⊥⊥e.
SimilartoFigure2a,thiscausalgraphicalmodelimpliesthatthemappingfromZ toitscausalchildY
dg
ispreserved,soEquation1holds[Pearl,2010,Peters et al.,2016]. Thissettingisespeciallyinteresting
because it represents a Fully Informative Invariant Features setting, that is Z spu ⊥⊥ Y |Z dg
[Ahuja et al., 2021]. Said differently, Z does not induce a backdoorpath from e to Y that Z does
spu dg
not block. As an example of this, we can consider the task of predicting hospital readmission rates.
Features may include the severity of illness, which is a direct cause of readmission rates, and also
include the length of stay, which is also caused by the severity of illness. However, length of stay may
not be a cause of readmission; the correlation between the two would be a result of the confounding
effect of a common cause, illness severity. e is an indicator for distinct hospitals.
We call the condition Y⊥⊥e|Z the domain invariance property. This condition is common to all
dg
the DAGs in Figure 2. We call the condition Z ⊥⊥Z |{Y,e} the target conditioned representation
dg spu
independence (TCRI) property. This condition is common to the DAGs in Figure 2a, 2b. In the settings
considered in this work, the TCRI property is equivalently Z ⊥⊥Z |Y∀e∈E since e will simply index
dg spu
the set of empirical distributions available at training.
Domain generalization with conditional independencies. Kaur et al.[2022]showedthatsufficiently
regularizingfor the correctconditionalindependencies described by the appropriateDAGs can give domain-
generalsolutions,i.e.,identifiesZ . However,inpractice,onedoesnot(partially)observethelatentfeatures
dg
independently to regularize directly. Other works have also highlighted the need to consider generative pro-
cesseswhendesigningrobustalgorithmstodistributeshifts[Veitch et al.,2021,Makar et al.,2022]. However,
previousworkhaslargelyfocusedonregularizingforthedomaininvarianceproperty,ignoringtheconditional
independence property Z ⊥⊥Z |Y,e.
dg spu
Sufficiency of ERM under Fully Informative Invariant Features. Despite the known challenges of
learning domain-general features from the domain-invariance properties in practice, this approach persists,
5likely due to it being the only property shared across all DAGs. We alleviate this constraint by observing
that Graph (Fig. 2c) falls under what Ahuja et al. [2021] refer to as the fully informative invariant features
settings,meaningthatZ isredundant,havingonlyinformationaboutY thatisalreadyinZ . Ahuja et al.
spu dg
[2021] show that the empirical risk minimizer is domain-generalfor bounded features.
Easy vs. hard DAGs imply the generality of TCRI. Consequently, we categorize the generative
processes into easy and hard cases Table 1: (i) easy meaning that minimizing average risk gives domain-
general solutions, i.e., ERM is sufficient (Fig. 2c), and (ii) hard meaning that one needs to identify Z to
dg
obtaindomain-generalsolutions(Figs. 2a-2b). WeshowempiricallythatregularizingforZ ⊥⊥Z |Y∀e∈
dg spu
E alsogivesadomain-generalsolutionintheeasycase. ThegeneralityofTCRIfollowsfromitssufficiency
for identifying domain-general Z in the hard cases while still giving domain-general solutions empirically
dg
in the easy case.
4 Proposed Learning Framework
We have now clarified that hard DAGs (i.e., those not solved by ERM) share the TCRI property. The
challenge is that Z and Z are not independently observed; otherwise, one could directly regularize.
dg spu
Existing work such as Kaur et al. [2022] empirically study semi-synthetic datasets where Z is (partially)
spu
observed and directly learn Z by regularizing that Φ(X) ⊥⊥ Z |Y,e for feature extractor Φ. To our
dg spu
knowledge, we are the first to leverage the TCRI property without requiring observation of Z . Next, we
spu
set up our approach with some key assumptions. The first is that the observed distributions are Markov to
an appropriate DAG.
Assumption 4.1. All distributions, sources and targets, are generated by one of the structural causal
models SCM that follow:
Z(e) ∼P(e), Y(e) ∼P ,
causal
dg Zdg
anticausal
Y
SCM(e):=Y(e) ←hw d∗ g,Z d(e g)i+η Y,
(2) SCM(e)
:=Z d(e g) ←hw dg,Yi+η Z(e d) g,
(3)
z }| {
Z s( pe u) ←hw s∗ pu,Yi+η Z(e s) pu,
z }| {
Z s( pe u) ←hw es∗ pu,Yi+η Z(e s) pu,
X ←Γ(Z ,Z ), X ←Γ(Z ,Z ),
dg spu dg spu

Z(e) ∼P(e),

FIIF
dg Zdg
SCM(e):=Y(e) ←hw d∗ g,Z d(e g) i+η Y,
(4)
z }| {
Z s( pe u) ←hw s∗ pu,Z dgi+η Z(e s) pu,
X ←Γ(Z ,Z ),
dg spu

where P is the causal covariate distribution, w’s are linear generative mechanisms, η’s are exogenous
Zdg
independent noise variables,and Γ:Z ×Z →X is an invertible function. It follows fromhaving causal
dg spu
mechanisms that we can learn a predictor w∗ for Z that is domain-general (Equation 2-4) – w∗ inverts
dg dg dg
the mapping w in the anticausal case.
dg
These structural causal models (Equation 2-4) correspond to causal graphs Figures 2a-2c, respectively.
e
Assumption4.2 (Structural). CausalGraphsandtheirdistributionsareMarkovandFaithful[Pearl,2010].
Given Assumption 4.2, we aim to leverage TCRI property (Z ⊥⊥ Z |Y∀e ∈ E ) to learn the latent
dg spu tr
Z without observing Z directly. We do this by learning two feature extractors that, together, recover
dg spu
Z and Z and satisfy TCRI (Figure 3). We formally define these properties as follows.
dg spu
Definition 4.3 (Total Information Criterion (TIC)). Φ=Φ dg⊕Φ spu satisfies TIC with respect to random
variables X, Y, e if for Φ(Xe) = [Φ (Xe);Φ (Xe)], there exists a linear operator T s.t., T(Φ(Xe)) =
dg spu
[Ze ;Ze ]∀e∈E .
dg spu tr
6Φ dg Z dg θ c y c
b b
Xe ⊕ θ y
e e
Φ b
spu Z
spu
Figure 3: Modeling approach. During trbaining, both representations, Φ dg, and Φ spu, generate domain-
general and domain-specific predictions, respectively. However, only the domain-invariant representa-
tions/predictions are used during testing – indicated by the solid red arrows.
In other words, a feature extractor that satisfies the total information criterion recovers the complete
latent feature sets Z , Z . This allows us to define the proposed implementation of the TCRI property
dg spu
non-trivially – the conditional independence of subsets of the latents may not have the same implications
on domain generalization. We note that X ⊥⊥Y|Z ,Z , so X has no information about Y that is not in
dg spu
Z ,Z .
dg spu
Definition 4.4 (Target Conditioned Representation Independence). Φ = Φ dg⊕Φ spu satisfies TCRI with
respect to random variables X, Y, e if Φ (X)⊥⊥Φ (X)|Y∀e∈E.
dg spu
Proposition4.5. Assumethat Φ dg(X)and Φ spu(X) are correlated with Y. Given Assumptions4.1-4.2 and
a representation Φ=Φ ⊕Φ that satisfies TIC, Φ (X)=Z ⇐⇒ Φ satisfies TCRI. (see Appendix C
dg spu dg dg
for proof).
Proposition4.5showsthatTCRIisnecessaryandsufficienttoidentifyZ fromasetoftrainingdomains.
dg
WenotethatwecanverifyifΦ (X)andΦ (X)arecorrelatedwithY bycheckingifthelearnedpredictors
dg spu
are equivalent to chance. Next, we describe our proposed algorithm to implement the conditions to learn
such a feature map. Figure 3 illustrates the learning framework.
Learning Objective: The first term in our proposed objective is
L =Re(θ ◦Φ ),
Φdg c dg
where Φ : X 7→ Rm is a feature extractor, θ : Rm 7→ Y is a linear predictor, and Re(θ ◦ Φ ) =
dg c c dg
E ℓ(y,θ ·Φ(x)) is the empirical risk achieved by the feature extractor and predictor pair on samples from
c
domain e. Φ and θ are designed to capture the domain-generalportion of the framework.
dg c
(cid:2) Next, to imp(cid:3) lement the total information criterion, we use another feature extractor Φ : X 7→ Ro,
spu
designed to capture the domain-specific information in X that is not captured by Φ . Together, we have
dg
Φ = Φ ⊕Φ where Φ has domain-specific predictors θ : Rm+o 7→ Y for each training domain, allowing
dg spu e
the feature extractor to utilize domain-specific information to learn distinct optimal domain-specific (non-
general) predictors:
L =Re θ ◦Φ .
Φ e
L
Φ
aims to ensure that Φ
dg
and Φ
spu
capture all of(cid:0)the in(cid:1)formation about Y in X – total information
criterion. Since we do not know o,m, we select them to be the same size on our experiments; o,m could be
treated as hyperparameters though we do not treat them as such.
Finally, we implement the TCRI property (Definition 4.4). We denote L to be a conditional
TCRI
independence penalty for Φ and Φ . We utilize the Hilbert Schmidt independence Criterion (HSIC)
dg spu
[Gretton et al., 2007] as L . However, in principle, any conditional independence penalty can be used
TCRI
in its place. HSIC:
L (Φ ,Φ )= 1 H\ SIC Φ (X),Φ (X) y=k = 1 1 tr K H K H y=k ,
TCRI dg spu 2 dg spu 2 n2 Φdg nk Φspu nk
k∈X{0,1} (cid:16) (cid:17) k∈X{0,1} k (cid:16) (cid:17)
7wherek,indicateswhichclasstheexamplesintheestimatecorrespondto,C isthenumberofclasses,K ∈
Φdg
Rnk×nk, K
Φspu
∈Rnk×nk are Gram matrices, Ki Φ,j = κ(Φ dg(X) i,Φ dg(X) j), Ki Φ,j
spu
= ω(Φ spu(X) i,Φ spu(X) j)
with kernels κ,ω are radial basis functions, H = I − 1 11⊤ is a centering matrix, I is the n ×n
nk nk n2 nk k k
k
dimensional identity matrix, 1 is the n -dimensional vector whose elements are all 1, and ⊤ denotes the
nk k
transpose. We condition on the label by taking only examples of each label and computing the empirical
HSIC; then, we take the average.
Taken together, the full objective to be minimized is as follows:
1
L= Re(θ ◦Φ )+Re(θ ◦Φ)+βL (Φ ,Φ ) ,
c dg e TCRI dg spu
E
tr " #
e X∈Etr
where β > 0 is a hyperparameter and E is the number of training domains. Figure 3 shows the full
tr
framework. We note that when β =0, this loss reduces to ERM.
Note that while we minimize this objective with respect to Φ,θ ,θ ,...,θ , only the domain-general
c 1 Etr
representation and its predictor, θ ·Φ are used for inference.
c dg
5 Experiments
We begin by evaluating with simulated data, i.e., with known ground truth mechanisms; we use Equation 5
to generate our simulated data, with domain parameter σ ; code is provided in the supplemental materials.
ei
Table 2: Continuous Simulated Results – Feature Extractor
with a dummy predictor θ = 1., i.e., y = x·Φ ·w, where
c dg
x ∈ RN×2, Φ ,Φ ∈ R2×1, w ∈ R. Oracle indicates the
dg spu
coefficients achieved by regressing y on z directly.
Z(ei) ∼N 0,σ2 bc
dg ei
SCM(e i):= y(ei) =Z d(e g(cid:0)i)+N
(cid:1)
0,σ y2 , Algorithm (Φ dg)0 (Φ dg)1
Z s( pe ui) =Y(ei)+N (cid:0)0,σ e2
(cid:1)i
.
ERM
(i.e., Z 0dg .2w 9eight) (i.e., Z 0sp .7u 1weight)
(5)
 (cid:0) (cid:1) IRM 0.28 0.71
TCRI 1.01 0.06
Oracle 1.04 0.00
We observe 2 domains with parameters σ =0.1, σe=1 =0.2 with σ =0.25, 5000 samples, and linear
e=0 y
featureextractorsandpredictors. WeusepartialcovarianceasourconditionalindependencepenaltyL .
TCRI
Table 2 showsthe learnedvalue of Φ , where ‘Oracle’indicates the true coefficients obtainedby regressing
dg
Y on domain-general Z directly. The ideal Φ recovers Z and puts zero weight on Z .
dg dg dg spu
Now, we evaluate the efficacy of our proposed objective on non-simulated datasets.
5.1 Semisynthetic and Real-World Datasets
Algorithms: WecompareourmethodtobaselinescorrespondingtoDAGproperties: EmpiricalRiskMini-
mization(ERM,[Vapnik,1991]),InvariantRiskMinimization(IRM[Arjovsky et al.,2019]),VarianceRisk
Extrapolation(V-REx,[Krueger et al.,2021]), [Li et al.,2018a]), GroupDistributionallyRobustOptimiza-
tion (GroupDRO), [Sagawa et al., 2019]), and Information Bottleneck methods (IB_ERM/IB_IRM,
[Ahuja et al., 2021]). Additional baseline methods are provided in the Appendix A.
We evaluate our proposed method on the semisynthetic ColoredMNIST [Arjovsky et al., 2019] and real-
worldTerraIncognitadataset[Beery et al.,2018]. GivenobserveddomainsE ={e:1,2,...,E },wetrain
tr tr
on E \e and evaluate the model on the unseen domain e , for each e∈E .
tr i i tr
ColoredMNIST: The ColoredMNIST dataset[Arjovsky et al., 2019] is composedof 7000(2×28×28,1)
images of a hand-written digit and binary-label pairs. There are three domains with different correlations
between image color and label, i.e., the image color is spuriously related to the label by assigning a color to
8eachofthetwoclasses(0: digits0-4,1: digits5-9). Thecoloristhenflippedwithprobabilities{0.1,0.2,0.9}
to create three domains, making the color-label relationship domain-specific because it changes across do-
mains. There is also label flip noise of 0.25, so we expect that the best accuracy a domain-general model
can achieve is 75%, while a non-domain general model can achieve higher. In this dataset, Z corresponds
dg
to the original image, Z the color, e the label-color correlation, Y the image label, and X the observed
spu
colored image. This DAG follows the generative process of Figure 2a [Arjovsky et al., 2019].
Spurrious PACS: Variables. X: images, Y: non-urban (elephant, giraffe, horse) vs. urban (dog, guitar,
house, person). Domains. {{cartoon,art painting}, {art painting, cartoon},{photo}} [Li et al., 2017]. The
photo domainis the same as inthe originaldataset. In the {cartoon,artpainting}domain, urbanexamples
are selected from the original cartoon domain, while non-urban examples are selected from the original art
painting domain. In the {art painting, cartoon} domain, urban examples are selected from the original art
painting domain, while non-urban examples are selected from the original cartoon domain. This sampling
encouragesthemodeltousespuriouscorrelations(domain-relatedinformation)topredictthelabels;however,
since these relationships are flipped between domains {{cartoon, art painting} and {art painting, cartoon},
these predictions will be wrong when generalized to other domains.
Terra Incognita: The Terra Incognita dataset contains subsets of the Caltech Camera Traps dataset
[Beery et al., 2018] defined by [Gulrajani and Lopez-Paz, 2020]. There are four domains representing differ-
ent locations {L100, L38, L43, L46} of cameras in the American Southwest. There are 9 species of wild
animals {bird, bobcat, cat, coyote, dog, empty, opossum, rabbit, raccoon, squirrel} and a ‘no-animal’ class
to be predicted. Like Ahuja et al. [2021], we classify this dataset as following the generative process in
Figure 2c, the Fully InformativeInvariantFeatures (FIIF) setting. Additional details onmodel architecture,
training, and hyperparameters are detailed in Appendix 5.
Model Selection. The standard approach for model selection is a training-domain hold-out validation
set accuracy. We find that model selection across hyperparameters using this held-out training domain
validation accuracy often returns non-domain-general models in the ‘hard’ cases. One advantage of our
modelisthatwecandomodelselectionbasedontheTCRIcondition(conditionalindependencebetweenthe
tworepresentations)onheld-outtrainingdomainvalidationexamplesto mitigatethis challenge. Inthe easy
case,weexpectthe empiricalrisk minimizerto be domain-general,so selectingthe best-performingtraining-
domain model is sound – we additionally do this for all baselines (see Appendix A.1 for further discussion).
We find that, empirically, this heuristic works in the examples we study in this work. Nevertheless, model
selection under distribution shift remains a significant bottleneck for domain generalization.
5.2 Results and Discussion
Table 3: E\e → e (model selection on held-out source domains validation set). The ‘mean’ column
test test
indicates the averagegeneralizationaccuracyoverall three domains asthe e distinctly; the ‘min’ column
test
indicates the worst generalizationaccuracy.
ColoredMNIST Spurious PACS Terra Incognita
Algorithm average worst-case average worst-case average worst-case
ERM 51.6 ± 0.1 10.0 ± 0.1 57.2 ± 0.7 31.2 ± 1.3 44.2 ± 1.8 35.1 ± 2.8
IRM 51.7 ± 0.1 9.9 ± 0.1 54.7 ± 0.8 30.3 ± 0.3 38.9 ± 3.7 32.6 ± 4.7
GroupDRO 52.0 ± 0.1 9.9 ± 0.1 58.5 ± 0.4 37.7 ± 0.7 47.8 ± 0.9 39.9 ± 0.7
VREx 51.7 ± 0.2 10.2 ± 0.0 58.8 ± 0.4 37.5 ± 1.1 45.1 ± 0.4 38.1 ± 1.3
IB_ERM 51.5 ± 0.2 10.0 ± 0.1 56.3 ± 1.1 35.5 ± 0.4 46.0 ± 1.4 39.3 ± 1.1
IB_IRM 51.7 ± 0.0 9.9 ± 0.0 55.9 ± 1.2 33.8 ± 2.2 37.0 ± 2.8 29.6 ± 4.1
TCRI_HSIC 59.6 ± 1.8 45.1 ± 6.7 63.4 ± 0.2 62.3 ± 0.2 49.2 ± 0.3 40.4 ± 1.6
9Table4: TotalInformationCriterion: DomainGeneral(DG)andDomainSpecific(DS)Accuracies. TheDG
classifier is shared across all training domains, and the DS classifiers are trained on each domain. The first
row indicates the domain from which the held-out examples are sampled, and the second indicates which
domain-specific predictor is used. {+90%, +80%, -90%} indicate domains – {0.1,0.2,0.9} digit label and
color correlation, respectively.
DG Classifier DS Classifier on +90 DS Classifier on +80 DS Classifier on -90
Test Domain +90% +80% -90% +90% +80% -90% +90% +80% -90% +90% +80% -90%
NoDSclf.
+90% 68.7 69.0 68.5 - 90.1 9.8 - 79.9 20.1 - 10.4 89.9
+80% 63.1 62.4 64.4 76.3 - 24.3 70.0 - 30.4 24.5 - 76.3
-90% 65.6 63.4 44.1 75.3 75.3 - 69.2 69.5 - 29.3 26.0 -
Table 5: TIC ablation for ColoredMNIST.
Algorithm average worst-case
TCRI_HSIC (No TIC) 51.8 ± 5.9 27.7 ± 8.9
TCRI_HSIC 59.6 ± 1.8 45.1 ± 6.7
Worst-domain Accuracy. A critical implication of domain generality is stability – robustness in worst-
domain performance up to domain difficulty. While average accuracy across domains provides some insight
into an algorithm’s ability to generalize to new domains, the average hides the variance of performance
across domains. Average improvement can be increased while the worst-domainaccuracy stays the same or
decreases,leadingtoincorrectconclusionsaboutdomaingeneralization. Additionally,inreal-worldchallenges
such as algorithmic fairness where worst-group performance is considered, some metrics or fairness are
analogous to achieving domain generalization [Creager et al., 2021].
Results. TCRI achieves the highest average and worst-case accuracy across all baselines (Table 3). We
find no method recovers the exact domain-general model’s accuracy of 75%. However, TCRI achieves over
7% increase in both average accuracy and worst-case accuracy. Appendix A.2 shows transfer accuracies
with cross-validation on held-out test domain examples (oracle) and TCRI again outperforms all baselines,
achieving an average accuracy of 70.0% ± 0.4% and a worst-case accuracy of 65.7% ± 1.5, showing that
regularizing for TCRI gives very close to optimal domain-general solutions.
Similarly, for the Spurious-PACS dataset, we observe that TCRI outperforms the baselines. TRCI
achieves the highest average accuracy of 63.4% ± 0.2 and worst-case accuracy of 62.3% ± 0.1 with the
next best, VREx, achieving 58.8±1.0 and 33.8±0.0, respectively. Additionally, for the Terra-Incognita
dataset, TCRI achieves the highest average and worst-case accuracies of 49.2% ± 0.3% and 40.4% ± 1.6%
with the next best, GroupDRO, achieving 47.8±0.9 and 39.9±0.7, respectively.
Appendix A.2 shows transfer accuracies with cross-validation held-out target domain examples (oracle)
where we observe that TCRI also obtains the highest average and worst-case accuracy for Spurrious-PACS
and Terra Incognita.
Overall,regularizingforTCRIgivesthemostdomain-generalsolutionscomparedtoourbaselines,achiev-
ing the highest worst-case accuracy on all benchmarks. Additionally, TCRI achieves the highest average
accuracyonColoredMNIST and Spurious-PAC andthe secondhighest onTerraIncognita, where we expect
the empirical risk minimizer to be domain-general.
Additional results are provided in the Appendix A.
The Effect of the Total Information Criterion. Without the TIC loss term, our proposed method is
less effective. Table 5 shows that for Colored MNIST, the hardest ‘hard’ case we encounter, removing the
TIC criteria, performs worse in averageand worst case accuracy,dropping over 8% and 18, respectively.
Separation of Domain General and Domain Specific Features . Inthe case ofColoredMNIST,we
can reason about the extent of feature disentanglement from the accuracies achieved by the domain-general
and domain-specific predictors. Table 4 shows how much each component of Φ, Φ and Φ , behaves as
dg spu
10expected. Foreachdomain,weobservethatthedomain-specificpredictors’accuraciesfollowthe sametrend
as the color-label correlation, indicating that they capture the color-label relationship. The domain-general
predictor, however,does not follow such a trend, indicating that it is not using color as the predictor.
For example, when evaluating the domain-specific predictors from the +90% test domain experiment
(row +90%) on held-out examples from the +80% training domain (column "DS Classifier on +80%"), we
find that the +80% domain-specific predictor achieves an accuracy of nearly 79.9% – exactly what one
would expect from a predictor that uses a color correlation with the same direction ‘+’. Conversely, the
-90% predictor achieves an accuracy of 20.1%, exactly what one would expect from a predictor that uses a
color correlationwith the opposite direction ‘-’. The -90% domain has the opposite label-color pairing, so a
color-basedclassifier will give the opposite label in any ‘+’ domain.
Another advantage of this method, exemplified by Table 4, is that if one believes a particular domain is
close to one of the training domains, one can opt to use the close domain’s domain-specific predictor and
leverage spurious information to improve performance.
On Benchmarking Domain Generalization. Previous work on benchmarking domain generalization
showed that across standard benchmarks, the domain-unaware empirical risk minimizer outperforms or
achievesequivalentperformancetothestate-of-the-artdomaingeneralizationmethods[Gulrajani and Lopez-Paz,
2020]. Additionally,Rosenfeld et al.[2022]givesresultsthatshowweakconditionsthatdefineregimeswhere
the empirical risk minimizer across domains is optimal in both average and worst-case accuracy. Conse-
quently, to accurately evaluate our work and baselines, we focus on settings where it is clear that (i) the
empirical risk minimizer fails, (ii) spurious features, as we have defined them, do not generalize across the
observeddomains,and(iii)thereisroomforimprovementviabetterdomain-generalpredictions. Wediscuss
this point further in the Appendix A.1.
Oracle Transfer Accuracies. While model selection is an integral part of the machine learning develop-
ment cycle, it remains a non-trivial challenge when there is a distribution shift. While we have proposed a
selection process tailored to our method that can be generalized to other methods with an assumed causal
graph,weacknowledgethatmodelselectionunderdistributionshiftisstillanimportantopenproblem. Con-
sequently, we disentangle this challenge from the learning problem and evaluate an algorithm’s capacity to
give domain-generalsolutions independently of model selection. We report experimental reports using held-
out test-set examples for model selection in Appendix A Table 6. We find that our method, TCRI_HSIC,
also outperforms baselines in this setting.
6 Conclusion and Future Work
We reduce the gap in learning domain-generalpredictors by leveraging conditional independence properties
implied by generative processes to identify domain-general mechanisms. We do this without independent
observations of domain-general and spurious mechanisms and show that our framework outperforms other
state-of-the-art domain-generalization algorithms on real-world datasets in average and worst-case across
domains. Future work includes further improvements to the framework to fully recover the strict set of
domain-generalmechanisms and model selection strategies that preserve desired domain-generalproperties.
Acknowledgements
OS was partially supported by the UIUC Beckman Institute Graduate Research Fellowship, NSF-NRT
1735252. This work is partially supported by the NSF III 2046795, IIS 1909577, CCF 1934986, NIH
1R01MH116226-01A,NIFA award2020-67021-32799,the Alfred P. Sloan Foundation, and Google Inc.
References
KartikAhuja,KarthikeyanShanmugam,KushVarshney,andAmitDhurandhar.Invariantriskminimization
games. In International Conference on Machine Learning, pages 145–155.PMLR, 2020.
11Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioan-
nis Mitliagkas, and Irina Rish. Invariance principle meets information bottleneck for out-of-distribution
generalization. Advances in Neural Information Processing Systems, 34:3438–3450,2021.
Martín Arjovsky, L. Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. ArXiv,
abs/1907.02893,2019.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the
European conference on computer vision (ECCV), pages 456–473,2018.
ShaiBen-David,JohnBlitzer,K.Crammer,A.Kulesza,FernandoCPereira,andJenniferWortmanVaughan.
A theory of learning from different domains. Machine Learning, 79:151–175,2009.
SteffenBickel,MichaelBrückner,andTobiasScheffer. Discriminativelearningundercovariateshift. Journal
of Machine Learning Research, 10(9), 2009.
Gilles Blanchard,Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and ClaytonScott. Domain general-
ization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017.
Xiangli Chen, Mathew Monfort, Anqi Liu, and Brian D Ziebart. Robust covariate shift regression. In
Artificial Intelligence and Statistics, pages 1270–1279.PMLR, 2016.
Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. Advances in Neural Information Processing Systems, 30, 2017.
Elliot Creager, Jörn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In
International Conference on Machine Learning, pages 2189–2200.PMLR, 2021.
Rémi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J. Gordon. Domain adaptation with
conditional distribution matching and generalized label shift. ArXiv, abs/2003.04475,2020.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarialtraining of neural networks. The journal of
machine learning research, 17(1):2096–2030,2016.
A. Gretton, K. Fukumizu, C. Teo, Le Song, B. Schölkopf, and Alex Smola. A kernel statistical test of
independence. In NIPS, 2007.
Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard
Schölkopf. Covariate shift by kernel mean matching. Dataset shift in machine learning, 3(4):5, 2009.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. CoRR, abs/2007.01434,
2020. URL https://arxiv.org/abs/2007.01434.
KaimingHe, XiangyuZhang,Shaoqing Ren, andJianSun. Deep residuallearning forimage recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778,2016.
Christopher Hitchcock and Miklós Rédei. Reichenbach’s Common Cause Principle. In Edward N. Zalta,
editor,The Stanford Encyclopedia of Philosophy. MetaphysicsResearchLab,StanfordUniversity,Summer
2021 edition, 2021.
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Schölkopf, and Alex Smola. Correcting
sample selection bias by unlabeled data. Advances in Neural Information Processing Systems, 19, 2006.
Jivat Neet Kaur, Emre Kiciman, and Amit Sharma. Modeling the data-generating process is necessary for
out-of-distribution generalization. arXiv preprint arXiv:2206.07837, 2022.
Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for
robustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022.
12Samory Kpotufe and Guillaume Martinet. Marginal singularity, and the benefits of labels in covariate-shift.
In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference
On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1882–1886. PMLR,
06–09 Jul 2018. URL https://proceedings.mlr.press/v75/kpotufe18a.html.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,
Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In
International Conference on Machine Learning, pages 5815–5826.PMLR, 2021.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain gen-
eralization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550,
2017.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for
domain generalization. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018a.
HaoliangLi,SinnoJialinPan,ShiqiWang,andAlexC.Kot. Domaingeneralizationwithadversarialfeature
learning. In2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages5400–5409,
2018b. doi: 10.1109/CVPR.2018.00566.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and D. Tao. Deep domain
generalization via conditional invariant adversarialnetworks. In ECCV, 2018c.
ZacharyChaseLipton, Yu-Xiang Wang,andAlex Smola. Detecting andcorrectingfor labelshift withblack
box predictors. ArXiv, abs/1802.03916,2018.
EvanZLiu,BehzadHaghgoo,AnnieSChen,AditiRaghunathan,PangWeiKoh,ShioriSagawa,PercyLiang,
and Chelsea Finn. Just train twice: Improving group robustness without training group information. In
International Conference on Machine Learning, pages 6781–6792.PMLR, 2021.
Mingsheng Long,Yue Cao, Jianmin Wang, and Michael I. Jordan. Learning transferablefeatures with deep
adaptation networks. ArXiv, abs/1502.02791,2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with
residual transfer networks. Advances in neural information processing systems, 29, 2016.
MaggieMakar,BenPacker,DanMoldovan,DavisBlalock,YoniHalpern,andAlexanderD’Amour. Causally
motivatedshortcutremovalusingauxiliarylabels.InGustauCamps-Valls,FranciscoJ.R.Ruiz,andIsabel
Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,
volume 151 of Proceedings of Machine Learning Research, pages 739–766. PMLR, 28–30 Mar 2022. URL
https://proceedings.mlr.press/v151/makar22a.html.
Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant feature
representation. In International conference on machine learning, pages 10–18.PMLR, 2013.
J. Pearl. Causal inference. In NIPS Causality: Objectives and Assessment, 2010.
Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction:
identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical
Methodology), pages 947–1012,2016.
MiklósRédei. Reichenbach’s Common Cause Principle and Quantum Correlations, pages259–270. Springer
Netherlands, Dordrecht, 2002. ISBN 978-94-010-0385-8. doi: 10.1007/978-94-010-0385-8_17. URL
https://doi.org/10.1007/978-94-010-0385-8_17.
Alexander Robey, George J Pappas, and Hamed Hassani. Model-based domain generalization. Advances in
Neural Information Processing Systems, 34:20210–20229,2021.
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. arXiv
preprint arXiv:2010.05761, 2020.
13Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation
and extrapolation in domain generalization. In International Conference on Artificial Intelligence and
Statistics, pages 2641–2657.PMLR, 2022.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural
networksforgroupshifts: Ontheimportanceofregularizationforworst-casegeneralization.arXivpreprint
arXiv:1911.08731, 2019.
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge.
Improving robustness against common corruptions by covariate shift adaptation. Advances in Neural
Information Processing Systems, 33:11539–11551,2020.
JessicaSchrouff, Natalie Harris,OluwasanmiKoyejo,IbrahimAlabdulmohsin, Eva Schnider, KristaOpsahl-
Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Maintaining fairness across distri-
bution shift: do we have viable solutions for real-world applications? arXiv preprint arXiv:2202.01034,
2022.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of statistical planning and inference, 90(2):227–244,2000.
Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Buenau, and Motoaki Kawanabe. Direct
importance estimationwith modelselectionandits applicationto covariateshift adaptation. Advances in
Neural Information Processing Systems, 20, 2007.
Vladimir Vapnik. Principles of risk minimization for learning theory. In NIPS, volume 91, pages 831–840,
1991.
Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invariance to
spurious correlations: Why and how to pass stress tests. arXiv preprint arXiv:2106.00545, 2021.
Haoxiang Wang, Haozhe Si, Bo Li, and Han Zhao. Provable domain generalization via invariant-feature
subspace recovery. In ICML, 2022.
Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In Proceedings of the
twenty-first international conference on Machine learning, page 114, 2004.
MarvinZhang,HenrikMarklund,NikitaDhawan,AbhishekGupta,SergeyLevine,andChelseaFinn. Adap-
tive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing
Systems, 34, 2021.
H. Zhao, Rémi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On learning invariant representa-
tions for domain adaptation. In ICML, 2019.
14A Additional Results and Discussion
A.1 On Benchmarking Domain Generalization
Table 6: Oracle (model selection on held-out target domain validation set) E\e → e . The ‘mean’
test test
column indicates the averagegeneralizationaccuracy overall three domains as the e distinctly; the ‘min’
test
column indicates the worst generalization accuracy.
ColoredMNIST Spurious PACS Terra Incognita
Algorithm average worst-case average worst-case average worst-case
ERM 57.8 ± 0.2 38.4 ± 1.4 59.2 ± 1.3 38.4 ± 1.4 52.9 ± 0.8 42.0 ± 0.6
IRM 68.9 ± 1.6 62.0 ± 4.9 67.5 ± 5.8 53.9 ± 6.6 42.6 ± 4.0 42.7 ± 1.2
GroupDRO 61.1 ± 1.3 37.6 ± 3.6 61.8 ± 1.8 40.0 ± 1.6 50.7 ± 1.0 42.7 ± 1.2
VREx 68.0 ± 2.5 59.4 ± 7.3 62.8 ± 2.4 38.7 ± 0.9 43.2 ± 2.0 34.9 ± 4.2
IB_ERM 65.0 ± 0.1 50.6 ± 0.3 67.3 ± 3.7 53.1 ± 8.0 49.0 ± 0.3 39.9 ± 0.8
IB_IRM 68.4 ± 1.0 58.5 ± 2.8 69.0 ± 1.3 62.3 ± 0.3 32.8 ± 6.6 20.4 ± 7.5
TCRI_HSIC 70.4 ± 0.4 65.7 ± 1.5 69.5 ± 1.1 62.3 ± 0.2 51.2 ± 0.1 43.0 ± 0.4
Oracle Transfer Accuracies. While model selection is an integral part of the machine learning develop-
ment cycle, it remains a non-trivial challenge when there is a distribution shift. While we have proposed a
selection process tailored to our method that can be generalized to other methods with an assumed causal
graph, we acknowledge that model selection under distribution shift is still an important open problem.
Consequently, we disentangle this challenge from the learning problem and evaluate an algorithm’scapacity
to give domain-general solutions independently of model selection. We report experimental reports using
held-out test-set examples for model selection in Appendix A Table 6.
In this case, we find that there is indeed a separation between ERM and some domain-generalization
algorithms, suggesting that model selection might be a substantial bottleneck for learning domain-general
predictors. Nevertheless, we still find that our method, TCRI_HSIC, also outperforms baselines in this
setting.
Challenges of Benchmarking Domaing Generalization. We show some results below that illustrate
the challenge of accurately evaluating the efficacy of an algorithm for domain generalization. We first note
thatweexpectERM(naive)toperformpoorlyindomaingeneralizationtasks,certainlysowhenweobserve
worst-case shifts at test time. However, like other works [Gulrajani and Lopez-Paz, 2020], we observe that
ERMperformsaswellasotherbaselinesduringtransferonvariousbenchmarkdatasets. Previoustheoretical
results [Rosenfeld et al., 2022] suggest that this observation is indicative of properties of the benchmark
domains that may be sufficient for ERM to give domain-generalsolutions - specifically that the distribution
(and equivalently the loss) of the target domain can be written as a convex combination of the those in the
source domains.
Tofurtherinvestigatethis,wedevelopadditionalexperimentsmotivatedbytheColoredMNIST[Arjovsky et al.,
2019]–sinceitsgenerativeprocessiswellunderstood. Wenotethatinthe+90%,+80%,and-90%domains
of ColoredMNIST, the -90% domain has the opposite relationship between the spurious correlationand the
label, so the use of spurious correlations from {+90%, +80%} generalizes catastrophically to the -90% do-
main. Inthis setting,thebaselinealgorithmswepresent,includingERM,achievepooraccuracyinthe -90%
domainwhilemaintaininghighaccuracyinthe+90%and+80%domains. Consequently,weinvestigatetwo
settings, setting a: observe {+90%, +80%, +70%, -90%} domains and setting b: observe {+90%, +80%,
-80%, -90%} domains – we focus on generalizing to the -90% domain. In setting a, we add another domain
with the majority direction in the relationship between spurious correlation and labels. In setting b, we
add another domain with the minority direction. Note that in setting a, the closest domain to -90% that
can be generated with a convex combination of the other domains still has a ‘+’ correlation between the
color and label. In setting b, however, one can generate a domain with a ‘-’ correlation between color and
label with a convexcombinationof the other domains. Thus, we expect the empiricalrisk minimizer to give
domain-generalsolutions in setting b but not in setting a.
15We use Oracle model selection (held-out target data) to remove the effect of model selection for all
methods in the results. We find that in setting a, where we add a domain (+70%), we observe that the
generalization accuracy to the -90% domain is still very different from the other domains (Table 7).
Table 7: ColoredMNIST setting a. Columns {+90%, +80%, +70%, -90%} indicate domains –
{0.1,0.2,0.3,0.9} digit label and color correlation, respectively. We report domain accuracies over 3 tri-
als each. We use the oracle selection method – held out target data. E\e →e .
test test
Algorithm +90% +80% +70% -90%
ERM 72.8 ± 0.3 74.7 ± 0.3 73.3 ± 0.1 16.3 ± 1.5
IRM 49.0 ± 0.1 54.2 ± 2.0 50.3 ± 0.3 43.8 ± 2.8
GroupDRO 71.0 ± 0.6 72.2 ± 0.3 70.7 ± 0.9 36.4 ± 4.2
VREx 74.1 ± 1.3 72.6 ± 0.5 72.1 ± 0.5 19.5 ± 5.5
TCRI_HSIC 72.1 ± 1.5 73.6 ± 0.4 72.6 ± 0.4 49.9 ± 0.3
However, in setting b, where we add a domain (-80%), we observe that the generalization accuracy to
the -90% domain is on par with the other domains (Table 8).
Table 8: ColoredMNIST setting b. Columns {+90%, +80%, -80%, -90%} indicate domains –
{0.1,0.2,0.8,0.9} digit label and color correlation, respectively. We report the average domain accuracies
over 3 trials each. We use the oracle selection method – held out target data. E\e →e .
test test
Algorithm +90% +80% -80% -90%
ERM 58.4 ± 1.3 67.0 ± 0.5 64.2 ± 2.0 52.6 ± 3.2
IRM 56.7 ± 3.3 56.6 ± 2.8 51.6 ± 0.7 51.7 ± 0.7
GroupDRO 69.7 ± 0.8 71.7 ± 0.3 72.0 ± 0.2 71.4 ± 1.9
VREx 67.4 ± 1.9 70.4 ± 0.1 71.2 ± 0.2 59.4 ± 4.3
TCRI_HSIC 62.2 ± 4.4 70.0 ± 1.3 67.9 ± 1.4 65.4 ± 2.8
This illustrates the challenge of accurately evaluating an algorithm’s ability to give domain-general pre-
dictions. We note that it is generally difficult to distinguish between setting a and setting b. The pri-
mary signature we see is some consistency between the empirical risk minimizer and the other baselines.
Gulrajani and Lopez-Paz [2020] observe a similar trend for standard benchmarks for domain generalization.
Hence, we focus our empirical evaluations in this work on settings where we know that the ERM solution
fails by design.
A.2 ColoredMNIST
ColoredMNIST:TheColoredMNISTdataset[Arjovsky et al.,2019]iscomposedof7000(2×28×28,1)images
of a hand-written digit and binary-label pairs. There are three domains with different correlations between
imagecolorandlabel,i.e.,theimagecolorisspuriouslyrelatedtothelabelbyassigningacolortoeachofthe
two classes (0: digits 0-4, 1: digits 5-9). The color is then flipped with probabilities {0.1,0.2,0.9} to create
threedomains,makingthecolor-labelrelationshipdomain-specificbecauseitchangesacrossdomains. There
is also label flip noise of 0.25, so we expect that the best accuracy a domain-general model can achieve is
75%, while a non-domain generalmodel can achieve higher. In this dataset, Z corresponds to the original
dg
image, Z the color, e the label-color correlation, Y the image label, and X the observed colored image.
spu
This DAG follows the generative process of Figure 2a
WeuseMNIST-ConvNet[Gulrajani and Lopez-Paz,2020]backbonesfortheMNISTdatasets(Table10).
Both Φ and Φ are linear layers of size 128×128 that are appended to the backbone. The predictors
dg spu
(classification hyperplanes) θ , {θ , θ } are also parameterized to be linear and appended to the Φ and Φ,
c 1 2 dg
respectively.
We do a random search to select hyperparameters using the same scheme as Gulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 25 hyperparameters with 5 random
restarts each to generate error bars.
16Table 9: ColoredMNIST Hyperparameters. Additional hyperparameters are provided in
https://github.com/olawalesalaudeen/tcri.
Algorithm Hyperparameter Default Random Distribution
Learning Rate 1−3 10Uniform(−4.5,−2.5)
All
Batch Size 64 2Uniform(3,9)
penalty weight 100 10Uniform(−1,5)
TCRI β
annealing steps 500 10Uniform(2.5,5)
Table 10: MNIST ConvNet architecture. All convolutions use 3×3 kernels and "same" padding.
# Layer
1 Conv2D (in=d, out=64)
2 ReLU
3 GroupNorm (groups=8)
4 Conv2D (in=64, out=128, stride=2)
5 ReLU
6 GroupNorm (groups=8)
7 Conv2D (in=128, out=128)
8 ReLU
9 GroupNorm (groups=8)
10 Conv2D (in=128, out=128)
11 ReLU
12 GroupNorm (8 groups)
13 Global average-pooling
We showtransferaccuracieswithbothsourceandtargetdomainvalidationformodelselectionin Tables
11-12. We find that TCRI outperforms all baselines in average and worst-case accuracy.
Table 11: ColoredMNIST Transfer Accuracy – model selection on held-out source validation set. Columns
{+90%, +80%, -90%} indicate domains – {0.1,0.2,0.9} digit label and color correlation, respectively.
E\e →e .
test test
Domains Domain Accuracy Statistics
Algorithm +90% +80% -90% Avg Std Min
ERM 71.6 ± 0.3 73.1 ± 0.1 10.0 ± 0.1 51.6 ± 0.1 29.4 ± 0.1 10.0 ± 0.1
IRM 72.1 ± 0.1 73.0 ± 0.3 9.9 ± 0.1 51.7 ± 0.1 29.5 ± 0.1 9.9 ± 0.1
GroupDRO 72.6 ± 0.2 73.4 ± 0.2 9.9 ± 0.1 52.0 ± 0.1 29.8 ± 0.1 9.9 ± 0.1
VREx 72.2 ± 0.2 72.7 ± 0.3 10.2 ± 0.0 51.7 ± 0.2 29.3 ± 0.1 10.2 ± 0.0
IB_ERM 71.0 ± 0.4 73.4 ± 0.3 10.0 ± 0.1 51.5 ± 0.2 29.4 ± 0.1 10.0 ± 0.1
IB_IRM 71.7 ± 0.2 73.4 ± 0.1 9.9 ± 0.0 51.7 ± 0.0 29.5 ± 0.0 9.9 ± 0.0
TCRI_HSIC 67.2 ± 2.3 65.6 ± 3.4 45.9 ± 6.9 59.6 ± 1.8 11.4 ± 3.3 45.1 ± 6.7
A.3 Spurrious PACS
Spurious–PACS. Variables. X: images, Y: non-urban (elephant, giraffe, horse) vs. urban (dog, guitar,
house, person). Domains. {{cartoon,art painting}, {art painting, cartoon},{photo}} [Li et al., 2017]. The
photo domainis the same as inthe originaldataset. In the {cartoon,artpainting}domain, urbanexamples
are selected from the original cartoon domain, while non-urban examples are selected from the original art
painting domain. In the {art painting, cartoon} domain, urban examples are selected from the original art
painting domain, while non-urban examples are selected from the original cartoon domain. This sampling
encouragesthemodeltousespuriouscorrelations(domain-relatedinformation)topredictthelabels;however,
since these relationships are flipped between domains {{cartoon, art painting} and {art painting, cartoon},
17Table 12: Oracle ColoredMNIST Transfer Accuracy – model selection on held-out target validation set
accuracy. Columns {+90%,+80%, -90%} indicate domains – {0.1,0.2,0.9}digit label and color correlation,
respectively. E\e →e .
test test
ColoredMNIST Spurious PACS Terra Incognita
Algorithm average worst-case average worst-case average worst-case
ERM 57.8 ± 0.2 38.4 ± 1.4 59.2 ± 1.3 38.4 ± 1.4 52.9 ± 0.8 42.0 ± 0.6
IRM 68.9 ± 1.6 62.0 ± 4.9 67.5 ± 5.8 53.9 ± 6.6 42.6 ± 4.0 42.7 ± 1.2
GroupDRO 61.1 ± 1.3 37.6 ± 3.6 61.8 ± 1.8 40.0 ± 1.6 50.7 ± 1.0 42.7 ± 1.2
VREx 68.0 ± 2.5 59.4 ± 7.3 62.8 ± 2.4 38.7 ± 0.9 43.2 ± 2.0 34.9 ± 4.2
IB_ERM 65.0 ± 0.1 50.6 ± 0.3 67.3 ± 3.7 53.1 ± 8.0 49.0 ± 0.3 39.9 ± 0.8
IB_IRM 68.4 ± 1.0 58.5 ± 2.8 69.0 ± 1.3 62.3 ± 0.3 32.8 ± 6.6 20.4 ± 7.5
TCRI_HSIC 70.4 ± 0.4 65.7 ± 1.5 69.5 ± 1.1 62.3 ± 0.2 51.2 ± 0.1 43.0 ± 0.4
these predictions will be wrong when generalized to other domains.
Table 13: Spurrious PACS Hyperparameters. Additional hyperparameters provided in
https://github.com/olawalesalaudeen/tcri.
Algorithm Hyperparameter Default Range
Learning Rate 1−3 10Uniform(−4.5,−2.5)
All
Batch Size 64 2Uniform(3,9)
penalty weight 100 10Uniform(−1,5)
TCRI β
annealing steps 500 10Uniform(2.5,5)
We use a ResNet-50 backbone [He et al., 2016]. Φ and Φ are linear layers of size 2048×2048 that
dg spu
are appended to the backbone. The predictors (classification hyperplanes) θ ,{θ ,θ ,θ } are linear and
c 1 2 3
appended to Φ and Φ layers,respectively.
dg
Hyperparameters: WedoarandomsearchtoselecthyperparametersusingthesameschemeasGulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 5 hyperparameters with 3 random
restarts each to generate error bars.
We showtransferaccuracieswithbothsourceandtargetdomainvalidationformodelselectionin Tables
14-15. We find that TCRI outperforms all baselines in average and worst-case accuracy.
Table 14: Spurious–PACS Transfer Accuracy – model selection on held-out source validation set. E\e →
test
e .
test
Domains Domain Accuracy Statistics
Algorithm C x A A x C P mean std min
ERM 31.2 ± 1.3 42.8 ± 0.7 97.6 ± 0.2 57.2 ± 0.7 29.0 ± 0.4 31.2 ± 1.3
IRM 30.3 ± 0.3 39.0 ± 1.3 94.9 ± 1.4 54.7 ± 0.8 28.6 ± 0.8 30.3 ± 0.3
GroupDRO 37.7 ± 0.7 42.1 ± 1.6 95.7 ± 0.5 58.5 ± 0.4 26.4 ± 0.3 37.7 ± 0.
VREx 37.5 ± 1.1 43.0 ± 0.5 95.7 ± 1.5 58.8 ± 0.4 26.2 ± 1.0 37.5 ± 1.1
IB_ERM 35.5 ± 0.4 48.6 ± 3.3 84.8 ± 0.6 56.3 ± 1.1 20.8 ± 0.6 35.5 ± 0.4
IB_IRM 33.8 ± 2.2 38.8 ± 3.0 95.1 ± 1.5 55.9 ± 1.2 27.8 ± 1.5 33.8 ± 0.4
TCRI_HSIC 62.8 ± 0.1 62.3 ± 0.2 65.0 ± 0.4 63.4 ± 0.2 1.2± 0.2 62.3 ± 0.2
18Table 15: Oracle Spurious–PACS Transfer Accuracy – model selection on held-out target validation set.
E\e →e .
test test
Domains Domain Accuracy Statistics
Algorithm C x A A x C P mean std min
ERM 38.4 ± 1.4 43.4 ± 1.9 95.9 ± 0.6 59.2 26.0 38.4
IRM 62.8 ± 0.1 53.9 ± 6.6 85.8 ± 8.2 67.5 13.4 53.9
GroupDRO 40.0 ± 1.6 49.7 ± 2.9 95.7 ± 0.6 61.8 24.3 40.0
VREx 55.8 ± 5.5 38.7 ± 0.9 93.8 ± 0.8 62.8 23.0 38.7
IB_ERM 53.1 ± 8.0 55.4 ± 5.7 93.5 ± 1.8 67.3 18.5 53.1
IB_IRM 62.8 ± 0.1 62.3 ± 0.3 81.8 ± 7.0 69.0 9.1 62.3
TCRI_HSIC 64.0 ± 0.7 62.3 ± 0.2 82.4 ± 5.7 69.5 9.1 62.3
A.4 Terra Incognita
TheTerraIncognitadatasetcontainssubsetsoftheCaltechCameraTrapsdataset[Beery et al.,2018]defined
by [Gulrajani and Lopez-Paz, 2020]. Four domains represent different locations {L100, L38, L43, L46} of
camerasintheAmericanSouthwest. Thereare10differentspeciesofwildanimals{bird,bobcat,cat,coyote,
dog,empty,opossum,rabbit,raccoon,squirrel}(classes)tobepredicted. LikeAhuja et al.[2021],weclassify
this datasetasfollowingthe generativeprocessinFigure 2c, the Fully InformativeInvariantFeatures (FIIF)
setting.
Table 16: Terra Incognita Hyperparameters. Additional hyperparameters provided in
https://github.com/olawalesalaudeen/tcri.
Algorithm Hyperparameter Default Range
Learning Rate 1−3 10Uniform(−4.5,−2.5)
All
Batch Size 64 2Uniform(3,9)
penalty weight 100 10Uniform(−1,5)
TCRI β
annealing steps 500 10Uniform(0,4)
We use a ResNet-50 backbone [He et al., 2016]. Φ and Φ are linear layers of size 2048×2048 that
dg spu
are appended to the backbone. The predictors (classification hyperplanes) θ ,{θ ,θ ,θ ,θ } are linear and
c 1 2 3 4
appended to Φ and Φ layers,respectively.
dg
Hyperparameters: WedoarandomsearchtoselecthyperparametersusingthesameschemeasGulrajani and Lopez-Paz
[2020] (https://github.com/facebookresearch/DomainBed). We select 5 hyperparameters with 3 random
restarts each to generate error bars.
We showtransferaccuracieswithbothsourceandtargetdomainvalidationformodelselectionin Tables
17-18. We find that TCRI outperforms all baselines except ERM on average and outperforms all baselines
in worst-case accuracy.
19Table 17: Terra Incognita Transfer Accuracy – model selection on held-out source validation set. E\e →
test
e .
test
Domains Domain Accuracy Statistics
Algorithm L100 L38 L43 L46 Avg Std Min
ERM 43.6 ± 3.9 45.2 ± 0.6 53.0 ± 1.2 35.1 ± 2.8 44.2 ± 1.8 6.8 ± 1.0 35.1 ± 2.8
IRM 43.9 ± 3.3 35.7 ± 4.0 37.7 ± 7.8 38.3 ± 2.4 38.9 ± 3.7 5.4 ± 1.8 32.6 ± 4.7
GroupDRO 53.8 ± 4.6 40.5 ± 0.7 55.3 ± 1.5 41.8 ± 1.1 47.8 ± 0.9 7.7 ± 0.9 39.9 ± 0.7
VREx 48.8 ± 2.0 38.1 ± 1.3 54.4 ± 0.6 39.0 ± 1.4 45.1 ± 0.4 7.0 ± 0.9 38.1 ± 1.3
IB_ERM 46.1 ± 4.5 40.7 ± 0.7 55.2 ± 0.8 42.2 ± 1.1 46.0 ± 1.4 6.4 ± 0.8 39.3 ± 1.1
IB_IRM 39.7 ± 7.3 40.8 ± 2.3 34.7 ± 4.3 32.9 ± 2.6 37.0 ± 2.8 6.7 ± 1.3 29.6 ± 4.1
TCRI_HSIC 54.6 ± 2.4 48.6 ± 2.0 53.2 ± 1.0 40.4 ± 1.6 49.2 ± 0.3 6.1 ± 1.1 40.4 ± 1.6
Table 18: Oracle Terra Incognita Transfer Accuracy – model selection on held-out target validation set.
E\e →e .
test test
Domains Domain Accuracy Statistics
Algorithm L100 L38 L43 L46 Avg Std Min
ERM 58.5 ± 1.8 52.0 ± 1.3 59.2 ± 0.2 42.0 ± 0.6 52.9 ± 0.8 7.0 ± 0.5 42.0 ± 0.6
IRM 53.0 ± 0.9 48.0 ± 1.8 36.3 ± 9.6 33.2 ± 3.9 42.6 ± 4.0 9.6 ± 1.7 30.8 ± 5.4
GroupDRO 56.2 ± 3.0 45.2 ± 2.3 58.0 ± 0.2 43.3 ± 0.7 50.7 ± 1.0 6.9 ± 0.9 42.7 ± 1.2
VREx 43.2 ± 1.5 49.3 ± 1.2 41.5 ± 7.8 38.9 ± 1.1 43.2 ± 2.0 6.5 ± 1.8 34.9 ± 4.2
IB_ERM 55.6 ± 1.7 47.2 ± 1.1 53.4 ± 0.7 39.9 ± 0.8 49.0 ± 0.3 6.4 ± 0.5 39.9 ± 0.8
IB_IRM 40.2 ± 8.2 31.9 ± 11.8 29.4 ± 4.4 29.7 ± 3.8 32.8 ± 6.6 8.2 ± 1.0 20.4 ± 7.5
TCRI_HSIC 57.7 ± 1.8 50.1 ± 1.8 54.1 ± 0.6 43.0 ± 0.4 51.2 ± 0.1 5.8 ± 0.7 43.0 ± 0.4
B DAGs
e Z dg Y
X Z spu
Figure 4: Partial Ancestral Graph (PAG). Dashed edges indicate that the edge may or may not exist. The
combination of Y →Z →Z , and Y →Z , e→Z is not allowed.
dg spu dg dg
B.1 On Valid DAGS:
We consider other edges that could be introduced to Figure 4 where Z 6⊥⊥ Z |Y,e, Z 6⊥⊥ Y |Z , or
dg spu spu dg
are not included in Figure 5. We then show that these edges either make the problem intractable or require
new assumptions about the generative process – note we do not discuss edges that induce a cycle, thus, are
invalid.
(i) e−Y: we cannot have a direct edge in either direction e between Y otherwise, Y is always dependent
on e and the problem becomes intractable.
(ii) e−X: we cannot have a direct edge from e−X without making additional parametric assumptions
about the role of e in Γ(Z ,Z ,e).
dg spu
(iii) Z → Y: we cannot have both Z → Y and Z → Y, since then, both mechanisms are domain
spu dg spu
general. WLOG, we let Z denote the features that never have domain-generalmechanisms to Y.
spu
20(iv) Y →Z →Z and Y →Z ← e: conditioning on Z and/or Z make Y dependent on e, so Y
dg spu dg dg spu
is always dependent on e and the problem becomes intractable.
e Z dg Y e Z dg Y e Z dg Y
X Z spu X Z spu X Z spu
(a) (b) (c)
Figure5: Generative Processes. Graphicalmodeldepicting thestructureofourdata-generatingprocess-
shadednodesindicateobservedvariables. X representstheobservedfeatures,Y representsobservedtargets,
and e represents domain influences. There is an explicit separation of domain-general Z and domain-
dg
specific Z features combined to generate observed X. Dashed edges indicate the possibility of an edge.
spu
Table 19: Generative Processes and Sufficient Conditions for Domain-Generality
Graphs in Figure 5
(a) (b) (c)
Z ⊥⊥Z |{Y,e} ✓ ✓ ✗
dg spu
Identifying Z is necessary ✓ ✓ ✗
dg
Table 20: Generative Processes and Sufficient Algorithms
Graphs in Figure 5
(a) (b) (c)
Solved by ERM ✗ ✗ ✓
Solved by TCRI ✓ ✓ ✓
B.2 Fully Informative Invariant Features
We briefly summarizeAhuja et al.[2021]’s results onminimax-optimalityofEmpiricalRisk Minimizationin
the Fully Informative Invariant Features setting (their Lemma 4). First, we informally state their assump-
tions.
Assumption 2: Linear structural equation model.
Assumption 3-4: Bounded Features.
Assumption 8: w partitions Z up to noise η .
dg Y
These assumptions are implied by our Assumption 4.1.
B.2.1 Proof Sufficiency of ERM [Ahuja et al., 2021]
If Assumptions 2, 4, and 8 hold, then there exists a classifier that puts a non-zero weight on the spurious
feature and continues to be Bayes optimal in all the training environments.
21Proof. Choose an arbitrary non-zero vector and derive a bound on the margin of (w , γ), where w is
dg dg
the true (optimal) linear predictor of Y from Z . Recall domain-general and domain-specific features
dg
z ∈ Z , z ∈ Z , respectively. Let y∗ = sign(w ·z ). The margin of (w , γ)) at point (z , z )
dg dg spu spu dg dg dg dg spu
with respect to y∗ is defined as:
y∗(w ·z )+y∗(γ·z ).
dg dg spu
Using Cauchy-Schwartzinequality, we get
|y∗(γ·z )|=|γ·z |≤kkγkz k.
spu spu spu
SinceZ isbounded,onecansetγ sufficientlysmallenoughtocontroly∗(γ·Z ). Ifkγk≤ c ,then
spu spu 2zsup
|γ·z |≤ c, where zsup satisfies that kzk≤zsup∀z ∈Z . From Assumption 8, ∃c>0 s.t.,
spu 2 spu
y∗(w ·z )≥c.
dg dg
Using |γ·z |≤ c, the margin becomes
spu 2
c
y∗(w ·z )+y∗(γ·z )≥c−|γ·z |≥ .
dg dg spu spu
2
From the above equation, it follows that sign (w ,γ)·(z ,z ) = sign (w ,0)·(z ,z ) ∀z ∈
dg dg spu dg dg spu dg
Z , z ∈Z .
dg spu spu
(cid:0) (cid:1) (cid:0) (cid:1)
Now, this condition is used to compute the error of a spurious classifier, i.e., based on (,γ). Define
g = I ◦(w ,γ)◦Γ−1, where I(·) is an indicator function that returns 1 if its input is ≥ 0. The error
spu dg
achieved by g is
spu
Re(g )=E Ye⊕I((w ,γ)·(z ,z )
spu dg dg spu
=E(cid:2)I (w ,0)·(z ,z ) ⊕η(cid:3)⊕I (w ,γ)·(z ,z )
dg dg spu y dg dg spu
=E[hη(cid:0)]. (cid:1) (cid:0) (cid:1)i
y
The error achieved by g is then due to the noise in observed Y and is, therefore, optimal in all
spu
domains.
It follows from above that since g is Bayes optimal in every domain, it is also the empirical risk
spu
minimizer (ERM) as it minimizes the sum of risks across training domains.
C Proof of Proposition 4.5
Assume that Φ (X) and Φ (X) are correlated with Y. Given Assumptions 4.1-4.2 and a representation
dg spu
Φ=Φ ⊕Φ that satisfies TIC, Φ (X)=Z ⇐⇒ Φ satisfies TCRI.
dg spu dg dg
Proof. ‘only if’. Assume that Φ (X)= Z . By the Total Information Criterion, we have that Φ (X)=
dg dg spu
Z . We observe the following paths from Z to Z : (i) Z → Y → Z , (ii) Z ← e → Z , and
spu dg spu dg spu dg spu
(iii) Z →X →Z . Conditioning onY,e blocks both paths (i) and path (ii); path (iii) contains a collider
dg spu
(Z and Z are common causes of X), so this path is blocked when X is not in the conditioning set. So,
dg spu
Z ⊥⊥Z |Y,e and therefore Φ (X)⊥⊥Φ (X)|Y,e, which completes this direction.
spu dg dg spu
‘if’. Assume that Φ satisfies TCRI. We proceed by contradiction. Let Φ = [Φ ;Φ ]. We consider the
dg spu
following scenario for Φ 6=Z .
dg dg
Scenario 1 (causal aggregation): Assume that Φ (X) ⊂ Z . From TIC, we have that Z† ⊂ Φ (X),
dg dg dg spu
where Z† ⊂Z is the subset of Z not captured by Φ . Since Φ (X) and Z† are colliders on Y, given
dg dg dg dg dg dg
botharesubsetsofZ ,Φ (X)6⊥⊥Φ (X)|Y,e,violatingTCRIandgivingacontradiction. So,Z ⊂Φ(X)
dg dg spu dg
Scenario2(anticausalexclusion): AssumethatΦ (X)⊂Z . FromTIC,wehavethatZ† ⊂Φ (X),
dg spu spu spu
where Z† ⊂Z is the subset of Z not captured by Φ . From Assumption 4.2 (faithfulness), we have
spu spu spu dg
that Φ (X)6⊥⊥Φ (X)|Y,e, violating TCRI and giving a contradiction. So, Z 6⊂Φ (X).
dg spu spu dg
Combining scenarios 1-2, it follows that Φ (X)=Z .
dg dg
22